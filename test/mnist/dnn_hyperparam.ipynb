{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/50 | Train Loss: 0.4768 | Val Loss: 0.2296\n",
      "Epoch   6/50 | Train Loss: 0.1261 | Val Loss: 0.1382\n",
      "Epoch  11/50 | Train Loss: 0.0914 | Val Loss: 0.1219\n",
      "Epoch  16/50 | Train Loss: 0.0695 | Val Loss: 0.1218\n",
      "Epoch  21/50 | Train Loss: 0.0596 | Val Loss: 0.1156\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 97.40%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nnum layers and neurons per layer\\nshuffling before each epoch\\nbatch_size for training per batchs\\ndropout\\nweight_decay\\noptmizers_type  \\nlearning rate\\nearly_stopping\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch adding hyperparameters\n",
    "# Dataset: MNIST (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "show_images(X_train)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, alpha, 1)\n",
    "        return x\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_pred.T + eps)) / m\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1. / layer_dims[i - 1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations, dropout_rates=None, training=True):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "    dropout_masks = {}\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i-1] == \"sigmoid\": A = sigmoid(Z)\n",
    "        elif activations[i-1] == \"relu\": A = relu(Z)\n",
    "        elif activations[i-1] == \"tanh\": A = tanh(Z)\n",
    "        elif activations[i-1] == \"leaky_relu\": A = leaky_relu(Z)\n",
    "        elif activations[i-1] == \"softmax\": A = softmax(Z)\n",
    "        else: raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        # Dropout\n",
    "        if training and dropout_rates and i <= len(dropout_rates):\n",
    "            rate = dropout_rates[i - 1]\n",
    "            mask = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A *= mask\n",
    "            A /= (1 - rate)\n",
    "            dropout_masks[f\"M{i}\"] = mask\n",
    "\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache, dropout_masks\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, params, cache, activations, dropout_masks=None, dropout_rates=None, weight_decay=0.0):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    dZ = cache[f\"A{L}\"] - Y\n",
    "\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T) + weight_decay * W\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        grads = {k: np.clip(v, -1, 1) for k, v in grads.items()}\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "            if dropout_masks and i - 1 in [int(k[1:]) for k in dropout_masks.keys()]:\n",
    "                mask = dropout_masks[f\"M{i-1}\"]\n",
    "                dA_prev *= mask\n",
    "                dA_prev /= (1 - dropout_rates[i - 2])\n",
    "\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = dA_prev * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = dA_prev * relu(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"tanh\":\n",
    "                dZ = dA_prev * tanh(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"leaky_relu\":\n",
    "                dZ = dA_prev * leaky_relu(Z_prev, is_derivative=True)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "    θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "'''\n",
    "def update_parameters(params, grads, lr, optimizer_type, velocity=None, beta=0.9):\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            velocity[f\"W{i}\"] = beta * velocity[f\"W{i}\"] + (1 - beta) * grads[f\"dW{i}\"]\n",
    "            velocity[f\"b{i}\"] = beta * velocity[f\"b{i}\"] + (1 - beta) * grads[f\"db{i}\"]\n",
    "            params[f\"W{i}\"] -= lr * velocity[f\"W{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * velocity[f\"b{i}\"]\n",
    "\n",
    "    return params, velocity\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X, \n",
    "    Y, \n",
    "    hidden_layers=[128, 64], \n",
    "    lr=0.01, \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=None, \n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=5, \n",
    "    print_every=100\n",
    "):\n",
    "    input_dim, output_dim = X.shape[1], Y.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "    velocity = None\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        X_shuffled, Y_shuffled = X[perm], Y[perm]\n",
    "\n",
    "        # Mini-batch iteration\n",
    "        if batch_size is None:\n",
    "            batches = [(X_shuffled, Y_shuffled)]\n",
    "        else:\n",
    "            batches = [\n",
    "                (X_shuffled[i:i+batch_size], Y_shuffled[i:i+batch_size])\n",
    "                for i in range(0, X.shape[0], batch_size)\n",
    "            ]\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, Y_batch in batches:\n",
    "            A_out, cache, dropout_masks = forward_propagation(X_batch, params, activations, dropout_rates)\n",
    "            loss = cross_entropy_loss(Y_batch, A_out)\n",
    "            grads = backward_propagation(Y_batch, params, cache, activations, dropout_masks, dropout_rates, weight_decay)\n",
    "            params, velocity = update_parameters(params, grads, lr, optimizer_type, velocity)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        # Validation loss\n",
    "        A_val, _, _ = forward_propagation(X_test, params, activations, training=False)\n",
    "        val_loss = cross_entropy_loss(y_test, A_val) if loss_function == \"cross_entropy\" else mse_loss(y_test, A_val)\n",
    "        loss = epoch_loss / len(batches)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_params = params.copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    params = best_params\n",
    "                    break\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _, _ = forward_propagation(X, params, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Run Training and Evaluate\n",
    "# =======================================\n",
    "params = train_neural_network(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    hidden_layers=[128, 64], \n",
    "    lr=0.05, \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    activations=[\"relu\", \"relu\", \"softmax\"],\n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=[0.2, 0.2],\n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=5, \n",
    "    print_every=5\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = predict(X_test, params, [\"relu\", \"relu\", \"softmax\"])\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nTest Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Preprocessing data...\n",
      "(48999, 784) (10500, 784) (10501, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:43:04,850] A new study created in memory with name: no-name-4f04fe53-cc4b-40eb-ab2d-7f437e744713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.3383 | Val Loss: 0.1772\n",
      "Epoch   6/40 | Train Loss: 0.0538 | Val Loss: 0.1374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:43:19,252] Trial 0 finished with value: 0.13890573049334692 and parameters: {'n_layers': 4, 'n_units_l0': 206, 'n_units_l1': 112, 'n_units_l2': 88, 'n_units_l3': 105, 'lr': 0.002325563077200157, 'batch_size': 128, 'dropout_l0': 0.055393567960190904, 'dropout_l1': 0.12337239482327178, 'dropout_l2': 0.34957144343035734, 'dropout_l3': 0.0736959587853595, 'weight_decay': 1.2584811620150104e-06, 'optimizer': 'adam', 'stopping_patience': 3, 'activation_l0': 'tanh', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'activation_l3': 'sigmoid'}. Best is trial 0 with value: 0.13890573049334692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 2.3089 | Val Loss: 2.2616\n",
      "Epoch   6/40 | Train Loss: 0.5536 | Val Loss: 0.3992\n",
      "Epoch  11/40 | Train Loss: 0.3507 | Val Loss: 0.2767\n",
      "Epoch  16/40 | Train Loss: 0.2765 | Val Loss: 0.2258\n",
      "Epoch  21/40 | Train Loss: 0.2416 | Val Loss: 0.2011\n",
      "Epoch  26/40 | Train Loss: 0.2198 | Val Loss: 0.1874\n",
      "Epoch  31/40 | Train Loss: 0.1999 | Val Loss: 0.1750\n",
      "Epoch  36/40 | Train Loss: 0.1890 | Val Loss: 0.1716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:44:38,473] Trial 1 finished with value: 0.16390229997651776 and parameters: {'n_layers': 4, 'n_units_l0': 79, 'n_units_l1': 198, 'n_units_l2': 206, 'n_units_l3': 244, 'lr': 0.070190964818856, 'batch_size': 64, 'dropout_l0': 0.09877502512895164, 'dropout_l1': 0.10888732902535808, 'dropout_l2': 0.2695215767792415, 'dropout_l3': 0.36557910812739364, 'weight_decay': 0.0003751288914134187, 'optimizer': 'sgd', 'stopping_patience': 7, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'activation_l2': 'sigmoid', 'activation_l3': 'relu'}. Best is trial 0 with value: 0.13890573049334692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.6251 | Val Loss: 0.2711\n",
      "Epoch   6/40 | Train Loss: 0.2058 | Val Loss: 0.1555\n",
      "Epoch  11/40 | Train Loss: 0.1515 | Val Loss: 0.1331\n",
      "Epoch  16/40 | Train Loss: 0.1148 | Val Loss: 0.1238\n",
      "Epoch  21/40 | Train Loss: 0.1017 | Val Loss: 0.1178\n",
      "Epoch  26/40 | Train Loss: 0.0867 | Val Loss: 0.1163\n",
      "Epoch  31/40 | Train Loss: 0.0762 | Val Loss: 0.1133\n",
      "Epoch  36/40 | Train Loss: 0.0704 | Val Loss: 0.1140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:45:08,068] Trial 2 finished with value: 0.11358019067388293 and parameters: {'n_layers': 3, 'n_units_l0': 112, 'n_units_l1': 111, 'n_units_l2': 233, 'lr': 0.08299437435649357, 'batch_size': 128, 'dropout_l0': 0.2686610620306382, 'dropout_l1': 0.33413741530263, 'dropout_l2': 0.16747458529192166, 'weight_decay': 0.0003637832141816612, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'tanh'}. Best is trial 2 with value: 0.11358019067388293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.3088 | Val Loss: 2.2946\n",
      "Epoch   6/40 | Train Loss: 2.2652 | Val Loss: 2.2442\n",
      "Epoch  11/40 | Train Loss: 2.1737 | Val Loss: 2.1210\n",
      "Epoch  16/40 | Train Loss: 1.9051 | Val Loss: 1.7767\n",
      "Epoch  21/40 | Train Loss: 1.5122 | Val Loss: 1.3531\n",
      "Epoch  26/40 | Train Loss: 1.2094 | Val Loss: 1.0489\n",
      "Epoch  31/40 | Train Loss: 0.9974 | Val Loss: 0.8493\n",
      "Epoch  36/40 | Train Loss: 0.8497 | Val Loss: 0.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:48:00,283] Trial 3 finished with value: 0.6109161888744702 and parameters: {'n_layers': 5, 'n_units_l0': 122, 'n_units_l1': 182, 'n_units_l2': 141, 'n_units_l3': 163, 'n_units_l4': 255, 'lr': 0.00024718886618107914, 'batch_size': 16, 'dropout_l0': 0.20596790268022253, 'dropout_l1': 0.08597771936217385, 'dropout_l2': 0.09785485807839489, 'dropout_l3': 0.3518590837649707, 'dropout_l4': 0.00889621774109206, 'weight_decay': 0.00020481657798929838, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'leaky_relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid', 'activation_l3': 'leaky_relu', 'activation_l4': 'sigmoid'}. Best is trial 2 with value: 0.11358019067388293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 3.0944 | Val Loss: 2.3154\n",
      "Epoch   6/40 | Train Loss: 2.6499 | Val Loss: 2.1556\n",
      "Epoch  11/40 | Train Loss: 2.4767 | Val Loss: 2.0700\n",
      "Epoch  16/40 | Train Loss: 2.3649 | Val Loss: 1.9989\n",
      "Epoch  21/40 | Train Loss: 2.2675 | Val Loss: 1.9249\n",
      "Epoch  26/40 | Train Loss: 2.1874 | Val Loss: 1.8440\n",
      "Epoch  31/40 | Train Loss: 2.1029 | Val Loss: 1.7569\n",
      "Epoch  36/40 | Train Loss: 2.0182 | Val Loss: 1.6665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:48:57,225] Trial 4 finished with value: 1.5947415727846206 and parameters: {'n_layers': 5, 'n_units_l0': 238, 'n_units_l1': 137, 'n_units_l2': 237, 'n_units_l3': 235, 'n_units_l4': 176, 'lr': 0.00011870031521177732, 'batch_size': 128, 'dropout_l0': 0.3446672261210866, 'dropout_l1': 0.32255639796998903, 'dropout_l2': 0.1767139365260697, 'dropout_l3': 0.2712684803423522, 'dropout_l4': 0.2572184773664086, 'weight_decay': 2.3638571269007726e-06, 'optimizer': 'momentum', 'stopping_patience': 8, 'activation_l0': 'leaky_relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'relu', 'activation_l3': 'relu', 'activation_l4': 'leaky_relu'}. Best is trial 2 with value: 0.11358019067388293.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.7565 | Val Loss: 0.2771\n",
      "Epoch   6/40 | Train Loss: 0.2038 | Val Loss: 0.1544\n",
      "Epoch  11/40 | Train Loss: 0.1616 | Val Loss: 0.1252\n",
      "Epoch  16/40 | Train Loss: 0.1436 | Val Loss: 0.1213\n",
      "Epoch  21/40 | Train Loss: 0.1385 | Val Loss: 0.1184\n",
      "Epoch  26/40 | Train Loss: 0.1308 | Val Loss: 0.1134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:49:42,170] Trial 5 finished with value: 0.11205717862343649 and parameters: {'n_layers': 3, 'n_units_l0': 194, 'n_units_l1': 162, 'n_units_l2': 120, 'lr': 0.024594307825148386, 'batch_size': 32, 'dropout_l0': 0.3658098142514846, 'dropout_l1': 0.3728911332352973, 'dropout_l2': 0.25019823283867504, 'weight_decay': 0.005457717031132873, 'optimizer': 'momentum', 'stopping_patience': 6, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'relu'}. Best is trial 5 with value: 0.11205717862343649.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.5594 | Val Loss: 0.2488\n",
      "Epoch   6/40 | Train Loss: 0.2049 | Val Loss: 0.1665\n",
      "Epoch  11/40 | Train Loss: 0.1623 | Val Loss: 0.1438\n",
      "Epoch  16/40 | Train Loss: 0.1379 | Val Loss: 0.1279\n",
      "Epoch  21/40 | Train Loss: 0.1205 | Val Loss: 0.1173\n",
      "Epoch  26/40 | Train Loss: 0.1018 | Val Loss: 0.1100\n",
      "Epoch  31/40 | Train Loss: 0.0923 | Val Loss: 0.1045\n",
      "Epoch  36/40 | Train Loss: 0.0820 | Val Loss: 0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:50:38,436] Trial 6 finished with value: 0.095066858144535 and parameters: {'n_layers': 3, 'n_units_l0': 215, 'n_units_l1': 206, 'n_units_l2': 76, 'lr': 0.00048272235518405935, 'batch_size': 128, 'dropout_l0': 0.2933672107211511, 'dropout_l1': 0.21597496679543338, 'dropout_l2': 0.2417606163252196, 'weight_decay': 0.00020577702806877135, 'optimizer': 'adam', 'stopping_patience': 4, 'activation_l0': 'relu', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 6 with value: 0.095066858144535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.0930 | Val Loss: 1.5015\n",
      "Epoch   6/40 | Train Loss: 0.7336 | Val Loss: 0.5277\n",
      "Epoch  11/40 | Train Loss: 0.5406 | Val Loss: 0.3940\n",
      "Epoch  16/40 | Train Loss: 0.4658 | Val Loss: 0.3452\n",
      "Epoch  21/40 | Train Loss: 0.4270 | Val Loss: 0.3181\n",
      "Epoch  26/40 | Train Loss: 0.4016 | Val Loss: 0.3009\n",
      "Epoch  31/40 | Train Loss: 0.3801 | Val Loss: 0.2874\n",
      "Epoch  36/40 | Train Loss: 0.3618 | Val Loss: 0.2776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:51:36,340] Trial 7 finished with value: 0.27050436589415766 and parameters: {'n_layers': 2, 'n_units_l0': 110, 'n_units_l1': 242, 'lr': 0.00019722914405121894, 'batch_size': 16, 'dropout_l0': 0.3984143373826136, 'dropout_l1': 0.37200151097899903, 'weight_decay': 1.7054943863967904e-06, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'tanh', 'activation_l1': 'leaky_relu'}. Best is trial 6 with value: 0.095066858144535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.5189 | Val Loss: 2.2593\n",
      "Epoch   6/40 | Train Loss: 2.0684 | Val Loss: 1.8841\n",
      "Epoch  11/40 | Train Loss: 1.7187 | Val Loss: 1.4321\n",
      "Epoch  16/40 | Train Loss: 1.4034 | Val Loss: 1.0660\n",
      "Epoch  21/40 | Train Loss: 1.1801 | Val Loss: 0.8399\n",
      "Epoch  26/40 | Train Loss: 1.0376 | Val Loss: 0.7055\n",
      "Epoch  31/40 | Train Loss: 0.9325 | Val Loss: 0.6200\n",
      "Epoch  36/40 | Train Loss: 0.8556 | Val Loss: 0.5609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:51:55,042] Trial 8 finished with value: 0.525439090395242 and parameters: {'n_layers': 3, 'n_units_l0': 96, 'n_units_l1': 97, 'n_units_l2': 69, 'lr': 0.0007576028863270012, 'batch_size': 128, 'dropout_l0': 0.20207716439720158, 'dropout_l1': 0.28194077610939977, 'dropout_l2': 0.1275657368080822, 'weight_decay': 0.0015936178616589942, 'optimizer': 'sgd', 'stopping_patience': 10, 'activation_l0': 'sigmoid', 'activation_l1': 'leaky_relu', 'activation_l2': 'relu'}. Best is trial 6 with value: 0.095066858144535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.5849 | Val Loss: 2.2215\n",
      "Epoch   6/40 | Train Loss: 1.8183 | Val Loss: 1.5011\n",
      "Epoch  11/40 | Train Loss: 1.4116 | Val Loss: 1.0936\n",
      "Epoch  16/40 | Train Loss: 1.1485 | Val Loss: 0.8503\n",
      "Epoch  21/40 | Train Loss: 0.9845 | Val Loss: 0.6999\n",
      "Epoch  26/40 | Train Loss: 0.8680 | Val Loss: 0.6033\n",
      "Epoch  31/40 | Train Loss: 0.7861 | Val Loss: 0.5378\n",
      "Epoch  36/40 | Train Loss: 0.7311 | Val Loss: 0.4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:52:44,516] Trial 9 finished with value: 0.4637957769067639 and parameters: {'n_layers': 3, 'n_units_l0': 222, 'n_units_l1': 223, 'n_units_l2': 191, 'lr': 0.0001233930536444182, 'batch_size': 64, 'dropout_l0': 0.2441568394454861, 'dropout_l1': 0.3714679945102517, 'dropout_l2': 0.3149165698690786, 'weight_decay': 3.846423348007209e-06, 'optimizer': 'sgd', 'stopping_patience': 9, 'activation_l0': 'tanh', 'activation_l1': 'leaky_relu', 'activation_l2': 'leaky_relu'}. Best is trial 6 with value: 0.095066858144535.\n",
      "/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_24062/1339452458.py:68: RuntimeWarning: overflow encountered in square\n",
      "  return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.4118 | Val Loss: 0.2014\n",
      "Epoch   6/40 | Train Loss: 0.1997 | Val Loss: 0.1359\n",
      "Epoch  11/40 | Train Loss: 0.1602 | Val Loss: 0.1149\n",
      "Epoch  16/40 | Train Loss: 0.1466 | Val Loss: 0.1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:53:23,740] Trial 10 finished with value: 0.10786982966676721 and parameters: {'n_layers': 2, 'n_units_l0': 162, 'n_units_l1': 256, 'lr': 0.0063441948822323575, 'batch_size': 32, 'dropout_l0': 0.49658880502421787, 'dropout_l1': 0.48036567821264864, 'weight_decay': 1.8468741225680186e-05, 'optimizer': 'adam', 'stopping_patience': 3, 'activation_l0': 'relu', 'activation_l1': 'sigmoid'}. Best is trial 6 with value: 0.095066858144535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.4127 | Val Loss: 0.2104\n",
      "Epoch   6/40 | Train Loss: 0.1981 | Val Loss: 0.1349\n",
      "Epoch  11/40 | Train Loss: 0.1630 | Val Loss: 0.1200\n",
      "Epoch  16/40 | Train Loss: 0.1430 | Val Loss: 0.1076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:54:06,948] Trial 11 finished with value: 0.10244187511721994 and parameters: {'n_layers': 2, 'n_units_l0': 164, 'n_units_l1': 246, 'lr': 0.007077118234668615, 'batch_size': 32, 'dropout_l0': 0.4705262998339709, 'dropout_l1': 0.4871413894035229, 'weight_decay': 2.5644750311072154e-05, 'optimizer': 'adam', 'stopping_patience': 3, 'activation_l0': 'relu', 'activation_l1': 'sigmoid'}. Best is trial 6 with value: 0.095066858144535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.4136 | Val Loss: 0.2271\n",
      "Epoch   6/40 | Train Loss: 0.1926 | Val Loss: 0.1384\n",
      "Epoch  11/40 | Train Loss: 0.1630 | Val Loss: 0.1297\n",
      "Epoch  16/40 | Train Loss: 0.1450 | Val Loss: 0.1174\n",
      "Epoch  21/40 | Train Loss: 0.1349 | Val Loss: 0.1139\n",
      "Epoch  26/40 | Train Loss: 0.1256 | Val Loss: 0.1192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:55:00,012] Trial 12 finished with value: 0.1125155968139811 and parameters: {'n_layers': 2, 'n_units_l0': 169, 'n_units_l1': 214, 'lr': 0.00629205101994105, 'batch_size': 32, 'dropout_l0': 0.4864729321228312, 'dropout_l1': 0.18651304537810376, 'weight_decay': 2.5880069693268053e-05, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'tanh'}. Best is trial 6 with value: 0.095066858144535.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.3519 | Val Loss: 0.1887\n",
      "Epoch   6/40 | Train Loss: 0.1366 | Val Loss: 0.1169\n",
      "Epoch  11/40 | Train Loss: 0.0993 | Val Loss: 0.1033\n",
      "Epoch  16/40 | Train Loss: 0.0789 | Val Loss: 0.0952\n",
      "Epoch  21/40 | Train Loss: 0.0673 | Val Loss: 0.0927\n",
      "Epoch  26/40 | Train Loss: 0.0564 | Val Loss: 0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:56:29,476] Trial 13 finished with value: 0.08818614636231767 and parameters: {'n_layers': 2, 'n_units_l0': 253, 'n_units_l1': 232, 'lr': 0.0010525018935465867, 'batch_size': 32, 'dropout_l0': 0.42049973224465, 'dropout_l1': 0.48623946328498946, 'weight_decay': 4.032118261007188e-05, 'optimizer': 'adam', 'stopping_patience': 4, 'activation_l0': 'relu', 'activation_l1': 'sigmoid'}. Best is trial 13 with value: 0.08818614636231767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.5714 | Val Loss: 0.2246\n",
      "Epoch   6/40 | Train Loss: 0.1907 | Val Loss: 0.1557\n",
      "Epoch  11/40 | Train Loss: 0.1502 | Val Loss: 0.1346\n",
      "Epoch  16/40 | Train Loss: 0.1281 | Val Loss: 0.1241\n",
      "Epoch  21/40 | Train Loss: 0.1095 | Val Loss: 0.1145\n",
      "Epoch  26/40 | Train Loss: 0.0950 | Val Loss: 0.1112\n",
      "Epoch  31/40 | Train Loss: 0.0812 | Val Loss: 0.1088\n",
      "Epoch  36/40 | Train Loss: 0.0751 | Val Loss: 0.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:57:18,094] Trial 14 finished with value: 0.10242048232035657 and parameters: {'n_layers': 4, 'n_units_l0': 255, 'n_units_l1': 70, 'n_units_l2': 108, 'n_units_l3': 66, 'lr': 0.0008337948695482007, 'batch_size': 128, 'dropout_l0': 0.30728648914943635, 'dropout_l1': 0.21175670892535692, 'dropout_l2': 0.48687260110256736, 'dropout_l3': 0.047164212081373114, 'weight_decay': 5.9579473939663726e-05, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'sigmoid', 'activation_l2': 'leaky_relu', 'activation_l3': 'tanh'}. Best is trial 13 with value: 0.08818614636231767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.3647 | Val Loss: 0.2113\n",
      "Epoch   6/40 | Train Loss: 0.1504 | Val Loss: 0.1315\n",
      "Epoch  11/40 | Train Loss: 0.1215 | Val Loss: 0.1134\n",
      "Epoch  16/40 | Train Loss: 0.1060 | Val Loss: 0.1025\n",
      "Epoch  21/40 | Train Loss: 0.0999 | Val Loss: 0.0993\n",
      "Epoch  26/40 | Train Loss: 0.0960 | Val Loss: 0.0941\n",
      "Epoch  31/40 | Train Loss: 0.0894 | Val Loss: 0.0918\n",
      "Epoch  36/40 | Train Loss: 0.0873 | Val Loss: 0.0897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:59:00,973] Trial 15 finished with value: 0.08904783248152663 and parameters: {'n_layers': 2, 'n_units_l0': 253, 'n_units_l1': 180, 'lr': 0.000649684193716557, 'batch_size': 32, 'dropout_l0': 0.4233994973685379, 'dropout_l1': 0.19088196482466901, 'weight_decay': 0.0011247072908364684, 'optimizer': 'adam', 'stopping_patience': 4, 'activation_l0': 'relu', 'activation_l1': 'sigmoid'}. Best is trial 13 with value: 0.08818614636231767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.2696 | Val Loss: 0.1558\n",
      "Epoch   6/40 | Train Loss: 0.0959 | Val Loss: 0.0945\n",
      "Epoch  11/40 | Train Loss: 0.0806 | Val Loss: 0.0920\n",
      "Epoch  16/40 | Train Loss: 0.0764 | Val Loss: 0.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:59:44,478] Trial 16 finished with value: 0.08763876655329464 and parameters: {'n_layers': 2, 'n_units_l0': 252, 'n_units_l1': 172, 'lr': 0.002031659555669348, 'batch_size': 32, 'dropout_l0': 0.42825161275128787, 'dropout_l1': 0.041746952144603944, 'weight_decay': 0.0012977862486594559, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'tanh'}. Best is trial 16 with value: 0.08763876655329464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.3487 | Val Loss: 0.2134\n",
      "Epoch   6/40 | Train Loss: 0.1953 | Val Loss: 0.1635\n",
      "Epoch  11/40 | Train Loss: 0.1877 | Val Loss: 0.1541\n",
      "Epoch  16/40 | Train Loss: 0.1831 | Val Loss: 0.1558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 17:00:18,616] Trial 17 finished with value: 0.1571502657481007 and parameters: {'n_layers': 2, 'n_units_l0': 188, 'n_units_l1': 139, 'lr': 0.0020046864294471973, 'batch_size': 32, 'dropout_l0': 0.41913409722446476, 'dropout_l1': 0.4460628162204647, 'weight_decay': 0.00858700800755168, 'optimizer': 'adam', 'stopping_patience': 6, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh'}. Best is trial 16 with value: 0.08763876655329464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 2.0245 | Val Loss: 1.5949\n",
      "Epoch   6/40 | Train Loss: 0.6493 | Val Loss: 0.4763\n",
      "Epoch  11/40 | Train Loss: 0.4875 | Val Loss: 0.3680\n",
      "Epoch  16/40 | Train Loss: 0.4277 | Val Loss: 0.3301\n",
      "Epoch  21/40 | Train Loss: 0.3937 | Val Loss: 0.3096\n",
      "Epoch  26/40 | Train Loss: 0.3735 | Val Loss: 0.2965\n",
      "Epoch  31/40 | Train Loss: 0.3532 | Val Loss: 0.2865\n",
      "Epoch  36/40 | Train Loss: 0.3375 | Val Loss: 0.2786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 17:01:12,820] Trial 18 finished with value: 0.27295989782093494 and parameters: {'n_layers': 2, 'n_units_l0': 137, 'n_units_l1': 229, 'lr': 0.0014279549273826912, 'batch_size': 32, 'dropout_l0': 0.3536292645755336, 'dropout_l1': 0.0006888428833396534, 'weight_decay': 8.758302333476594e-06, 'optimizer': 'momentum', 'stopping_patience': 5, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh'}. Best is trial 16 with value: 0.08763876655329464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.5572 | Val Loss: 0.3918\n",
      "Epoch   6/40 | Train Loss: 0.2683 | Val Loss: 0.2041\n",
      "Epoch  11/40 | Train Loss: 0.2481 | Val Loss: 0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 17:01:44,837] Trial 19 finished with value: 0.19164454356941024 and parameters: {'n_layers': 3, 'n_units_l0': 232, 'n_units_l1': 157, 'n_units_l2': 169, 'lr': 0.014969041678153945, 'batch_size': 32, 'dropout_l0': 0.44866557678034674, 'dropout_l1': 0.010090782051309567, 'dropout_l2': 0.0034486369313878606, 'weight_decay': 0.0017392105064738164, 'optimizer': 'adam', 'stopping_patience': 4, 'activation_l0': 'relu', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu'}. Best is trial 16 with value: 0.08763876655329464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best trial:\n",
      "  Value: 0.0876\n",
      "  Params:\n",
      "    n_layers: 2\n",
      "    n_units_l0: 252\n",
      "    n_units_l1: 172\n",
      "    lr: 0.002031659555669348\n",
      "    batch_size: 32\n",
      "    dropout_l0: 0.42825161275128787\n",
      "    dropout_l1: 0.041746952144603944\n",
      "    weight_decay: 0.0012977862486594559\n",
      "    optimizer: adam\n",
      "    stopping_patience: 5\n",
      "    activation_l0: relu\n",
      "    activation_l1: tanh\n",
      "Epoch   1/40 | Train Loss: 0.2696 | Val Loss: 0.1558\n",
      "Epoch   6/40 | Train Loss: 0.0959 | Val Loss: 0.0945\n",
      "Epoch  11/40 | Train Loss: 0.0806 | Val Loss: 0.0920\n",
      "Epoch  16/40 | Train Loss: 0.0764 | Val Loss: 0.0915\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 97.31%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch (made better)\n",
    "# Dataset: MNIST (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from functools import partial\n",
    "import optuna\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "show_images(X_train)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    x = np.clip(x, -500, 500) # prevent overflow in exp\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, alpha, 1)\n",
    "        return x\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    Y_pred = np.clip(Y_pred.T, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(Y * np.log(Y_pred), axis=1))\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims, activations):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        act = activations[i - 1]\n",
    "        if act in [\"relu\", \"leaky_relu\"]:\n",
    "            scale = np.sqrt(2. / layer_dims[i - 1])  # He init\n",
    "        else:\n",
    "            scale = np.sqrt(1. / layer_dims[i - 1])  # Xavier\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * scale\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations, dropout_rates=None, training=True):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "    dropout_masks = {}\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i-1] == \"sigmoid\": A = sigmoid(Z)\n",
    "        elif activations[i-1] == \"relu\": A = relu(Z)\n",
    "        elif activations[i-1] == \"tanh\": A = tanh(Z)\n",
    "        elif activations[i-1] == \"leaky_relu\": A = leaky_relu(Z)\n",
    "        elif activations[i-1] == \"softmax\": A = softmax(Z)\n",
    "        else: raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        # Dropout\n",
    "        if training and dropout_rates and i <= len(dropout_rates):\n",
    "            rate = dropout_rates[i - 1]\n",
    "            mask = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A *= mask\n",
    "            A /= (1 - rate)\n",
    "            dropout_masks[f\"M{i}\"] = mask\n",
    "\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache, dropout_masks\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, params, cache, activations, dropout_masks=None, dropout_rates=None, weight_decay=0.0):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    dZ = cache[f\"A{L}\"] - Y\n",
    "\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T) + weight_decay * W\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        max_norm = 5.0\n",
    "        total_norm = np.sqrt(sum(np.sum(np.square(g)) for g in grads.values()))\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            for k in grads.keys():\n",
    "                grads[k] *= clip_coef\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "            if dropout_masks and i - 1 in [int(k[1:]) for k in dropout_masks.keys()]:\n",
    "                mask = dropout_masks[f\"M{i-1}\"]\n",
    "                dA_prev *= mask\n",
    "                dA_prev /= (1 - dropout_rates[i - 2])\n",
    "\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = dA_prev * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = dA_prev * relu(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"tanh\":\n",
    "                dZ = dA_prev * tanh(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"leaky_relu\":\n",
    "                dZ = dA_prev * leaky_relu(Z_prev, is_derivative=True)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "    θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "\n",
    "    Adam:\n",
    "    m^(t+1) <- β1 m^t + (1-β1)∇L(y, ŷ)^t\n",
    "    v^(t+1) <- β2 v^t + (1-β2)(∇L(y, ŷ)^t)^2\n",
    "    m̂^(t+1) <- m^(t+1) / (1 - β1^(t+1))\n",
    "    v̂^(t+1) <- v^(t+1) / (1 - β2^(t+1))\n",
    "    θ^(t+1) <- θ^t - η m̂^(t+1) / (√(v̂^(t+1)) + ε)\n",
    "'''\n",
    "def update_parameters(params, grads, lr, optimizer_type, velocity=None, beta=0.9, m=None, v=None, t=1, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            velocity[f\"W{i}\"] = beta * velocity[f\"W{i}\"] + (1 - beta) * grads[f\"dW{i}\"]\n",
    "            velocity[f\"b{i}\"] = beta * velocity[f\"b{i}\"] + (1 - beta) * grads[f\"db{i}\"]\n",
    "            params[f\"W{i}\"] -= lr * velocity[f\"W{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * velocity[f\"b{i}\"]\n",
    "\n",
    "    elif optimizer_type == \"adam\":\n",
    "        if m is None:\n",
    "            m = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "            v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            for key in [\"W\", \"b\"]:\n",
    "                name = f\"{key}{i}\"\n",
    "                m[name] = beta1 * m[name] + (1 - beta1) * grads[f\"d{name}\"]\n",
    "                v[name] = beta2 * v[name] + (1 - beta2) * (grads[f\"d{name}\"] ** 2)\n",
    "                m_hat = m[name] / (1 - beta1 ** t)\n",
    "                v_hat = v[name] / (1 - beta2 ** t)\n",
    "                params[name] -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "    return params, velocity, (m, v)\n",
    "\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    X_val, \n",
    "    Y_val,\n",
    "    hidden_layers=[128, 64], \n",
    "    lr=0.01, \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=None, \n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=5, \n",
    "    print_every=100\n",
    "):\n",
    "    input_dim, output_dim = X_train.shape[1], Y_train.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims, activations)\n",
    "    velocity = None\n",
    "    (m,v) = (None, None)\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled, Y_shuffled = X_train[perm], Y_train[perm]\n",
    "\n",
    "        # Mini-batch iteration\n",
    "        def batch_iterator(X, Y, batch_size):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                yield X[i:i+batch_size], Y[i:i+batch_size]\n",
    "\n",
    "        # Training by batch\n",
    "        epoch_loss = []\n",
    "        for X_batch, Y_batch in batch_iterator(X_shuffled, Y_shuffled, batch_size):\n",
    "            A_out, cache, dropout_masks = forward_propagation(X_batch, params, activations, dropout_rates)\n",
    "            loss = cross_entropy_loss(Y_batch, A_out)\n",
    "            grads = backward_propagation(Y_batch, params, cache, activations, dropout_masks, dropout_rates, weight_decay)\n",
    "            params, velocity, (m,v) = update_parameters(params, grads, lr, optimizer_type, velocity, m=m, v=v, t=epoch+1)\n",
    "            epoch_loss.append(loss)\n",
    "\n",
    "        # Validation loss\n",
    "        A_val, _, _ = forward_propagation(X_val, params, activations, training=False)\n",
    "        val_loss = cross_entropy_loss(Y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(Y_val, A_val)\n",
    "        loss = np.mean(epoch_loss)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_params = params.copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    params = best_params\n",
    "                    break\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _, _ = forward_propagation(X, params, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Hyperparameter Optimization with Optuna\n",
    "# =======================================\n",
    "def objective(trial, loss_function=\"cross_entropy\", epochs=40):\n",
    "    # Hyperparameters to search\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
    "    hidden_layers = [trial.suggest_int(f\"n_units_l{i}\", 64, 256) for i in range(n_layers)]\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    dropout_rates = [trial.suggest_float(f\"dropout_l{i}\", 0.0, 0.5) for i in range(n_layers)]\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"adam\"])\n",
    "    stopping_patience = trial.suggest_int(\"stopping_patience\", 3, 10)\n",
    "    # Activation functions per layer\n",
    "    possible_activations = [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\"]\n",
    "    activations = [trial.suggest_categorical(f\"activation_l{i}\", possible_activations) for i in range(n_layers)]\n",
    "    activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "    # Train network\n",
    "    params = train_neural_network(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        X_val, \n",
    "        y_val,\n",
    "        hidden_layers=hidden_layers,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        activations=activations,\n",
    "        loss_function=loss_function,\n",
    "        optimizer_type=optimizer_type,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_decay=weight_decay,\n",
    "        early_stopping=True,\n",
    "        patience=stopping_patience,\n",
    "        print_every=5\n",
    "    )\n",
    "\n",
    "    # Optuna tries to minimize the loss function\n",
    "    A_val, _, _ = forward_propagation(X_val, params, activations, training=False)\n",
    "    val_loss = cross_entropy_loss(y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(y_val, A_val)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "loss_function = \"cross_entropy\"\n",
    "epochs = 40\n",
    "obj = partial(objective, loss_function=loss_function, epochs=epochs)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(obj, n_trials=20)\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# =======================================\n",
    "# 11. Retrain Best Model on Train + Val and Evaluate on Test\n",
    "# =======================================\n",
    "best_params = trial.params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_layers = [best_params[f\"n_units_l{i}\"] for i in range(n_layers)]\n",
    "lr = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "dropout_rates = [best_params[f\"dropout_l{i}\"] for i in range(n_layers)]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "optimizer_type = best_params[\"optimizer\"]\n",
    "stopping_patience = best_params[\"stopping_patience\"]\n",
    "activations = [best_params[f\"activation_l{i}\"] for i in range(n_layers)]\n",
    "activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "# Combine train and val sets\n",
    "final_params = train_neural_network(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_val, \n",
    "    y_val,\n",
    "    hidden_layers=hidden_layers,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    activations=activations,\n",
    "    loss_function=loss_function,\n",
    "    optimizer_type=optimizer_type,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_decay=weight_decay,\n",
    "    early_stopping=True,\n",
    "    patience=stopping_patience,\n",
    "    print_every=5\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = predict(X_test, final_params, activations)\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

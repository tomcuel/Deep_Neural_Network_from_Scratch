{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Preprocessing data...\n",
      "(48999, 784) (10500, 784) (10501, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:41:53,398] A new study created in memory with name: no-name-19b278ee-7ef9-4c42-9588-5d1565d69f2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.2878 | Val Loss: 2.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:42:45,822] Trial 0 finished with value: 0.2747677830147684 and parameters: {'n_layers': 4, 'n_units_l0': 183, 'n_units_l1': 152, 'n_units_l2': 107, 'n_units_l3': 255, 'lr': 0.0019069327996107386, 'batch_size': 64, 'dropout_l0': 0.4362596795047295, 'dropout_l1': 0.04100131827808462, 'dropout_l2': 0.3090282279178333, 'dropout_l3': 0.367185825156483, 'weight_decay': 5.1852354036004476e-05, 'optimizer': 'sgd', 'stopping_patience': 10, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu', 'activation_l3': 'relu'}. Best is trial 0 with value: 0.2747677830147684.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.9862 | Val Loss: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:43:09,851] Trial 1 finished with value: 0.1247305167646052 and parameters: {'n_layers': 4, 'n_units_l0': 245, 'n_units_l1': 251, 'n_units_l2': 82, 'n_units_l3': 145, 'lr': 0.038510534913813836, 'batch_size': 128, 'dropout_l0': 0.1831658641852092, 'dropout_l1': 0.05753111130311167, 'dropout_l2': 0.4032623979329467, 'dropout_l3': 0.2775726206038843, 'weight_decay': 3.360078890453724e-06, 'optimizer': 'sgd', 'stopping_patience': 6, 'activation_l0': 'leaky_relu', 'activation_l1': 'relu', 'activation_l2': 'leaky_relu', 'activation_l3': 'leaky_relu'}. Best is trial 1 with value: 0.1247305167646052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 2.1258 | Val Loss: 1.6880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:43:41,316] Trial 2 finished with value: 0.15050061259281608 and parameters: {'n_layers': 3, 'n_units_l0': 219, 'n_units_l1': 69, 'n_units_l2': 219, 'lr': 0.012685189356145628, 'batch_size': 128, 'dropout_l0': 0.18148274329122954, 'dropout_l1': 0.4166414502415938, 'dropout_l2': 0.07035315976760986, 'weight_decay': 0.0012194984725748038, 'optimizer': 'sgd', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 1 with value: 0.1247305167646052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.3359 | Val Loss: 2.2977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:44:50,060] Trial 3 finished with value: 0.9534596992727886 and parameters: {'n_layers': 4, 'n_units_l0': 138, 'n_units_l1': 182, 'n_units_l2': 116, 'n_units_l3': 94, 'lr': 0.0001212963517310713, 'batch_size': 32, 'dropout_l0': 0.21966290842161845, 'dropout_l1': 0.20072581115340532, 'dropout_l2': 0.16850296740864162, 'dropout_l3': 0.15733978500732032, 'weight_decay': 3.4172197553250706e-05, 'optimizer': 'sgd', 'stopping_patience': 10, 'activation_l0': 'sigmoid', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'activation_l3': 'relu'}. Best is trial 1 with value: 0.1247305167646052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.3084 | Val Loss: 2.2765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:45:56,987] Trial 4 finished with value: 0.36202827318395375 and parameters: {'n_layers': 4, 'n_units_l0': 225, 'n_units_l1': 158, 'n_units_l2': 207, 'n_units_l3': 249, 'lr': 0.0009089731876527512, 'batch_size': 64, 'dropout_l0': 0.27774603438072987, 'dropout_l1': 0.4767326215066528, 'dropout_l2': 0.016377155971389834, 'dropout_l3': 0.04018858120965335, 'weight_decay': 3.477374803581573e-05, 'optimizer': 'sgd', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'leaky_relu', 'activation_l3': 'sigmoid'}. Best is trial 1 with value: 0.1247305167646052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 1.9428 | Val Loss: 1.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:46:37,226] Trial 5 finished with value: 0.16659939607304047 and parameters: {'n_layers': 3, 'n_units_l0': 226, 'n_units_l1': 141, 'n_units_l2': 123, 'lr': 0.07666736310139373, 'batch_size': 128, 'dropout_l0': 0.42251233827860735, 'dropout_l1': 0.49677994527164027, 'dropout_l2': 0.34400159467606006, 'weight_decay': 3.012259862416159e-06, 'optimizer': 'sgd', 'stopping_patience': 4, 'activation_l0': 'sigmoid', 'activation_l1': 'relu', 'activation_l2': 'sigmoid'}. Best is trial 1 with value: 0.1247305167646052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.8631 | Val Loss: 0.2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:46:57,897] Trial 6 finished with value: 0.11348644915100957 and parameters: {'n_layers': 4, 'n_units_l0': 75, 'n_units_l1': 129, 'n_units_l2': 132, 'n_units_l3': 127, 'lr': 0.04961137732586776, 'batch_size': 64, 'dropout_l0': 0.07759228868565299, 'dropout_l1': 0.3457869345157897, 'dropout_l2': 0.17785942245458192, 'dropout_l3': 0.08857534923169036, 'weight_decay': 0.0013950885314389552, 'optimizer': 'sgd', 'stopping_patience': 3, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh', 'activation_l2': 'sigmoid', 'activation_l3': 'relu'}. Best is trial 6 with value: 0.11348644915100957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 1.0252 | Val Loss: 0.3387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:47:29,116] Trial 7 finished with value: 0.12247606715497832 and parameters: {'n_layers': 5, 'n_units_l0': 76, 'n_units_l1': 178, 'n_units_l2': 179, 'n_units_l3': 105, 'n_units_l4': 251, 'lr': 0.026550046492733428, 'batch_size': 128, 'dropout_l0': 0.04239009659270471, 'dropout_l1': 0.0008622051525433161, 'dropout_l2': 0.20647599916756004, 'dropout_l3': 0.4411144443509405, 'dropout_l4': 0.22064040135841173, 'weight_decay': 0.00035163125510774025, 'optimizer': 'momentum', 'stopping_patience': 5, 'activation_l0': 'leaky_relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'tanh', 'activation_l3': 'tanh', 'activation_l4': 'relu'}. Best is trial 6 with value: 0.11348644915100957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 1.5278 | Val Loss: 0.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:48:42,120] Trial 8 finished with value: 0.1493878671082619 and parameters: {'n_layers': 4, 'n_units_l0': 145, 'n_units_l1': 243, 'n_units_l2': 111, 'n_units_l3': 75, 'lr': 0.0036627055007548283, 'batch_size': 32, 'dropout_l0': 0.10599422297405653, 'dropout_l1': 0.3942583879455519, 'dropout_l2': 0.4662282359695203, 'dropout_l3': 0.2473275630518052, 'weight_decay': 0.0005425457400539275, 'optimizer': 'momentum', 'stopping_patience': 5, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu', 'activation_l3': 'leaky_relu'}. Best is trial 6 with value: 0.11348644915100957.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.6672 | Val Loss: 0.2536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:49:55,179] Trial 9 finished with value: 0.11160049825029879 and parameters: {'n_layers': 3, 'n_units_l0': 209, 'n_units_l1': 120, 'n_units_l2': 189, 'lr': 0.009696525122452946, 'batch_size': 16, 'dropout_l0': 0.28072771666019763, 'dropout_l1': 0.39294364211467586, 'dropout_l2': 0.28250159471779807, 'weight_decay': 1.743137341591765e-06, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'leaky_relu'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.6137 | Val Loss: 0.2778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:51:11,737] Trial 10 finished with value: 0.1386691799475336 and parameters: {'n_layers': 2, 'n_units_l0': 182, 'n_units_l1': 92, 'lr': 0.007928611623259835, 'batch_size': 16, 'dropout_l0': 0.3354540085573868, 'dropout_l1': 0.2645891153984711, 'weight_decay': 0.008755463809790438, 'optimizer': 'momentum', 'stopping_patience': 8, 'activation_l0': 'relu', 'activation_l1': 'leaky_relu'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.3827 | Val Loss: 0.2128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:51:32,466] Trial 11 finished with value: 0.11707044899923187 and parameters: {'n_layers': 2, 'n_units_l0': 66, 'n_units_l1': 117, 'lr': 0.013995031309546444, 'batch_size': 16, 'dropout_l0': 0.03549404150278715, 'dropout_l1': 0.3075561906682012, 'weight_decay': 1.1536752167369663e-06, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 0.7095 | Val Loss: 0.2839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:51:44,208] Trial 12 finished with value: 0.1738404384029142 and parameters: {'n_layers': 5, 'n_units_l0': 109, 'n_units_l1': 113, 'n_units_l2': 252, 'n_units_l3': 183, 'n_units_l4': 78, 'lr': 0.09780420618183983, 'batch_size': 64, 'dropout_l0': 0.27343201451580756, 'dropout_l1': 0.3355408155489111, 'dropout_l2': 0.14099778112117425, 'dropout_l3': 0.0002934991025892453, 'dropout_l4': 0.4652250209988801, 'weight_decay': 0.004778293214429581, 'optimizer': 'sgd', 'stopping_patience': 3, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh', 'activation_l2': 'sigmoid', 'activation_l3': 'relu', 'activation_l4': 'tanh'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch   1/40 | Train Loss: 1.5116 | Val Loss: 0.6252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:52:52,894] Trial 13 finished with value: 0.12055794643093504 and parameters: {'n_layers': 3, 'n_units_l0': 110, 'n_units_l1': 118, 'n_units_l2': 158, 'lr': 0.004427308130368531, 'batch_size': 16, 'dropout_l0': 0.34150763030872666, 'dropout_l1': 0.15698960955719837, 'dropout_l2': 0.27299323144458865, 'weight_decay': 8.8995044573147e-06, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'sigmoid'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 1.8158 | Val Loss: 1.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:54:56,123] Trial 14 finished with value: 0.15430397447845298 and parameters: {'n_layers': 3, 'n_units_l0': 187, 'n_units_l1': 208, 'n_units_l2': 158, 'lr': 0.000796497676916399, 'batch_size': 16, 'dropout_l0': 0.11824667181804396, 'dropout_l1': 0.3777025794329134, 'dropout_l2': 0.23291862602670338, 'weight_decay': 0.0001792595390428631, 'optimizer': 'momentum', 'stopping_patience': 3, 'activation_l0': 'relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'leaky_relu'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.6658 | Val Loss: 0.2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:55:15,246] Trial 15 finished with value: 0.1129476438354298 and parameters: {'n_layers': 2, 'n_units_l0': 108, 'n_units_l1': 86, 'lr': 0.03155356296479119, 'batch_size': 64, 'dropout_l0': 0.49548410198201726, 'dropout_l1': 0.25812032679452135, 'weight_decay': 0.001525189781042008, 'optimizer': 'sgd', 'stopping_patience': 7, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.7313 | Val Loss: 0.3139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:56:03,263] Trial 16 finished with value: 0.14175917184203052 and parameters: {'n_layers': 2, 'n_units_l0': 112, 'n_units_l1': 68, 'lr': 0.02304077327630813, 'batch_size': 16, 'dropout_l0': 0.48487178985402296, 'dropout_l1': 0.16052386759169202, 'weight_decay': 1.5031212166486185e-05, 'optimizer': 'sgd', 'stopping_patience': 7, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 0.9717 | Val Loss: 0.4413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:56:32,021] Trial 17 finished with value: 0.15406291105304934 and parameters: {'n_layers': 2, 'n_units_l0': 201, 'n_units_l1': 92, 'lr': 0.007786255413065325, 'batch_size': 64, 'dropout_l0': 0.35759326454732177, 'dropout_l1': 0.27355432682002656, 'weight_decay': 0.0027066539780302478, 'optimizer': 'sgd', 'stopping_patience': 9, 'activation_l0': 'tanh', 'activation_l1': 'leaky_relu'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.4623 | Val Loss: 2.3386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:57:16,719] Trial 18 finished with value: 1.3922644492908818 and parameters: {'n_layers': 2, 'n_units_l0': 162, 'n_units_l1': 94, 'lr': 0.00010664570618456758, 'batch_size': 32, 'dropout_l0': 0.49676028692433244, 'dropout_l1': 0.4365547779749992, 'weight_decay': 0.00012296829319922612, 'optimizer': 'momentum', 'stopping_patience': 7, 'activation_l0': 'relu', 'activation_l1': 'sigmoid'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/40 | Train Loss: 2.0145 | Val Loss: 1.4093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 15:57:59,444] Trial 19 finished with value: 0.19009591453722519 and parameters: {'n_layers': 3, 'n_units_l0': 255, 'n_units_l1': 94, 'n_units_l2': 251, 'lr': 0.0017741401001514258, 'batch_size': 64, 'dropout_l0': 0.3936046594457588, 'dropout_l1': 0.21292766785234962, 'dropout_l2': 0.49785106679099655, 'weight_decay': 1.0841990582687776e-06, 'optimizer': 'sgd', 'stopping_patience': 9, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu'}. Best is trial 9 with value: 0.11160049825029879.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.1116\n",
      "  Params:\n",
      "    n_layers: 3\n",
      "    n_units_l0: 209\n",
      "    n_units_l1: 120\n",
      "    n_units_l2: 189\n",
      "    lr: 0.009696525122452946\n",
      "    batch_size: 16\n",
      "    dropout_l0: 0.28072771666019763\n",
      "    dropout_l1: 0.39294364211467586\n",
      "    dropout_l2: 0.28250159471779807\n",
      "    weight_decay: 1.743137341591765e-06\n",
      "    optimizer: sgd\n",
      "    stopping_patience: 8\n",
      "    activation_l0: relu\n",
      "    activation_l1: leaky_relu\n",
      "    activation_l2: leaky_relu\n",
      "Epoch   1/40 | Train Loss: 0.6672 | Val Loss: 0.2536\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 97.39%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch using Optuna for Hyperparameter Optimization\n",
    "# Dataset: MNIST (Handwritten Digit Recognition)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from functools import partial\n",
    "import optuna\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "show_images(X_train)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, alpha, 1)\n",
    "        return x\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_pred.T + eps)) / m\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1. / layer_dims[i - 1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations, dropout_rates=None, training=True):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "    dropout_masks = {}\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i-1] == \"sigmoid\": A = sigmoid(Z)\n",
    "        elif activations[i-1] == \"relu\": A = relu(Z)\n",
    "        elif activations[i-1] == \"tanh\": A = tanh(Z)\n",
    "        elif activations[i-1] == \"leaky_relu\": A = leaky_relu(Z)\n",
    "        elif activations[i-1] == \"softmax\": A = softmax(Z)\n",
    "        else: raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        # Dropout\n",
    "        if training and dropout_rates and i <= len(dropout_rates):\n",
    "            rate = dropout_rates[i - 1]\n",
    "            mask = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A *= mask\n",
    "            A /= (1 - rate)\n",
    "            dropout_masks[f\"M{i}\"] = mask\n",
    "\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache, dropout_masks\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, params, cache, activations, dropout_masks=None, dropout_rates=None, weight_decay=0.0):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    dZ = cache[f\"A{L}\"] - Y\n",
    "\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T) + weight_decay * W\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        grads = {k: np.clip(v, -1, 1) for k, v in grads.items()}\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "            if dropout_masks and i - 1 in [int(k[1:]) for k in dropout_masks.keys()]:\n",
    "                mask = dropout_masks[f\"M{i-1}\"]\n",
    "                dA_prev *= mask\n",
    "                dA_prev /= (1 - dropout_rates[i - 2])\n",
    "\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = dA_prev * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = dA_prev * relu(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"tanh\":\n",
    "                dZ = dA_prev * tanh(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"leaky_relu\":\n",
    "                dZ = dA_prev * leaky_relu(Z_prev, is_derivative=True)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "    θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "'''\n",
    "def update_parameters(params, grads, lr, optimizer_type, velocity=None, beta=0.9):\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            velocity[f\"W{i}\"] = beta * velocity[f\"W{i}\"] + (1 - beta) * grads[f\"dW{i}\"]\n",
    "            velocity[f\"b{i}\"] = beta * velocity[f\"b{i}\"] + (1 - beta) * grads[f\"db{i}\"]\n",
    "            params[f\"W{i}\"] -= lr * velocity[f\"W{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * velocity[f\"b{i}\"]\n",
    "\n",
    "    return params, velocity\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    X_val, \n",
    "    Y_val,\n",
    "    hidden_layers=[128, 64], \n",
    "    lr=0.01, \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=None, \n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=5, \n",
    "    print_every=100\n",
    "):\n",
    "    input_dim, output_dim = X_train.shape[1], Y_train.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "    velocity = None\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled, Y_shuffled = X_train[perm], Y_train[perm]\n",
    "\n",
    "        # Mini-batch iteration\n",
    "        if batch_size is None:\n",
    "            batches = [(X_shuffled, Y_shuffled)]\n",
    "        else:\n",
    "            batches = [(X_shuffled[i:i+batch_size], Y_shuffled[i:i+batch_size]) for i in range(0, X_shuffled.shape[0], batch_size)]\n",
    "\n",
    "        # Training by batch\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, Y_batch in batches:\n",
    "            A_out, cache, dropout_masks = forward_propagation(X_batch, params, activations, dropout_rates)\n",
    "            loss = cross_entropy_loss(Y_batch, A_out)\n",
    "            grads = backward_propagation(Y_batch, params, cache, activations, dropout_masks, dropout_rates, weight_decay)\n",
    "            params, velocity = update_parameters(params, grads, lr, optimizer_type, velocity)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        # Validation loss\n",
    "        A_val, _, _ = forward_propagation(X_val, params, activations, training=False)\n",
    "        val_loss = cross_entropy_loss(Y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(Y_val, A_val)\n",
    "        loss = epoch_loss / len(batches)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_params = params.copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    params = best_params\n",
    "                    break\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _, _ = forward_propagation(X, params, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Hyperparameter Optimization with Optuna\n",
    "# =======================================\n",
    "def objective(trial, loss_function=\"cross_entropy\", epochs=40):\n",
    "    # Hyperparameters to search\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
    "    hidden_layers = [trial.suggest_int(f\"n_units_l{i}\", 64, 256) for i in range(n_layers)]\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    dropout_rates = [trial.suggest_float(f\"dropout_l{i}\", 0.0, 0.5) for i in range(n_layers)]\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\"])\n",
    "    stopping_patience = trial.suggest_int(\"stopping_patience\", 3, 10)\n",
    "    # Activation functions per layer\n",
    "    possible_activations = [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\"]\n",
    "    activations = [trial.suggest_categorical(f\"activation_l{i}\", possible_activations) for i in range(n_layers)]\n",
    "    activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "    # Train network\n",
    "    params = train_neural_network(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        X_val, \n",
    "        y_val,\n",
    "        hidden_layers=hidden_layers,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        activations=activations,\n",
    "        loss_function=loss_function,\n",
    "        optimizer_type=optimizer_type,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_decay=weight_decay,\n",
    "        early_stopping=True,\n",
    "        patience=stopping_patience,\n",
    "        print_every=5\n",
    "    )\n",
    "\n",
    "    # Optuna tries to minimize the loss function\n",
    "    A_val, _, _ = forward_propagation(X_val, params, activations, training=False)\n",
    "    val_loss = cross_entropy_loss(y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(y_val, A_val)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "loss_function = \"cross_entropy\"\n",
    "epochs = 40\n",
    "obj = partial(objective, loss_function=loss_function, epochs=epochs)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(obj, n_trials=20)\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# =======================================\n",
    "# 11. Retrain Best Model on Train + Val and Evaluate on Test\n",
    "# =======================================\n",
    "\n",
    "best_params = trial.params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_layers = [best_params[f\"n_units_l{i}\"] for i in range(n_layers)]\n",
    "lr = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "dropout_rates = [best_params[f\"dropout_l{i}\"] for i in range(n_layers)]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "optimizer_type = best_params[\"optimizer\"]\n",
    "stopping_patience = best_params[\"stopping_patience\"]\n",
    "activations = [best_params[f\"activation_l{i}\"] for i in range(n_layers)]\n",
    "activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "# Combine train and val sets\n",
    "final_params = train_neural_network(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_val, \n",
    "    y_val,\n",
    "    hidden_layers=hidden_layers,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    activations=activations,\n",
    "    loss_function=loss_function,\n",
    "    optimizer_type=optimizer_type,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_decay=weight_decay,\n",
    "    early_stopping=True,\n",
    "    patience=stopping_patience,\n",
    "    print_every=5\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = predict(X_test, final_params, activations)\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48999, 784) (10500, 784) (10501, 784) float64\n",
      "(48999,) (10500,) (10501,) int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 06:51:35,759] A new study created in memory with name: no-name-192eaf03-113f-45b7-a0f0-48597f804bb8\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 1.9771 | Val Loss: 0.4133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 1.4341 | Val Loss: 0.3288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-10-30 06:59:45,571] Trial 0 failed with parameters: {'n_layers': 2, 'n_units_l0': 126, 'n_units_l1': 131, 'lr': 0.0018758684916965187, 'batch_size': 64, 'dropout_l0': 0.04287580447254946, 'dropout_l1': 0.018080365979805557, 'weight_decay': 0.006399417919831835, 'optimizer': 'momentum', 'stopping_patience': 3, 'activation_l0': 'leaky_relu', 'activation_l1': 'sigmoid', 'momentum': 0.8203598564607232, 'step_size': 14.181965456354856, 'gamma': 0.39242605819935683} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_56534/1141066968.py\", line 357, in objective\n",
      "    model = train_model(\n",
      "        X_train, y_train,\n",
      "    ...<3 lines>...\n",
      "        print_every=5, plot_loss=False\n",
      "    )\n",
      "  File \"/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_56534/1141066968.py\", line 248, in train_model\n",
      "    grads = tape.gradient(loss, model.trainable_variables)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/eager/backprop.py\", line 1066, in gradient\n",
      "    flat_grad = imperative_grad.imperative_grad(\n",
      "        self._tape,\n",
      "    ...<3 lines>...\n",
      "        sources_raw=flat_sources_raw,\n",
      "        unconnected_gradients=unconnected_gradients)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/eager/imperative_grad.py\", line 67, in imperative_grad\n",
      "    return pywrap_tfe.TFE_Py_TapeGradient(\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        tape._tape,  # pylint: disable=protected-access\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<3 lines>...\n",
      "        sources_raw,\n",
      "        ^^^^^^^^^^^^\n",
      "        compat.as_str(unconnected_gradients.value))\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/eager/backprop.py\", line 148, in _gradient_function\n",
      "    return grad_fn(mock_op, *out_grads)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/math_grad.py\", line 1507, in _DivNoNanGrad\n",
      "    return _ReduceGradientArgs(x, y, gx, gy)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/math_grad.py\", line 143, in _ReduceGradientArgs\n",
      "    gx = _ReduceGradientArg(gx, bx)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/math_grad.py\", line 135, in _ReduceGradientArg\n",
      "    grad = array_ops.reshape(grad, shape)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 88, in wrapper\n",
      "    return op(*args, **kwargs)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/util/dispatch.py\", line 1264, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/array_ops.py\", line 199, in reshape\n",
      "    result = gen_array_ops.reshape(tensor, shape, name)\n",
      "  File \"/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 8787, in reshape\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "      _ctx, \"Reshape\", name, tensor, shape)\n",
      "KeyboardInterrupt\n",
      "[W 2025-10-30 06:59:45,604] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_56534/1141066968.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    377\u001b[39m loss_function = \u001b[33m\"cross_entropy\"\u001b[39m\n\u001b[32m    378\u001b[39m epochs = \u001b[32m20\u001b[39m\n\u001b[32m    379\u001b[39m obj = partial(objective, loss_function=loss_function, epochs=epochs)\n\u001b[32m    380\u001b[39m study = optuna.create_study(direction=\u001b[33m\"minimize\"\u001b[39m, pruner=optuna.pruners.MedianPruner())\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m study.optimize(obj, n_trials=\u001b[32m3\u001b[39m)\n\u001b[32m    382\u001b[39m \n\u001b[32m    383\u001b[39m \u001b[38;5;66;03m# Best result summary\u001b[39;00m\n\u001b[32m    384\u001b[39m print(\u001b[33m\"Best trial:\"\u001b[39m)\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/optuna/study/study.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    486\u001b[39m         Raises:\n\u001b[32m    487\u001b[39m             RuntimeError:\n\u001b[32m    488\u001b[39m                 If nested invocation of this method occurs.\n\u001b[32m    489\u001b[39m         \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m         _optimize(\n\u001b[32m    491\u001b[39m             study=self,\n\u001b[32m    492\u001b[39m             func=func,\n\u001b[32m    493\u001b[39m             n_trials=n_trials,\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/optuna/study/_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    115\u001b[39m                         )\n\u001b[32m    116\u001b[39m                     )\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    118\u001b[39m         study._thread_local.in_optimize_loop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         progress_bar.close()\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/optuna/study/_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    162\u001b[39m             \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m             \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m             \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m             \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n\u001b[32m    167\u001b[39m                 gc.collect()\n\u001b[32m    168\u001b[39m \n\u001b[32m    169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/optuna/study/_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    254\u001b[39m         updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m func_err \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(func_err, catch)\n\u001b[32m    257\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/optuna/study/_optimize.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    254\u001b[39m         updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m func_err \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(func_err, catch)\n\u001b[32m    257\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[32m/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_56534/1141066968.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(trial, loss_function, epochs)\u001b[39m\n\u001b[32m    353\u001b[39m         patience=stopping_patience,\n\u001b[32m    354\u001b[39m         use_amp=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    355\u001b[39m     )\n\u001b[32m    356\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     model = train_model(\n\u001b[32m    358\u001b[39m         X_train, y_train,\n\u001b[32m    359\u001b[39m         X_val, y_val,\n\u001b[32m    360\u001b[39m         input_dim, output_dim,\n",
      "\u001b[32m/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_56534/1141066968.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, input_dim, output_dim, config, print_every, plot_loss)\u001b[39m\n\u001b[32m    244\u001b[39m                 loss = loss_fn(y_batch, logits)\n\u001b[32m    245\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m config.weight_decay > \u001b[32m0\u001b[39m:\n\u001b[32m    246\u001b[39m                     reg_loss = tf.add_n([tf.nn.l2_loss(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;28;01min\u001b[39;00m model.trainable_variables])\n\u001b[32m    247\u001b[39m                     loss += config.weight_decay * reg_loss\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m             grads = tape.gradient(loss, model.trainable_variables)\n\u001b[32m    249\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m config.clip_grad_norm:\n\u001b[32m    250\u001b[39m                 grads, _ = tf.clip_by_global_norm(grads, config.clip_grad_norm)\n\u001b[32m    251\u001b[39m             optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/eager/backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1062\u001b[39m               output_gradients))\n\u001b[32m   1063\u001b[39m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                           \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m output_gradients]\n\u001b[32m   1065\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     flat_grad = imperative_grad.imperative_grad(\n\u001b[32m   1067\u001b[39m         self._tape,\n\u001b[32m   1068\u001b[39m         flat_targets,\n\u001b[32m   1069\u001b[39m         flat_sources,\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ValueError:\n\u001b[32m     64\u001b[39m     raise ValueError(\n\u001b[32m     65\u001b[39m         \u001b[33m\"Unknown value for unconnected_gradients: %r\"\u001b[39m % unconnected_gradients)\n\u001b[32m     66\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[32m     68\u001b[39m       tape._tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m     69\u001b[39m       target,\n\u001b[32m     70\u001b[39m       sources,\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/eager/backprop.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    144\u001b[39m     gradient_name_scope = \u001b[33m\"gradient_tape/\"\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m forward_pass_name_scope:\n\u001b[32m    146\u001b[39m       gradient_name_scope += forward_pass_name_scope + \u001b[33m\"/\"\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n\u001b[32m    149\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/math_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m   1503\u001b[39m   x = math_ops.conj(op.inputs[\u001b[32m0\u001b[39m])\n\u001b[32m   1504\u001b[39m   y = math_ops.conj(op.inputs[\u001b[32m1\u001b[39m])\n\u001b[32m   1505\u001b[39m   gx = math_ops.div_no_nan(grad, y)\n\u001b[32m   1506\u001b[39m   gy = grad * math_ops.div_no_nan(math_ops.div_no_nan(-x, y), y)\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _ReduceGradientArgs(x, y, gx, gy)\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/math_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(x, y, gx, gy)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m _ReduceGradientArgs(x, y, gx, gy):\n\u001b[32m    140\u001b[39m   \u001b[33m\"\"\"Reduces gradients of both arguments of a broadcasting binary op.\"\"\"\u001b[39m\n\u001b[32m    141\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m gx \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mor\u001b[39;00m gy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    142\u001b[39m     bx, by = SmartBroadcastGradientArgs(x, y)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     gx = _ReduceGradientArg(gx, bx)\n\u001b[32m    144\u001b[39m     gy = _ReduceGradientArg(gy, by)\n\u001b[32m    145\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m gx, gy\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/math_grad.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(grad, shape_axes_must_reduce)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;66;03m# Applying keepdims=True in presence of unknown axes opens up some\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# opportunities for optimizations. For example, _SumGrad below won't have to\u001b[39;00m\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# emit extra ops to recover reduced indices for broadcasting.\u001b[39;00m\n\u001b[32m    134\u001b[39m     grad = math_ops.reduce_sum(grad, axes, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     grad = array_ops.reshape(grad, shape)\n\u001b[32m    136\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m wrapper(*args, **kwargs):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m op(*args, **kwargs)\n\u001b[32m     89\u001b[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m     bound_arguments.apply_defaults()\n\u001b[32m     91\u001b[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    153\u001b[39m       \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/util/dispatch.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1261\u001b[39m \n\u001b[32m   1262\u001b[39m       \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[32m   1263\u001b[39m       \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1264\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(*args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1265\u001b[39m       \u001b[38;5;28;01mexcept\u001b[39;00m (TypeError, ValueError):\n\u001b[32m   1266\u001b[39m         \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[32m   1267\u001b[39m         \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[32m   1268\u001b[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tensor, shape, name)\u001b[39m\n\u001b[32m    195\u001b[39m \n\u001b[32m    196\u001b[39m   Returns:\n\u001b[32m    197\u001b[39m     A `Tensor`. Has the same type \u001b[38;5;28;01mas\u001b[39;00m `tensor`.\n\u001b[32m    198\u001b[39m   \"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m   result = gen_array_ops.reshape(tensor, shape, name)\n\u001b[32m    200\u001b[39m   shape_util.maybe_set_static_shape(result, shape)\n\u001b[32m    201\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[32m~/.venvs/global_env/lib/python3.13/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(tensor, shape, name)\u001b[39m\n\u001b[32m   8788\u001b[39m         _ctx, \u001b[33m\"Reshape\"\u001b[39m, name, tensor, shape)\n\u001b[32m   8789\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   8790\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   8791\u001b[39m       _ops.raise_from_not_ok_status(e, name)\n\u001b[32m-> \u001b[39m\u001b[32m8792\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   8793\u001b[39m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   8794\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   8795\u001b[39m       return reshape_eager_fallback(\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network using Tensorflow with more complex features\n",
    "# Dataset: MNIST (Handwritten Digit Recognition)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from typing import List, Optional, Tuple\n",
    "from functools import partial\n",
    "import optuna\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "tf.random.set_seed(RNG_SEED)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape,  X_train.dtype)\n",
    "print(y_train.shape, y_val.shape, y_test.shape,  y_train.dtype)\n",
    "input_dim = int(X_train.shape[1])\n",
    "output_dim = int(len(np.unique(y_train)))\n",
    "show_images(X_train)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Tensor Creation\n",
    "# =======================================\n",
    "def df_to_tf_dataset(X, y, batch_size=64, shuffle=True):\n",
    "    X_tensor = tf.cast(X, tf.float32)\n",
    "    if y is not None:\n",
    "        y_tensor = tf.cast(y, tf.int32)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(X_tensor)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(X))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Activation function\n",
    "# =======================================\n",
    "_ACTIVATION_MAP = {\n",
    "    \"relu\": lambda: tf.keras.layers.ReLU(),\n",
    "    \"sigmoid\": lambda: tf.keras.layers.Activation(\"sigmoid\"),\n",
    "    \"tanh\": lambda: tf.keras.layers.Activation(\"tanh\"),\n",
    "    \"leaky_relu\": lambda: tf.keras.layers.LeakyReLU(negative_slope=0.01),\n",
    "    \"gelu\": lambda: tf.keras.layers.Activation(tf.nn.gelu),\n",
    "    \"elu\": lambda: tf.keras.layers.ELU(),\n",
    "    \"softmax_logit\": None  # handled separately for logits\n",
    "}\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Dataclass for hyperparameters\n",
    "# =======================================\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    hidden_layers: list[int] = None\n",
    "    activations: Optional[List[float]] = None\n",
    "    dropout_rates: Optional[List[float]] = None\n",
    "    loss_function: str = \"cross_entropy\"\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 100\n",
    "    weight_decay: float = 1e-5\n",
    "    momentum: float = 0.9\n",
    "    optimizer_type: str = \"adam\"\n",
    "    step_size: int = 20\n",
    "    gamma: float = 0.5\n",
    "    early_stopping: bool = True\n",
    "    patience: int = 10\n",
    "    clip_grad_norm: Optional[float] = 1.0\n",
    "    use_amp: bool = True  # automatic mixed precision\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Model Definition\n",
    "# =======================================\n",
    "class DeepNN(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_layers: List[int],\n",
    "        output_dim: int,\n",
    "        activations: Optional[List[str]] = None,\n",
    "        dropout_rates: Optional[List[float]] = None,\n",
    "        use_batchnorm: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers_list = []\n",
    "        n_hidden = len(hidden_layers)\n",
    "\n",
    "        if activations is None:\n",
    "            activations = [\"relu\"] * len(hidden_layers)\n",
    "        if dropout_rates is None:\n",
    "            dropout_rates = [0.0] * len(hidden_layers)\n",
    "        assert len(activations) == n_hidden\n",
    "        assert len(dropout_rates) == n_hidden\n",
    "\n",
    "        for idx, h in enumerate(hidden_layers):\n",
    "            act_name = activations[idx]\n",
    "            act_ctor = _ACTIVATION_MAP.get(act_name)\n",
    "            # Choose weight initializer based on activation\n",
    "            if act_name in (\"relu\", \"leaky_relu\"):\n",
    "                kernel_init = tf.keras.initializers.HeNormal()\n",
    "            else:\n",
    "                kernel_init = tf.keras.initializers.GlorotNormal()  # Xavier\n",
    "            # Dense layer\n",
    "            self.layers_list.append(tf.keras.layers.Dense(h, kernel_initializer=kernel_init, bias_initializer=\"zeros\"))\n",
    "            # Optional batch normalization\n",
    "            if use_batchnorm:\n",
    "                self.layers_list.append(tf.keras.layers.BatchNormalization())\n",
    "            # Activation\n",
    "            if act_ctor is not None:\n",
    "                self.layers_list.append(act_ctor() if callable(act_ctor) else act_ctor)\n",
    "            # Dropout\n",
    "            if dropout_rates[idx] and dropout_rates[idx] > 0:\n",
    "                self.layers_list.append(tf.keras.layers.Dropout(dropout_rates[idx]))\n",
    "\n",
    "        # Final linear layer â†’ logits (no activation)\n",
    "        self.layers_list.append(\n",
    "            tf.keras.layers.Dense(\n",
    "                output_dim,\n",
    "                kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
    "                bias_initializer=\"zeros\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x, training=training) if isinstance(layer, tf.keras.layers.Dropout) else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Training Function\n",
    "# =======================================\n",
    "def train_model(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val,\n",
    "    input_dim, output_dim,\n",
    "    config: TrainConfig,\n",
    "    print_every=10, plot_loss=False\n",
    "):  \n",
    "    # Dataloaders\n",
    "    train_dataset = df_to_tf_dataset(X_train, y_train, batch_size=config.batch_size)\n",
    "    val_dataset = df_to_tf_dataset(X_val, y_val, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "    # AMP policy\n",
    "    if config.use_amp:\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "    # Model creation\n",
    "    model = DeepNN(input_dim, config.hidden_layers, output_dim, config.activations, config.dropout_rates)\n",
    "\n",
    "    # Build the model (needed for weight loading later)\n",
    "    _ = model(tf.zeros((1, input_dim)), training=False)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=config.lr,\n",
    "        decay_steps=config.step_size * len(train_dataset),\n",
    "        decay_rate=config.gamma,\n",
    "        staircase=True,\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    if config.optimizer_type.lower() == \"adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    elif config.optimizer_type.lower() in (\"sgd\", \"momentum\"):\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=config.momentum)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer_type\")\n",
    "\n",
    "    # Loss\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Metrics\n",
    "    train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "    val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        # Training\n",
    "        train_loss_metric.reset_state()\n",
    "        for X_batch, y_batch in tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{config.epochs}\", leave=False):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(X_batch, training=True)\n",
    "                loss = loss_fn(y_batch, logits)\n",
    "                if config.weight_decay > 0:\n",
    "                    reg_loss = tf.add_n([tf.nn.l2_loss(v) for v in model.trainable_variables])\n",
    "                    loss += config.weight_decay * reg_loss\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            if config.clip_grad_norm:\n",
    "                grads, _ = tf.clip_by_global_norm(grads, config.clip_grad_norm)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            train_loss_metric.update_state(loss)\n",
    "\n",
    "        # Validation\n",
    "        val_loss_metric.reset_state()\n",
    "        for X_val_batch, y_val_batch in val_dataset:\n",
    "            logits_val = model(X_val_batch, training=False)\n",
    "            val_loss = loss_fn(y_val_batch, logits_val)\n",
    "            val_loss_metric.update_state(val_loss)\n",
    "\n",
    "        avg_train_loss = train_loss_metric.result().numpy()\n",
    "        avg_val_loss = val_loss_metric.result().numpy()\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config.epochs} - Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if config.early_stopping:\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                model.save_weights(\"best_model_1.weights.h5\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= config.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    # Later, reload the best model\n",
    "    if best_val_loss < np.inf:\n",
    "        model.load_weights(\"best_model_1.weights.h5\")\n",
    "\n",
    "    # Plot losses\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(train_losses, label=\"Train\")\n",
    "        plt.plot(val_losses, label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss over epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict_tf(model, X, y, batch_size=64): \n",
    "    ds = df_to_tf_dataset(X, y, batch_size=batch_size, shuffle=False) \n",
    "    probs_list = [] \n",
    "    for X_batch, _ in ds:\n",
    "        logits = model(X_batch, training=False) \n",
    "        probs = tf.nn.softmax(logits) \n",
    "        probs_list.append(probs) \n",
    "    probs = tf.concat(probs_list, axis=0) \n",
    "    preds = tf.argmax(probs, axis=1).numpy()\n",
    "    return preds, probs.numpy()\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Hyperparameter Optimization with Optuna\n",
    "# =======================================\n",
    "def objective(trial, loss_function=\"cross_entropy\", epochs=40):\n",
    "    # Hyperparameters to search\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
    "    hidden_layers = [trial.suggest_int(f\"n_units_l{i}\", 64, 256) for i in range(n_layers)]\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    dropout_rates = [trial.suggest_float(f\"dropout_l{i}\", 0.0, 0.5) for i in range(n_layers)]\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"adam\"])\n",
    "    stopping_patience = trial.suggest_int(\"stopping_patience\", 3, 10)\n",
    "    # Activation functions per layer\n",
    "    possible_activations = [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\"]\n",
    "    activations = [trial.suggest_categorical(f\"activation_l{i}\", possible_activations) for i in range(n_layers)]\n",
    "    momentum = 0.0\n",
    "    if optimizer_type in [\"sgd\", \"momentum\"]:\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.0, 0.9)\n",
    "    step_size = trial.suggest_float(\"step_size\", 10, 20, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.25, 0.5, log=True)\n",
    "\n",
    "    config = TrainConfig(\n",
    "        hidden_layers=hidden_layers, \n",
    "        activations=activations, \n",
    "        dropout_rates=dropout_rates, \n",
    "        loss_function=loss_function, \n",
    "        lr=lr, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs, \n",
    "        weight_decay=weight_decay, \n",
    "        momentum=momentum, \n",
    "        optimizer_type=optimizer_type, \n",
    "        step_size=step_size, \n",
    "        gamma=gamma,\n",
    "        early_stopping=True, \n",
    "        patience=stopping_patience, \n",
    "        use_amp=True\n",
    "    )\n",
    "\n",
    "    model = train_model(\n",
    "        X_train, y_train, \n",
    "        X_val, y_val,\n",
    "        input_dim, output_dim,\n",
    "        config,\n",
    "        print_every=5, plot_loss=False\n",
    "    )\n",
    "\n",
    "    # Validation loss\n",
    "    val_dataset = df_to_tf_dataset(X_val, y_val, batch_size=config.batch_size)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    val_loss_metric = tf.keras.metrics.Mean()\n",
    "    for X_batch, y_batch in val_dataset:\n",
    "        logits = model(X_batch, training=False)\n",
    "        val_loss_metric.update_state(loss_fn(y_batch, logits))\n",
    "\n",
    "    return val_loss_metric.result().numpy()\n",
    "\n",
    "\n",
    "# Run Optuna study\n",
    "loss_function = \"cross_entropy\"\n",
    "epochs = 20\n",
    "obj = partial(objective, loss_function=loss_function, epochs=epochs)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(obj, n_trials=3)\n",
    "\n",
    "# Best result summary\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Retrain Best Model on Train + Val and Evaluate on Test\n",
    "# =======================================\n",
    "best_params = trial.params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_layers = [best_params[f\"n_units_l{i}\"] for i in range(n_layers)]\n",
    "lr = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "dropout_rates = [best_params[f\"dropout_l{i}\"] for i in range(n_layers)]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "optimizer_type = best_params[\"optimizer\"]\n",
    "stopping_patience = best_params[\"stopping_patience\"]\n",
    "activations = [best_params[f\"activation_l{i}\"] for i in range(n_layers)]\n",
    "momentum = best_params.get(\"momentum\", 0.0)  # default 0.0 if not present\n",
    "step_size = best_params[\"step_size\"]\n",
    "gamma = best_params[\"gamma\"]\n",
    "\n",
    "config = TrainConfig(\n",
    "    hidden_layers=hidden_layers, \n",
    "    activations=activations, \n",
    "    dropout_rates=dropout_rates, \n",
    "    loss_function=loss_function, \n",
    "    lr=lr, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    weight_decay=weight_decay, \n",
    "    momentum=momentum, \n",
    "    optimizer_type=optimizer_type, \n",
    "    step_size=step_size, \n",
    "    gamma=gamma,\n",
    "    early_stopping=True, \n",
    "    patience=stopping_patience, \n",
    "    use_amp=True\n",
    "    )\n",
    "\n",
    "# Retrain final model\n",
    "best_model = train_model(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    input_dim, output_dim,\n",
    "    config,\n",
    "    print_every=5, plot_loss=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_test, _ = predict_tf(best_model, X_test, y_test, batch_size=batch_size)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomcuel/.venvs/global_env/lib/python3.13/site-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48999, 784) (10500, 784) (10501, 784) float64\n",
      "(48999,) (10500,) (10501,) int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:36:33,116] A new study created in memory with name: no-name-38a18c41-bb21-4646-b6f5-e5c92d7e9b0e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 0.9593 - Val Loss: 0.3160\n",
      "Epoch [6/40] | Train Loss: 0.2434 - Val Loss: 0.1752\n",
      "Epoch [11/40] | Train Loss: 0.1741 - Val Loss: 0.1582\n",
      "Epoch [16/40] | Train Loss: 0.1349 - Val Loss: 0.1513\n",
      "Epoch [21/40] | Train Loss: 0.1130 - Val Loss: 0.1491\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:37:07,409] Trial 0 finished with value: 0.15676858250010436 and parameters: {'n_layers': 5, 'n_units_l0': 223, 'n_units_l1': 124, 'n_units_l2': 178, 'n_units_l3': 255, 'n_units_l4': 116, 'lr': 0.09102656217646812, 'batch_size': 128, 'dropout_l0': 0.33083366308967976, 'dropout_l1': 0.2933541372217979, 'dropout_l2': 0.21242183079349852, 'dropout_l3': 0.40708322426199084, 'dropout_l4': 0.39990526064631104, 'weight_decay': 9.916433214533467e-05, 'optimizer': 'momentum', 'stopping_patience': 7, 'activation_l0': 'relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'relu', 'activation_l3': 'tanh', 'activation_l4': 'leaky_relu', 'momentum': 0.29396365905511856, 'step_size': 12.451280404655764, 'gamma': 0.42119169024423486}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 2.3739 - Val Loss: 1.3620\n",
      "Epoch [6/40] | Train Loss: 0.8260 - Val Loss: 0.5196\n",
      "Epoch [11/40] | Train Loss: 0.6287 - Val Loss: 0.4031\n",
      "Epoch [16/40] | Train Loss: 0.5432 - Val Loss: 0.3568\n",
      "Epoch [21/40] | Train Loss: 0.4867 - Val Loss: 0.3270\n",
      "Epoch [26/40] | Train Loss: 0.4507 - Val Loss: 0.3080\n",
      "Epoch [31/40] | Train Loss: 0.4208 - Val Loss: 0.2933\n",
      "Epoch [36/40] | Train Loss: 0.3992 - Val Loss: 0.2821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:37:59,124] Trial 1 finished with value: 0.2740970524634634 and parameters: {'n_layers': 2, 'n_units_l0': 251, 'n_units_l1': 245, 'lr': 0.0003158309947890911, 'batch_size': 64, 'dropout_l0': 0.4640519129254143, 'dropout_l1': 0.19097206167244263, 'weight_decay': 0.00041947169386070184, 'optimizer': 'momentum', 'stopping_patience': 8, 'activation_l0': 'tanh', 'activation_l1': 'relu', 'momentum': 0.5222619188673282, 'step_size': 19.183614372060394, 'gamma': 0.2920971888803626}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 1.1646 - Val Loss: 0.9318\n",
      "Epoch [6/40] | Train Loss: 1.8255 - Val Loss: 1.8536\n",
      "Epoch [11/40] | Train Loss: 2.3131 - Val Loss: 2.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:38:35,719] Trial 2 finished with value: 2.3018052574339367 and parameters: {'n_layers': 4, 'n_units_l0': 220, 'n_units_l1': 147, 'n_units_l2': 105, 'n_units_l3': 74, 'lr': 0.010986985614614971, 'batch_size': 32, 'dropout_l0': 0.3765890725144594, 'dropout_l1': 0.057825723086430414, 'dropout_l2': 0.22761615187612438, 'dropout_l3': 0.30699594795165736, 'weight_decay': 0.0003073115628282461, 'optimizer': 'adam', 'stopping_patience': 10, 'activation_l0': 'sigmoid', 'activation_l1': 'tanh', 'activation_l2': 'relu', 'activation_l3': 'sigmoid', 'step_size': 18.92060750797979, 'gamma': 0.3852167741677331}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.6319 - Val Loss: 0.3292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:38:43,909] Trial 3 finished with value: 0.36779238426436983 and parameters: {'n_layers': 4, 'n_units_l0': 207, 'n_units_l1': 76, 'n_units_l2': 68, 'n_units_l3': 225, 'lr': 0.00453981026860396, 'batch_size': 64, 'dropout_l0': 0.07303176424020696, 'dropout_l1': 0.4199375434535867, 'dropout_l2': 0.31630514365579304, 'dropout_l3': 0.245863818009753, 'weight_decay': 0.0035710602730241463, 'optimizer': 'adam', 'stopping_patience': 3, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'activation_l2': 'relu', 'activation_l3': 'tanh', 'step_size': 18.988363873266024, 'gamma': 0.2505325400023774}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 1.8614 - Val Loss: 2.0077\n",
      "Epoch [6/40] | Train Loss: 2.3238 - Val Loss: 2.3059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:39:21,354] Trial 4 finished with value: 2.303940666562035 and parameters: {'n_layers': 2, 'n_units_l0': 103, 'n_units_l1': 155, 'lr': 0.021647507888903992, 'batch_size': 16, 'dropout_l0': 0.15651867051018287, 'dropout_l1': 0.09487745061721709, 'weight_decay': 0.00035442598579540555, 'optimizer': 'adam', 'stopping_patience': 9, 'activation_l0': 'leaky_relu', 'activation_l1': 'leaky_relu', 'step_size': 18.311172993039072, 'gamma': 0.32079654572091504}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 1.4187 - Val Loss: 0.5217\n",
      "Epoch [6/40] | Train Loss: 0.4003 - Val Loss: 0.2829\n",
      "Epoch [11/40] | Train Loss: 0.3125 - Val Loss: 0.2414\n",
      "Epoch [16/40] | Train Loss: 0.2693 - Val Loss: 0.2167\n",
      "Epoch [21/40] | Train Loss: 0.2400 - Val Loss: 0.2029\n",
      "Epoch [26/40] | Train Loss: 0.2149 - Val Loss: 0.1890\n",
      "Epoch [31/40] | Train Loss: 0.2023 - Val Loss: 0.1804\n",
      "Epoch [36/40] | Train Loss: 0.1853 - Val Loss: 0.1745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:40:38,606] Trial 5 finished with value: 0.16933920469259223 and parameters: {'n_layers': 3, 'n_units_l0': 82, 'n_units_l1': 109, 'n_units_l2': 256, 'lr': 0.00027261697039411597, 'batch_size': 16, 'dropout_l0': 0.1880395310472533, 'dropout_l1': 0.12219891674648303, 'dropout_l2': 0.39423501396325145, 'weight_decay': 0.0003803032741126545, 'optimizer': 'sgd', 'stopping_patience': 3, 'activation_l0': 'sigmoid', 'activation_l1': 'leaky_relu', 'activation_l2': 'relu', 'momentum': 0.7263286999592395, 'step_size': 16.14391158572897, 'gamma': 0.35073772988350466}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 1.4530 - Val Loss: 0.6900\n",
      "Epoch [6/40] | Train Loss: 0.3908 - Val Loss: 0.3148\n",
      "Epoch [11/40] | Train Loss: 0.2953 - Val Loss: 0.2625\n",
      "Epoch [16/40] | Train Loss: 0.2549 - Val Loss: 0.2340\n",
      "Epoch [21/40] | Train Loss: 0.2246 - Val Loss: 0.2151\n",
      "Epoch [26/40] | Train Loss: 0.2075 - Val Loss: 0.2008\n",
      "Epoch [31/40] | Train Loss: 0.1880 - Val Loss: 0.1907\n",
      "Epoch [36/40] | Train Loss: 0.1775 - Val Loss: 0.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:41:00,230] Trial 6 finished with value: 0.17591704110020684 and parameters: {'n_layers': 2, 'n_units_l0': 179, 'n_units_l1': 232, 'lr': 0.0016549163069885166, 'batch_size': 128, 'dropout_l0': 0.0962737377689522, 'dropout_l1': 0.30749647065841135, 'weight_decay': 0.0028165519359605704, 'optimizer': 'momentum', 'stopping_patience': 10, 'activation_l0': 'tanh', 'activation_l1': 'relu', 'momentum': 0.6172368794069415, 'step_size': 11.603176847727065, 'gamma': 0.48004897396671725}. Best is trial 0 with value: 0.15676858250010436.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 1.7815 - Val Loss: 0.6685\n",
      "Epoch [6/40] | Train Loss: 0.3964 - Val Loss: 0.2620\n",
      "Epoch [11/40] | Train Loss: 0.2759 - Val Loss: 0.2096\n",
      "Epoch [16/40] | Train Loss: 0.2207 - Val Loss: 0.1796\n",
      "Epoch [21/40] | Train Loss: 0.1807 - Val Loss: 0.1669\n",
      "Epoch [26/40] | Train Loss: 0.1489 - Val Loss: 0.1572\n",
      "Epoch [31/40] | Train Loss: 0.1288 - Val Loss: 0.1458\n",
      "Epoch [36/40] | Train Loss: 0.1138 - Val Loss: 0.1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:41:31,957] Trial 7 finished with value: 0.14022537724660444 and parameters: {'n_layers': 5, 'n_units_l0': 112, 'n_units_l1': 195, 'n_units_l2': 204, 'n_units_l3': 162, 'n_units_l4': 146, 'lr': 0.015457518954012525, 'batch_size': 128, 'dropout_l0': 0.08357937152171618, 'dropout_l1': 0.47760216436641, 'dropout_l2': 0.3865986017257781, 'dropout_l3': 0.3958500438128876, 'dropout_l4': 0.11089503116842125, 'weight_decay': 0.001568571255087862, 'optimizer': 'sgd', 'stopping_patience': 6, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu', 'activation_l3': 'relu', 'activation_l4': 'tanh', 'momentum': 0.2973813548163341, 'step_size': 14.442925704845376, 'gamma': 0.46770811794083705}. Best is trial 7 with value: 0.14022537724660444.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 0.4631 - Val Loss: 0.2030\n",
      "Epoch [6/40] | Train Loss: 0.1408 - Val Loss: 0.1192\n",
      "Epoch [11/40] | Train Loss: 0.1054 - Val Loss: 0.1082\n",
      "Epoch [16/40] | Train Loss: 0.0889 - Val Loss: 0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:41:42,879] Trial 8 finished with value: 0.10757512406579085 and parameters: {'n_layers': 2, 'n_units_l0': 149, 'n_units_l1': 216, 'lr': 0.001639672455210421, 'batch_size': 128, 'dropout_l0': 0.4179105919108046, 'dropout_l1': 0.04806049705361376, 'weight_decay': 9.198054813151089e-06, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'relu', 'step_size': 17.88198021742477, 'gamma': 0.32209458902850857}. Best is trial 8 with value: 0.10757512406579085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.9318 - Val Loss: 0.2776\n",
      "Epoch [6/40] | Train Loss: 0.1623 - Val Loss: 0.1552\n",
      "Epoch [11/40] | Train Loss: 0.0902 - Val Loss: 0.1224\n",
      "Epoch [16/40] | Train Loss: 0.0600 - Val Loss: 0.1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:42:23,830] Trial 9 finished with value: 0.12065294774830856 and parameters: {'n_layers': 4, 'n_units_l0': 239, 'n_units_l1': 181, 'n_units_l2': 88, 'n_units_l3': 122, 'lr': 0.00027775933695993915, 'batch_size': 32, 'dropout_l0': 0.1980818534333078, 'dropout_l1': 0.13426064972367413, 'dropout_l2': 0.43881891892804087, 'dropout_l3': 0.3528665842393445, 'weight_decay': 2.8931016954746192e-05, 'optimizer': 'adam', 'stopping_patience': 4, 'activation_l0': 'tanh', 'activation_l1': 'leaky_relu', 'activation_l2': 'relu', 'activation_l3': 'sigmoid', 'step_size': 10.91463996799885, 'gamma': 0.31577674480438356}. Best is trial 8 with value: 0.10757512406579085.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.5409 - Val Loss: 0.2355\n",
      "Epoch [6/40] | Train Loss: 0.1717 - Val Loss: 0.1254\n",
      "Epoch [11/40] | Train Loss: 0.1289 - Val Loss: 0.1160\n",
      "Epoch [16/40] | Train Loss: 0.1102 - Val Loss: 0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:42:35,934] Trial 10 finished with value: 0.10281634139882712 and parameters: {'n_layers': 3, 'n_units_l0': 146, 'n_units_l1': 207, 'n_units_l2': 131, 'lr': 0.0013603185655136848, 'batch_size': 128, 'dropout_l0': 0.495729035920139, 'dropout_l1': 0.0010719553406374205, 'dropout_l2': 0.0013638101397459301, 'weight_decay': 2.1360941137468e-06, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'step_size': 16.092580229491617, 'gamma': 0.25868734234790897}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.5365 - Val Loss: 0.2160\n",
      "Epoch [6/40] | Train Loss: 0.1682 - Val Loss: 0.1316\n",
      "Epoch [11/40] | Train Loss: 0.1275 - Val Loss: 0.1173\n",
      "Epoch [16/40] | Train Loss: 0.1073 - Val Loss: 0.1143\n",
      "Epoch [21/40] | Train Loss: 0.0889 - Val Loss: 0.1123\n",
      "Epoch [26/40] | Train Loss: 0.0827 - Val Loss: 0.1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:42:55,177] Trial 11 finished with value: 0.11124813185442084 and parameters: {'n_layers': 3, 'n_units_l0': 143, 'n_units_l1': 211, 'n_units_l2': 135, 'lr': 0.0013059446917488924, 'batch_size': 128, 'dropout_l0': 0.4800314275941962, 'dropout_l1': 0.005393851153435339, 'dropout_l2': 0.005358935770730627, 'weight_decay': 1.693987579815385e-06, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'step_size': 15.79412373494475, 'gamma': 0.2503098508943653}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.4838 - Val Loss: 0.2066\n",
      "Epoch [6/40] | Train Loss: 0.1254 - Val Loss: 0.1363\n",
      "Epoch [11/40] | Train Loss: 0.0862 - Val Loss: 0.1192\n",
      "Epoch [16/40] | Train Loss: 0.0681 - Val Loss: 0.1226\n",
      "Epoch [21/40] | Train Loss: 0.0588 - Val Loss: 0.1129\n",
      "Epoch [26/40] | Train Loss: 0.0508 - Val Loss: 0.1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:43:26,437] Trial 12 finished with value: 0.11498816384260344 and parameters: {'n_layers': 3, 'n_units_l0': 159, 'n_units_l1': 215, 'n_units_l2': 141, 'lr': 0.0009181772710154324, 'batch_size': 128, 'dropout_l0': 0.3756541429968591, 'dropout_l1': 0.01851070709541118, 'dropout_l2': 0.004180432402818235, 'weight_decay': 2.359858577129683e-06, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'step_size': 16.16637178214209, 'gamma': 0.280541363886417}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.4218 - Val Loss: 0.1955\n",
      "Epoch [6/40] | Train Loss: 0.1634 - Val Loss: 0.1567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:43:36,350] Trial 13 finished with value: 0.13738839817840684 and parameters: {'n_layers': 2, 'n_units_l0': 135, 'n_units_l1': 253, 'lr': 0.0033170742141677484, 'batch_size': 128, 'dropout_l0': 0.29837517805422437, 'dropout_l1': 0.20516771200385317, 'weight_decay': 8.745124347489059e-06, 'optimizer': 'adam', 'stopping_patience': 6, 'activation_l0': 'relu', 'activation_l1': 'relu', 'step_size': 13.830243244134461, 'gamma': 0.2818277532055026}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 1.1636 - Val Loss: 0.4220\n",
      "Epoch [6/40] | Train Loss: 0.2771 - Val Loss: 0.1945\n",
      "Epoch [11/40] | Train Loss: 0.1992 - Val Loss: 0.1563\n",
      "Epoch [16/40] | Train Loss: 0.1566 - Val Loss: 0.1358\n",
      "Epoch [21/40] | Train Loss: 0.1267 - Val Loss: 0.1239\n",
      "Epoch [26/40] | Train Loss: 0.1058 - Val Loss: 0.1191\n",
      "Epoch [31/40] | Train Loss: 0.0909 - Val Loss: 0.1158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:44:20,095] Trial 14 finished with value: 0.11968121102590867 and parameters: {'n_layers': 3, 'n_units_l0': 182, 'n_units_l1': 180, 'n_units_l2': 195, 'lr': 0.00011825563287137705, 'batch_size': 128, 'dropout_l0': 0.42396447118527714, 'dropout_l1': 0.004279181004458659, 'dropout_l2': 0.09940693276998108, 'weight_decay': 7.233032835353958e-06, 'optimizer': 'adam', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'sigmoid', 'activation_l2': 'sigmoid', 'step_size': 17.44308399633098, 'gamma': 0.3551778222971411}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 2.6061 - Val Loss: 1.7170\n",
      "Epoch [6/40] | Train Loss: 1.1196 - Val Loss: 0.7049\n",
      "Epoch [11/40] | Train Loss: 0.8721 - Val Loss: 0.5211\n",
      "Epoch [16/40] | Train Loss: 0.7468 - Val Loss: 0.4420\n",
      "Epoch [21/40] | Train Loss: 0.6725 - Val Loss: 0.3988\n",
      "Epoch [26/40] | Train Loss: 0.6239 - Val Loss: 0.3713\n",
      "Epoch [31/40] | Train Loss: 0.5869 - Val Loss: 0.3514\n",
      "Epoch [36/40] | Train Loss: 0.5496 - Val Loss: 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:44:42,246] Trial 15 finished with value: 0.32733277648971193 and parameters: {'n_layers': 2, 'n_units_l0': 65, 'n_units_l1': 220, 'lr': 0.0008620855514526266, 'batch_size': 128, 'dropout_l0': 0.4887424546051916, 'dropout_l1': 0.08629969050380415, 'weight_decay': 1.0451221530918017e-05, 'optimizer': 'sgd', 'stopping_patience': 7, 'activation_l0': 'relu', 'activation_l1': 'relu', 'momentum': 0.06664924360026875, 'step_size': 14.579665200222875, 'gamma': 0.31286995476059104}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 0.7052 - Val Loss: 0.4429\n",
      "Epoch [6/40] | Train Loss: 0.6081 - Val Loss: 0.4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:44:59,802] Trial 16 finished with value: 0.4315361321777815 and parameters: {'n_layers': 3, 'n_units_l0': 133, 'n_units_l1': 194, 'n_units_l2': 239, 'lr': 0.006119630901321763, 'batch_size': 32, 'dropout_l0': 0.4057746245247604, 'dropout_l1': 0.17374149329722105, 'dropout_l2': 0.10652013388559778, 'weight_decay': 3.511697134279608e-05, 'optimizer': 'adam', 'stopping_patience': 4, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'tanh', 'step_size': 17.032339709041928, 'gamma': 0.26515241695201747}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 0.3067 - Val Loss: 0.1691\n",
      "Epoch [6/40] | Train Loss: 0.0507 - Val Loss: 0.1490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:45:35,197] Trial 17 finished with value: 0.15640133717696847 and parameters: {'n_layers': 2, 'n_units_l0': 174, 'n_units_l1': 168, 'lr': 0.0005518900913598941, 'batch_size': 16, 'dropout_l0': 0.0026731847986191104, 'dropout_l1': 0.25317163363187306, 'weight_decay': 2.8694027373793036e-06, 'optimizer': 'adam', 'stopping_patience': 6, 'activation_l0': 'relu', 'activation_l1': 'relu', 'step_size': 13.192487432153165, 'gamma': 0.3561452441450658}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch [1/40] | Train Loss: 1.0141 - Val Loss: 0.3631\n",
      "Epoch [6/40] | Train Loss: 0.3156 - Val Loss: 0.2150\n",
      "Epoch [11/40] | Train Loss: 0.2450 - Val Loss: 0.1852\n",
      "Epoch [16/40] | Train Loss: 0.2081 - Val Loss: 0.1680\n",
      "Epoch [21/40] | Train Loss: 0.1843 - Val Loss: 0.1555\n",
      "Epoch [26/40] | Train Loss: 0.1597 - Val Loss: 0.1487\n",
      "Epoch [31/40] | Train Loss: 0.1476 - Val Loss: 0.1435\n",
      "Epoch [36/40] | Train Loss: 0.1355 - Val Loss: 0.1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:46:16,154] Trial 18 finished with value: 0.14083777722538937 and parameters: {'n_layers': 3, 'n_units_l0': 112, 'n_units_l1': 237, 'n_units_l2': 122, 'lr': 0.0020735605073615325, 'batch_size': 64, 'dropout_l0': 0.2827584265280495, 'dropout_l1': 0.3677916413200983, 'dropout_l2': 0.1263104930943331, 'weight_decay': 1.0085793149460042e-06, 'optimizer': 'sgd', 'stopping_patience': 4, 'activation_l0': 'leaky_relu', 'activation_l1': 'sigmoid', 'activation_l2': 'leaky_relu', 'momentum': 0.8413901402466064, 'step_size': 15.187302498591649, 'gamma': 0.40050444824357767}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] | Train Loss: 1.0661 - Val Loss: 0.3661\n",
      "Epoch [6/40] | Train Loss: 0.3283 - Val Loss: 0.2191\n",
      "Epoch [11/40] | Train Loss: 0.2496 - Val Loss: 0.1845\n",
      "Epoch [16/40] | Train Loss: 0.2070 - Val Loss: 0.1594\n",
      "Epoch [21/40] | Train Loss: 0.1810 - Val Loss: 0.1527\n",
      "Epoch [26/40] | Train Loss: 0.1657 - Val Loss: 0.1467\n",
      "Epoch [31/40] | Train Loss: 0.1473 - Val Loss: 0.1432\n",
      "Epoch [36/40] | Train Loss: 0.1309 - Val Loss: 0.1405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 16:46:54,145] Trial 19 finished with value: 0.1441183645275111 and parameters: {'n_layers': 4, 'n_units_l0': 154, 'n_units_l1': 200, 'n_units_l2': 163, 'n_units_l3': 182, 'lr': 0.0380180586829731, 'batch_size': 128, 'dropout_l0': 0.4464054244011487, 'dropout_l1': 0.05971057541892137, 'dropout_l2': 0.4946363327282432, 'dropout_l3': 0.0006278017589415319, 'weight_decay': 3.164030023244018e-05, 'optimizer': 'momentum', 'stopping_patience': 8, 'activation_l0': 'relu', 'activation_l1': 'tanh', 'activation_l2': 'sigmoid', 'activation_l3': 'leaky_relu', 'momentum': 0.009278289274410934, 'step_size': 17.252638570108537, 'gamma': 0.2959000498895063}. Best is trial 10 with value: 0.10281634139882712.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value: 0.1028\n",
      "  Params:\n",
      "    n_layers: 3\n",
      "    n_units_l0: 146\n",
      "    n_units_l1: 207\n",
      "    n_units_l2: 131\n",
      "    lr: 0.0013603185655136848\n",
      "    batch_size: 128\n",
      "    dropout_l0: 0.495729035920139\n",
      "    dropout_l1: 0.0010719553406374205\n",
      "    dropout_l2: 0.0013638101397459301\n",
      "    weight_decay: 2.1360941137468e-06\n",
      "    optimizer: adam\n",
      "    stopping_patience: 5\n",
      "    activation_l0: relu\n",
      "    activation_l1: relu\n",
      "    activation_l2: tanh\n",
      "    step_size: 16.092580229491617\n",
      "    gamma: 0.25868734234790897\n",
      "Epoch [1/50] | Train Loss: 0.5344 - Val Loss: 0.2288\n",
      "Epoch [6/50] | Train Loss: 0.1725 - Val Loss: 0.1295\n",
      "Epoch [11/50] | Train Loss: 0.1315 - Val Loss: 0.1224\n",
      "Epoch [16/50] | Train Loss: 0.1086 - Val Loss: 0.1152\n",
      "Epoch [21/50] | Train Loss: 0.0947 - Val Loss: 0.1193\n",
      "Epoch [26/50] | Train Loss: 0.0874 - Val Loss: 0.1027\n",
      "Early stopping triggered.\n",
      "Test Accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network using PyTorch\n",
    "# Dataset: MNIST (Handwritten Digit Recognition)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from functools import partial\n",
    "import optuna\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# Choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "torch.manual_seed(RNG_SEED)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape,  X_train.dtype)\n",
    "print(y_train.shape, y_val.shape, y_test.shape,  y_train.dtype)\n",
    "input_dim = int(X_train.shape[1])\n",
    "output_dim = int(len(np.unique(y_train)))\n",
    "show_images(X_train)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Tensor Creation\n",
    "# =======================================\n",
    "def df_to_tensor_dataset(X,y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "    return TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataset = df_to_tensor_dataset(X_train, y_train)\n",
    "val_dataset = df_to_tensor_dataset(X_val, y_val)\n",
    "test_dataset = df_to_tensor_dataset(X_test, y_test)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Activation function\n",
    "# -------------------------\n",
    "_ACTIVATION_MAP = {\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"sigmoid\": nn.Sigmoid,\n",
    "    \"tanh\": nn.Tanh,\n",
    "    \"leaky_relu\": lambda: nn.LeakyReLU(negative_slope=0.01),\n",
    "    \"softmax_logit\": None  # handled as final linear (CrossEntropy expects logits)\n",
    "}\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Model Definition\n",
    "# =======================================\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_layers: List[int],\n",
    "        output_dim: int,\n",
    "        activations: Optional[List[str]] = None,\n",
    "        dropout_rates: Optional[List[float]] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        n_hidden = len(hidden_layers)\n",
    "\n",
    "        if activations is None:\n",
    "            activations = [\"relu\"] * n_hidden\n",
    "        if dropout_rates is None:\n",
    "            dropout_rates = [0.0] * n_hidden\n",
    "\n",
    "        assert len(activations) == n_hidden\n",
    "        assert len(dropout_rates) == n_hidden\n",
    "\n",
    "        for idx, h in enumerate(hidden_layers):\n",
    "            layers.append(nn.Linear(prev, h))\n",
    "            act_name = activations[idx]\n",
    "            act_ctor = _ACTIVATION_MAP.get(act_name)\n",
    "            if act_ctor is None and act_name != \"softmax_logit\":\n",
    "                raise ValueError(f\"Unsupported activation: {act_name}\")\n",
    "            if act_name != \"softmax_logit\":\n",
    "                layers.append(act_ctor())\n",
    "            if dropout_rates[idx] and dropout_rates[idx] > 0:\n",
    "                layers.append(nn.Dropout(dropout_rates[idx]))\n",
    "            prev = h\n",
    "\n",
    "        # final linear -> logits\n",
    "        layers.append(nn.Linear(prev, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        # init weights: He for relu, Xavier otherwise\n",
    "        self._init_weights(activations)\n",
    "\n",
    "    def _init_weights(self, activations: List[str]):\n",
    "        # He init for ReLU / LeakyReLU; Xavier for others\n",
    "        idx_hidden = 0\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # choose activation of the incoming layer index\n",
    "                act = activations[idx_hidden] if idx_hidden < len(activations) else None\n",
    "                if act in (\"relu\", \"leaky_relu\"):\n",
    "                    nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                else:\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "                idx_hidden += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x expected shape: (batch, input_dim)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    hidden_layers=[128, 64], \n",
    "    lr=0.01, \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    momentum=0.9,\n",
    "    step_size=15, \n",
    "    gamma=0.3,\n",
    "    dropout_rates=[0.1, 0.1], \n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=5, \n",
    "    print_every=5, \n",
    "    plot_loss=True\n",
    "):  \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = DeepNN(input_dim, hidden_layers, output_dim, activations, dropout_rates)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_type.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif optimizer_type.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_type.lower() == \"momentum\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer_type\")\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss() if loss_function == \"cross_entropy\" else nn.MSELoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "                y_val_batch = y_val_batch.to(device)\n",
    "                outputs_val = model(X_val_batch)\n",
    "                loss_val = criterion(outputs_val, y_val_batch)\n",
    "                val_loss += loss_val.item() * X_val_batch.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        scheduler.step()\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "         # Early stopping\n",
    "        if early_stopping:\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(train_losses, label=\"Train\")\n",
    "        plt.plot(val_losses, label=\"Val\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Prediction & Evaluation\n",
    "# =======================================\n",
    "# Prediction function for any DataLoader\n",
    "def predict_loader(model, loader, device=None):\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    all_probs, all_preds, all_targets = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(y_batch.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    return all_preds, all_probs, all_targets\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Hyperparameter Optimization with Optuna\n",
    "# =======================================\n",
    "def objective(trial, loss_function=\"cross_entropy\", epochs=40):\n",
    "    # Hyperparameters to search\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
    "    hidden_layers = [trial.suggest_int(f\"n_units_l{i}\", 64, 256) for i in range(n_layers)]\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    dropout_rates = [trial.suggest_float(f\"dropout_l{i}\", 0.0, 0.5) for i in range(n_layers)]\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"adam\"])\n",
    "    stopping_patience = trial.suggest_int(\"stopping_patience\", 3, 10)\n",
    "    # Activation functions per layer\n",
    "    possible_activations = [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\"]\n",
    "    activations = [trial.suggest_categorical(f\"activation_l{i}\", possible_activations) for i in range(n_layers)]\n",
    "    activations.append(\"softmax\")  # output layer always softmax\n",
    "    momentum = 0.0\n",
    "    if optimizer_type in [\"sgd\", \"momentum\"]:\n",
    "        momentum = trial.suggest_float(\"momentum\", 0.0, 0.9)\n",
    "    step_size = trial.suggest_float(\"step_size\", 10, 20, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.25, 0.5, log=True)\n",
    "\n",
    "    model = train_neural_network(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        lr=lr, \n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        activations=None, \n",
    "        loss_function=\"cross_entropy\",\n",
    "        optimizer_type=optimizer_type,\n",
    "        momentum=momentum,\n",
    "        step_size=step_size,\n",
    "        gamma=gamma,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_decay=weight_decay, \n",
    "        early_stopping=True, \n",
    "        patience=stopping_patience, \n",
    "        print_every=5, \n",
    "        plot_loss=False\n",
    "    )\n",
    "\n",
    "    # Validation loss\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss() if loss_function == \"cross_entropy\" else nn.MSELoss()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            logits = model(X_val_batch)\n",
    "            val_loss += criterion(logits, y_val_batch).item() * X_val_batch.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "loss_function = \"cross_entropy\"\n",
    "epochs = 40\n",
    "obj = partial(objective, loss_function=loss_function, epochs=epochs)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(obj, n_trials=20)\n",
    "\n",
    "# Best result summary\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Retrain Best Model on Train + Val and Evaluate on Test\n",
    "# =======================================\n",
    "best_params = trial.params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_layers = [best_params[f\"n_units_l{i}\"] for i in range(n_layers)]\n",
    "lr = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "dropout_rates = [best_params[f\"dropout_l{i}\"] for i in range(n_layers)]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "optimizer_type = best_params[\"optimizer\"]\n",
    "stopping_patience = best_params[\"stopping_patience\"]\n",
    "activations = [best_params[f\"activation_l{i}\"] for i in range(n_layers)]\n",
    "activations.append(\"softmax\")  # output layer always softmax\n",
    "momentum = best_params.get(\"momentum\", 0.0)  # default 0.0 if not present\n",
    "step_size = best_params[\"step_size\"]\n",
    "gamma = best_params[\"gamma\"]\n",
    "\n",
    "# Retrain final model\n",
    "best_model = train_neural_network(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    hidden_layers=hidden_layers,\n",
    "    lr=lr, \n",
    "    epochs=50,\n",
    "    batch_size=batch_size,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=optimizer_type,\n",
    "    momentum=momentum,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_decay=weight_decay, \n",
    "    early_stopping=True, \n",
    "    patience=stopping_patience, \n",
    "    print_every=5, \n",
    "    plot_loss=False\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "y_pred_test, _, _ = predict_loader(best_model, test_loader, device)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

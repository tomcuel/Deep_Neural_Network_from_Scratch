{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f585f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Preprocessing data...\n",
      "(48999, 784) (10500, 784) (10501, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAFECAYAAACNjDBvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFLtJREFUeJzt3X/MVmX9B/Bz48OihjPocZYS4gJhGoTUZg2a/OGKlRTVlrom08lUaKRh02GhsxytWavQoq1arvpDp5uopDnbEmO1WVEs7IHUmZr8geBj4MRYeX/nvn/EzfXkc3Oec+7P+fF6/Xc+u885133dn3jenl1Xp9PtdrsZAAAwcJMGf0sAAOANwjgAAAQRxgEAIIgwDgAAQYRxAAAIIowDAEAQYRwAAIII4wAAEGSo3w92Op1yR0KtTORdUXqJo+kliqKXKIpeYpC95Mk4AAAEEcYBACCIMA4AAEGEcQAACCKMAwBAEGEcAACCCOMAABBEGAcAgCDCOAAABBHGAQAgiDAOAABBhHEAAAgijAMAQBBhHAAAggjjAAAQRBgHAIAgwjgAAAQRxgEAIMhQ1I3bptvt5jqv0+kUPhbq7YEHHkhqF1xwQa5r6S8AiOXJOAAABBHGAQAgiDAOAABBhHEAAAjS6fa5s9BGr/I3a77htNNO6zneu3dvASOq1nfUS3Fzf6w1a9Yktc2bN2eDpJcGM4d/+tOfktqiRYuyJtFLFEUvVX+eL7roop7ju+66K6vrd/RkHAAAggjjAAAQRBgHAIAg1oyX4POf/3xSu/322/s69+WXX+45njZtWlZF1tM1Y834xRdfnNTuvPPObJD0Utwczpw5M6k9//zzWV3pJYqil+r396xT0Xm3ZhwAACpMGAcAgCDCOAAABBHGAQAgiA2cgRsSnn766aQ2e/bsrA5sbhmMCy+8cOAbLAf9++iluDls2vzppWq54oorkto555zTc3zVVVeVOoa8v2sbeunIkSNJbfLkyUntvvvuS2orVqwoZAwvvPBCUjv11FP7Onfr1q09x8uXL8+qyAZOAACoMGEcAACCCOMAABBEGAcAgCBDUTduktHR0XE/c/DgwaR26aWXljQimuL8888v9fqnnXZaqden2pYsWZLUtm/fHjIW+rNw4cKkdsYZZyS1d7/73Unthhtu6Dl+7bXXks+cfvrpEx4j9fh7MtZmzbF88pOfLGlEWXbKKafkPvftb3971hSejAMAQBBhHAAAggjjAAAQxJrxAtYo9bNu6a677kpq1mZyrK9+9as9x6tWrSrs2o8//nhS27t3b2HXp37GesmYf5fiTORFM0XZtWtXUnv00Ud7jg8dOpR85p577klqO3bsGPd+c+fOTWq7d+/uY6Qcr0ceeSSrmhNOOCH3ue9617uypvBkHAAAggjjAAAQRBgHAIAgwjgAAASxgfM45d1Y8sorrxQ+Fppnw4YNpV373HPPLe3a1FOTXprRZl/84heT2ne+852syX9Tb7rppnE3wFM9nU6nsGu95z3vyZrCk3EAAAgijAMAQBBhHAAAggjjAAAQpNPt85VfRS66r4szzzwzqe3ZsyfXtZo2fxN5U1zT5iKv3//+90ntAx/4QGHX/+1vf9tzvHjx4qyK9NLxmTp1alIb642IbZy/uvdSP+OvwjgjfqPp06f3HI+OjmZlqnsvlf2G16K+YxXHVLR+vqMn4wAAEEQYBwCAIMI4AAAEEcYBACCIDZwlbCxow1w1cXNLmebNm5fURkZGSr1nXeZZL02cf6ua0UubNm1KamvXrq3cOMv+PcZ6M+w///nPbJDq3kt12SxZxTEVzQZOAACoMGEcAACCCOMAABDEmvE3cfjw4aQ2ZcqUcc9rw1w1cT1dmYpcFzeWBx98MKl9/OMfz+pAL02cNePN7aX9+/f3HA8PDxd27fPPPz+p/epXv8p1rT/+8Y9JbdGiRbWd97r30oIFC5Lazp07C7u+NeP9s2YcAAAqTBgHAIAgwjgAAAQRxgEAIMhQ1I3roJ/NmjCWX/ziFwO934YNGwZ6P+J87Wtfy3XeLbfcUvhYKF+RGzaL2qz53ve+N9dmzTdccsklue7J8Xn11VfDNyX+8pe/TGonnXRSYWMYKfnFeYPkyTgAAAQRxgEAIIgwDgAAQYRxAAAIYgPnUebNm5frvNtuu63wsVAfGzduTGof+9jHCrn23/72t6Q2d+7cQq5NPX3lK1/Jdd6RI0cKHwvt9Je//KWvz23dujWp/fznPy9hRAz6rc/9WLZsWanXv+aaa7Km8GQcAACCCOMAABBEGAcAgCDWjBfwfyB/4oknFj4W6mP9+vWlXfstb3lLademXfpd5wt51h8/99xzSW358uUljYjxDA01P951K7AuviiejAMAQBBhHAAAggjjAAAQRBgHAIAgzV/h/z/ce++9uc57+umnk9pll11WwIiog7I3jNx55509xxdffHGp96M9tmzZEj0EGvJv3K5du5La/PnzSxoReezZsydruq4NnAAAwEQJ4wAAEEQYBwCAIMI4AAAEae0GzhUrVuQ679lnny18LFTTvn37Bn5PGzY51vve977oIdDyDXDH/t2zWbOeTj/99J7jm266KfnMzTff3NfbVfOaNm1az/FLL72U+1qdTidrCk/GAQAgiDAOAABBhHEAAAgijAMAQJBWbOC8//77C7vWokWLCrsW1fKTn/yk5/jkk08u9X5N2nxCea688spc57344ouFj4V662fD5uuvv57UZs2aVdKIGKRjN2JefvnlAx/D5MmTC7vW1KlTs6bwZBwAAIII4wAAEEQYBwCAIK1YM758+fLCrnXCCScUdi3iXHvttUnt0ksvLe1+1oeT14033thzvHr16r7OO3DgQEkjokkv9DmWv3GUacOGDYVda86cOVlTeDIOAABBhHEAAAgijAMAQBBhHAAAgrRiA2eR1q1bFz0ECrBy5cpSr7958+ZSr0977N+/P9d5r776auFjoZqeeOKJXOfZWE6dLV68OGsKT8YBACCIMA4AAEGEcQAACCKMAwBAEBs438SnPvWppLZly5aQsVCskZGRpLZgwYJc19qxY0dSW7NmTa5rQVE+9KEPRQ+BElx55ZVJ7ayzzipsw+aMGTOS2j/+8Y8+Rwdvbnh4uLBrfeITn8iawpNxAAAIIowDAEAQYRwAAIJ0ut1ut+kvB+jzKyZOOumkpHbw4MECRlR/eee0yr20cePGnuP169fX+vvURRN7qQrz9dnPfrbn+O67786arg29NJHvuHfv3p7jU089NfnMrbfemtSuu+66rG3a0EsRTj755J7jffv2FXbtqs57P73kyTgAAAQRxgEAIIgwDgAAQYRxAAAI0ooNnBTP5haKopeOz1gveHniiSeS2ujoaM/x9OnTs6ZrYi9N5DuN5+yzz05qf/3rX0u7X500sZeqYN68eeO+gK9p824DJwAAVJgwDgAAQYRxAAAIIowDAEAQGzjJxeYWiqKXKErde6nMzZpvWLhwYc/xzp07S71fndW9l6gOGzgBAKDChHEAAAgijAMAQBBhHAAAggxF3RgAKIdNhFAfnowDAEAQYRwAAIII4wAAEMSacQCoAOu8oZ08GQcAgCDCOAAABBHGAQAgiDAOAABBOt1utxt1cwAAaDNPxgEAIIgwDgAAQYRxAAAIIowDAEAQYRwAAIII4wAAEEQYBwCAIMI4AAAEEcYBACCIMA4AAEGEcQAACCKMAwBAEGEcAACCCOMAABBEGAcAgCDCOAAABBHGAQAgiDAOAABBhHEAAAgijAMAQBBhHAAAggjjAAAQRBgHAIAgwjgAAAQRxgEAIIgwDgAAQYRxAAAIIowDAEAQYRwAAIII4wAAEEQYBwCAIMI4AAAEEcYBACCIMA4AAEGEcQAACCKMAwBAEGEcAACCCOMAABBEGAcAgCDCOAAABBnq94OdTqfckVAr3W4397l6iaPpJYqilyiKXmKQveTJOAAABBHGAQAgiDAOAABBhHEAAAgijAMAQBBhHAAAggjjAAAQRBgHAIAgwjgAAAQRxgEAIIgwDgAAQYRxAAAIIowDAEAQYRwAAIII4wAAEEQYBwCAIMI4AAAEEcYBACDIUNSNAaiez3zmM0ntnnvuGfe8TqdT0ogAms2TcQAACCKMAwBAEGEcAACCCOMAABCk0+12u3190OYcjtJn24xJL3E0vdSM3+Pqq69Oaps2bcoGqU69NJGxFuWGG25Ial//+tdDxlI1deol6t9LnowDAEAQYRwAAIII4wAAEMSacXKxnm7i1q5dW9ga2x//+MdJbdWqVVkd6KVm/B5V+C2q0ktVWA+eVxV+xyqoSi9Rf9aMAwBAhQnjAAAQRBgHAIAgwjgAAAQZyhpmrI0Tr7/+el/nHjhwoOd4eHi4sHHRLg899FDP8bJly/o6b926dT3Hkyal/738rW99q68NIlu3bu053rJlS19jgPE888wz0UNopJGRkaR21llnFXb9Om8spZm/65IlS/r692Xv3r1Zk3kyDgAAQYRxAAAIIowDAEAQYRwAAII07g2cZW9kqOo83HfffT3H119/ffKZ3bt3F3a/tr6dbMGCBUlt586d4543ffr0pDY6OlrYuOqsrb1UBS+99FJSmzZtWm3nvSq91M84Iuawn3HNnz8/qe3atStrm6r0UtM3cE5Ep6L/Dh3LGzgBAKDChHEAAAgijAMAQJDarxlv61qpouT9Xeu0ni6vD37wg0ntd7/7XaO+YxW0oZfqPvf//ve/e44nT56cVVFVeqnOa8b9b6pavdSPH/3oR0nt8ssvT2pz5szpOX7qqaeyaBs3bkxq69evz7VXbsWKFVkVWTMOAAAVJowDAEAQYRwAAIII4wAAEKRWGziffPLJpDZ79uxc1zrllFOS2r59+1q3OdQGzol/xyp8nx07diS1mTNn9hwPDw9nVdSGXqrTPP/rX/9KalOmTMnqQC8d31z84Ac/SGqrV68uaUT1opfifOQjH0lqDz/8cG3n3QZOAACoMGEcAACCCOMAABBEGAcAgCC12sCZd0NFFcberzI3jc6YMSOpvfDCC7mu1cTNLXV+O13Tx16371QFx254GmtT1FjOOeecpPbnP/85q4M29NKhQ4eS2tSpUxv9nSO0oZeqqtuwrGcDJwAAVJgwDgAAQYRxAAAIIowDAECQoaxhqrqAfyx79uwp7FrnnXdez/Fjjz1W2LWpltHR0cb9b4GJ+c1vfpPUlixZMu55a9eure1mzbY4cOBAIZs1+91Y9tprryW1lStX9hzffffdhY0B8GQcAADCCOMAABBEGAcAgCC1eulPnX35y19Oarfcckuua336059Oavfee282SG14IUK/37HI7zNlypSe48OHD+e+VtPmuc7fsWx1fulTkdrQS2W+GG4ibrvttqT2hS98IaurNvRSVb344otJbXh4uLbz7qU/AABQYcI4AAAEEcYBACCIMA4AAEFs4CzB4sWLk9r27dv7Onfbtm09x0uXLs2qqA2bW975zncmtfXr1ye1z33uc0ntHe94x7jXH6snVq1aNe6LoQ4ePJjUjhw5kmvDSxW0oZci5uvYz02a1PxnL3qpHN/73vd6jtesWVPYtas673qp+nM/f/78nuNdu3ZlVWQDJwAAVJgwDgAAQYRxAAAIIowDAEAQGzgDNx/88Ic/TGpXXHFFVgc2t1Rr7q+++uqktmnTpqwO9NKb27x5c8/xVVdd1dd5L7/8cs/xtGnTsqbTS3HOPvvspFbkhrpB/z56aTB++tOfJrVLLrkkqT3++ONJ7dxzz83qwAZOAACoMGEcAACCCOMAABDEmvEC9DOFt956a1K77rrrsrqynm4wbrzxxqR28803N2pO9dJ/zZo1K6k988wzua7197//vef4jDPOyJpOL1XfAw880HN8wQUX9HXeY4891nN83nnnZWXSS4PR7zwvXLgwqe3cuTOrA2vGAQCgwoRxAAAIIowDAEAQYRwAAILYwHmcvvGNb+TaiNm0+bO5pVrzXOc51UvFzEXT56Yfeqk9v+PSpUuT2rZt2wY6hv9FL/XP37j/58k4AAAEEcYBACCIMA4AAEGEcQAACDIUdeM6WLZsWa7NmuvWrStpRJC+wY56evLJJ6OHALX16KOPNmqTX1sc+ybVsVx//fVZ23gyDgAAQYRxAAAIIowDAEAQYRwAAIJ4A2cJb+Bqw1x5O1k5HnrooXE3ETdt/traS3m/9x133JHULrvssgJGVH9t7aWmqcLfXr00cXPnzk1qu3fvbt38db2BEwAAqksYBwCAIMI4AAAEsWb8KP1MxSuvvJLUTjzxxKxtrKeLm9emzV8beunQoUNJberUqbmu9eEPfzipbd++Pde1mqYNvdQG1ow3w3/+85+kNmlS7zPg559/PvnMzJkzSx3XoFkzDgAAFSaMAwBAEGEcAACCCOMAABBkKGuB1atXJ7Xvf//74573s5/9LKmtXLmysHEB7ZB3s+ZYbNakSfJulJwxY0bhYyG/L33pS+Nu1nzDU0891XM8Z86cUsdVF56MAwBAEGEcAACCCOMAABBEGAcAgCCNewPn0qVLk9qvf/3rvs69/fbbe47Xrl1b2LiaxtvJBjOvhw8fTj7ztre9LWuSNvRSv9/x29/+dlJbt25dCSNqpjb0UlV985vfTGrXXnttafcr+/fSS+XM16xZs3qOn3322azpvIETAAAqTBgHAIAgwjgAAASp/Ut/8q7reu6555KaNeIM0rZt28b9zFvf+taBjIVqmD17dvQQaIg//OEPSe39739/z/HDDz+cfOajH/1oFq2Na67rZmRkJNd5bVgjnocn4wAAEEQYBwCAIMI4AAAEEcYBACBIZV/6M2XKlKQ21gtQ8rJBZGK8ECFuDps2f3qJouil/9qzZ09SO/PMM7No3/3ud3uOr7nmmqyK9NKbb7qcOXNmrms1bW764aU/AABQYcI4AAAEEcYBACCIMA4AAEEq+wbO+++/P3oIEG7SJP+9DBy/uXPnRg+Bhsi7WbOfzYxt3NA5Fn/pAQAgiDAOAABBhHEAAAgijAMAQJDKbuDcv39/YdeyQYAq0pcAtIm/e2PzZBwAAIII4wAAEEQYBwCAIJ3usf8P7P/rg9b5cJQ+22ZMeomj6SWKopcoil5ikL3kyTgAAAQRxgEAIIgwDgAAQYRxAACo+gZOAACgWJ6MAwBAEGEcAACCCOMAABBEGAcAgCDCOAAABBHGAQAgiDAOAABBhHEAAAgijAMAQBbj/wA4Ias/ebTWuQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 750x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:39,595] A new study created in memory with name: no-name-996a2631-58ae-424f-942b-cc57bb056c9b\n",
      "/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_92190/4155332778.py:70: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n",
      "/var/folders/8j/y46wyjvx641clg_t_cq0ggfc0000gn/T/ipykernel_92190/4155332778.py:68: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1.0 / (1.0 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 1.6749 | Val Loss: 0.7985\n",
      "Epoch    6/40 | Train Loss: 0.3562 | Val Loss: 0.2555\n",
      "Epoch   11/40 | Train Loss: 0.3314 | Val Loss: 0.2472\n",
      "Epoch   16/40 | Train Loss: 0.3258 | Val Loss: 0.2523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:34:11,592] Trial 0 finished with value: 0.2155817842427901 and parameters: {'n_layers': 5, 'n_units_l0': 203, 'n_units_l1': 217, 'n_units_l2': 246, 'n_units_l3': 99, 'n_units_l4': 247, 'lr': 0.026085758460852666, 'batch_size': 128, 'dropout_l0': 0.32033650078802944, 'dropout_l1': 0.47559379869973994, 'dropout_l2': 0.0023639672063314743, 'dropout_l3': 0.06699527548033768, 'dropout_l4': 0.05218430251692335, 'weight_decay': 0.003947451635152316, 'optimizer': 'adam', 'stopping_patience': 6, 'activation_l0': 'leaky_relu', 'activation_l1': 'relu', 'activation_l2': 'leaky_relu', 'activation_l3': 'leaky_relu', 'activation_l4': 'sigmoid'}. Best is trial 0 with value: 0.2155817842427901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 2.3732 | Val Loss: 1.1330\n",
      "Epoch    6/40 | Train Loss: 0.7954 | Val Loss: 0.5307\n",
      "Epoch   11/40 | Train Loss: 0.7335 | Val Loss: 0.5495\n",
      "Epoch   16/40 | Train Loss: 0.7326 | Val Loss: 0.5069\n",
      "Epoch   21/40 | Train Loss: 0.7701 | Val Loss: 0.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:35:33,210] Trial 1 finished with value: 0.41823529285091543 and parameters: {'n_layers': 2, 'n_units_l0': 184, 'n_units_l1': 230, 'lr': 0.05874243278937118, 'batch_size': 16, 'dropout_l0': 0.2311519158680878, 'dropout_l1': 0.24200377048718363, 'weight_decay': 9.630521562206306e-05, 'optimizer': 'adam', 'stopping_patience': 8, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh'}. Best is trial 0 with value: 0.2155817842427901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 1.2441 | Val Loss: 0.6671\n",
      "Epoch    6/40 | Train Loss: 0.3177 | Val Loss: 0.2814\n",
      "Epoch   11/40 | Train Loss: 0.2443 | Val Loss: 0.2275\n",
      "Epoch   16/40 | Train Loss: 0.2065 | Val Loss: 0.2002\n",
      "Epoch   21/40 | Train Loss: 0.1799 | Val Loss: 0.1827\n",
      "Epoch   26/40 | Train Loss: 0.1623 | Val Loss: 0.1698\n",
      "Epoch   31/40 | Train Loss: 0.1450 | Val Loss: 0.1605\n",
      "Epoch   36/40 | Train Loss: 0.1334 | Val Loss: 0.1528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:36:07,274] Trial 2 finished with value: 0.1476904645981253 and parameters: {'n_layers': 2, 'n_units_l0': 176, 'n_units_l1': 86, 'lr': 0.001990024783439099, 'batch_size': 32, 'dropout_l0': 0.12344111061764967, 'dropout_l1': 0.22439994068883595, 'weight_decay': 2.4384190861017746e-06, 'optimizer': 'sgd', 'stopping_patience': 4, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh'}. Best is trial 2 with value: 0.1476904645981253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 2.5287 | Val Loss: 2.3054\n",
      "Epoch    6/40 | Train Loss: 2.3621 | Val Loss: 2.2849\n",
      "Epoch   11/40 | Train Loss: 2.3265 | Val Loss: 2.2717\n",
      "Epoch   16/40 | Train Loss: 2.3075 | Val Loss: 2.2562\n",
      "Epoch   21/40 | Train Loss: 2.2802 | Val Loss: 2.2202\n",
      "Epoch   26/40 | Train Loss: 2.2306 | Val Loss: 2.1260\n",
      "Epoch   31/40 | Train Loss: 2.1299 | Val Loss: 1.9589\n",
      "Epoch   36/40 | Train Loss: 2.0070 | Val Loss: 1.8240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:37:03,492] Trial 3 finished with value: 1.7472876916561582 and parameters: {'n_layers': 5, 'n_units_l0': 166, 'n_units_l1': 158, 'n_units_l2': 110, 'n_units_l3': 201, 'n_units_l4': 201, 'lr': 0.0007098883106258884, 'batch_size': 64, 'dropout_l0': 0.3932636610066659, 'dropout_l1': 0.2337021468808455, 'dropout_l2': 0.38640723033125574, 'dropout_l3': 0.4723842041840774, 'dropout_l4': 0.14472201554769748, 'weight_decay': 3.161572155491006e-06, 'optimizer': 'momentum', 'stopping_patience': 8, 'activation_l0': 'sigmoid', 'activation_l1': 'relu', 'activation_l2': 'sigmoid', 'activation_l3': 'relu', 'activation_l4': 'relu'}. Best is trial 2 with value: 0.1476904645981253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 2.1340 | Val Loss: 1.9271\n",
      "Epoch    6/40 | Train Loss: 0.8314 | Val Loss: 0.7338\n",
      "Epoch   11/40 | Train Loss: 0.5461 | Val Loss: 0.4909\n",
      "Epoch   16/40 | Train Loss: 0.4481 | Val Loss: 0.4086\n",
      "Epoch   21/40 | Train Loss: 0.4003 | Val Loss: 0.3675\n",
      "Epoch   26/40 | Train Loss: 0.3691 | Val Loss: 0.3425\n",
      "Epoch   31/40 | Train Loss: 0.3507 | Val Loss: 0.3253\n",
      "Epoch   36/40 | Train Loss: 0.3326 | Val Loss: 0.3124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:37:31,914] Trial 4 finished with value: 0.3040033387628092 and parameters: {'n_layers': 2, 'n_units_l0': 147, 'n_units_l1': 151, 'lr': 0.0011891024738438297, 'batch_size': 64, 'dropout_l0': 0.02208748238046665, 'dropout_l1': 0.12926773869681513, 'weight_decay': 0.0008919977673884573, 'optimizer': 'momentum', 'stopping_patience': 7, 'activation_l0': 'sigmoid', 'activation_l1': 'relu'}. Best is trial 2 with value: 0.1476904645981253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 1.4826 | Val Loss: 0.7221\n",
      "Epoch    6/40 | Train Loss: 0.4548 | Val Loss: 0.3417\n",
      "Epoch   11/40 | Train Loss: 0.4375 | Val Loss: 0.3133\n",
      "Epoch   16/40 | Train Loss: 0.4102 | Val Loss: 0.2882\n",
      "Epoch   21/40 | Train Loss: 0.4024 | Val Loss: 0.2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:37:56,545] Trial 5 finished with value: 0.2797403385928275 and parameters: {'n_layers': 2, 'n_units_l0': 101, 'n_units_l1': 94, 'lr': 0.04852199007688445, 'batch_size': 32, 'dropout_l0': 0.08424207678298162, 'dropout_l1': 0.11561698563657274, 'weight_decay': 0.002654963756197771, 'optimizer': 'adam', 'stopping_patience': 7, 'activation_l0': 'relu', 'activation_l1': 'relu'}. Best is trial 2 with value: 0.1476904645981253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   26/40 | Train Loss: 0.4128 | Val Loss: 0.3385\n",
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 2.3560 | Val Loss: 2.1754\n",
      "Epoch    6/40 | Train Loss: 1.1883 | Val Loss: 0.7992\n",
      "Epoch   11/40 | Train Loss: 0.7540 | Val Loss: 0.4924\n",
      "Epoch   16/40 | Train Loss: 0.5983 | Val Loss: 0.3836\n",
      "Epoch   21/40 | Train Loss: 0.5157 | Val Loss: 0.3343\n",
      "Epoch   26/40 | Train Loss: 0.4652 | Val Loss: 0.3048\n",
      "Epoch   31/40 | Train Loss: 0.4274 | Val Loss: 0.2831\n",
      "Epoch   36/40 | Train Loss: 0.4064 | Val Loss: 0.2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:38:29,760] Trial 6 finished with value: 0.25772479476569976 and parameters: {'n_layers': 5, 'n_units_l0': 99, 'n_units_l1': 223, 'n_units_l2': 217, 'n_units_l3': 68, 'n_units_l4': 123, 'lr': 0.004581299072711664, 'batch_size': 128, 'dropout_l0': 0.40194499004687806, 'dropout_l1': 0.19689723611301618, 'dropout_l2': 0.39866504014347304, 'dropout_l3': 0.000700825667588223, 'dropout_l4': 0.3417324761808458, 'weight_decay': 0.00018860258536969925, 'optimizer': 'sgd', 'stopping_patience': 8, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid', 'activation_l3': 'leaky_relu', 'activation_l4': 'tanh'}. Best is trial 2 with value: 0.1476904645981253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 2.6727 | Val Loss: 2.0201\n",
      "Epoch    6/40 | Train Loss: 1.3613 | Val Loss: 0.9554\n",
      "Epoch   11/40 | Train Loss: 0.9454 | Val Loss: 0.6189\n",
      "Epoch   16/40 | Train Loss: 0.7592 | Val Loss: 0.4910\n",
      "Epoch   21/40 | Train Loss: 0.6636 | Val Loss: 0.4293\n",
      "Epoch   26/40 | Train Loss: 0.5990 | Val Loss: 0.3930\n",
      "Epoch   31/40 | Train Loss: 0.5488 | Val Loss: 0.3689\n",
      "Epoch   36/40 | Train Loss: 0.5180 | Val Loss: 0.3513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:39:02,643] Trial 7 finished with value: 0.3394267385495969 and parameters: {'n_layers': 3, 'n_units_l0': 152, 'n_units_l1': 134, 'n_units_l2': 201, 'lr': 0.000401314964913542, 'batch_size': 64, 'dropout_l0': 0.36027164450225213, 'dropout_l1': 0.05325717415855097, 'dropout_l2': 0.4232681088374386, 'weight_decay': 0.0026710232737030045, 'optimizer': 'momentum', 'stopping_patience': 5, 'activation_l0': 'relu', 'activation_l1': 'leaky_relu', 'activation_l2': 'leaky_relu'}. Best is trial 2 with value: 0.1476904645981253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 0.7100 | Val Loss: 0.3087\n",
      "Epoch    6/40 | Train Loss: 0.2393 | Val Loss: 0.1948\n",
      "Epoch   11/40 | Train Loss: 0.1930 | Val Loss: 0.1697\n",
      "Epoch   16/40 | Train Loss: 0.1678 | Val Loss: 0.1566\n",
      "Epoch   21/40 | Train Loss: 0.1537 | Val Loss: 0.1494\n",
      "Epoch   26/40 | Train Loss: 0.1446 | Val Loss: 0.1456\n",
      "Epoch   31/40 | Train Loss: 0.1387 | Val Loss: 0.1430\n",
      "Epoch   36/40 | Train Loss: 0.1318 | Val Loss: 0.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:39:45,002] Trial 8 finished with value: 0.13473366015496685 and parameters: {'n_layers': 3, 'n_units_l0': 202, 'n_units_l1': 193, 'n_units_l2': 130, 'lr': 0.015926837123636857, 'batch_size': 64, 'dropout_l0': 0.33901686140475495, 'dropout_l1': 0.0749146291272319, 'dropout_l2': 0.3615666575988998, 'weight_decay': 0.0024956223195560403, 'optimizer': 'momentum', 'stopping_patience': 4, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu'}. Best is trial 8 with value: 0.13473366015496685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 0.2859 | Val Loss: 0.1826\n",
      "Epoch    6/40 | Train Loss: 0.0673 | Val Loss: 0.1464\n",
      "Epoch   11/40 | Train Loss: 0.0391 | Val Loss: 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:39:54,136] Trial 9 finished with value: 0.14640615554679837 and parameters: {'n_layers': 2, 'n_units_l0': 65, 'n_units_l1': 86, 'lr': 0.006042291032902162, 'batch_size': 32, 'dropout_l0': 0.006627626982040391, 'dropout_l1': 0.19064959237014145, 'weight_decay': 1.420308766829856e-05, 'optimizer': 'adam', 'stopping_patience': 7, 'activation_l0': 'tanh', 'activation_l1': 'relu'}. Best is trial 8 with value: 0.13473366015496685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 2.3849 | Val Loss: 2.3044\n",
      "Epoch    6/40 | Train Loss: 2.3470 | Val Loss: 2.2782\n",
      "Epoch   11/40 | Train Loss: 2.3239 | Val Loss: 2.2510\n",
      "Epoch   16/40 | Train Loss: 2.2905 | Val Loss: 2.2164\n",
      "Epoch   21/40 | Train Loss: 2.2497 | Val Loss: 2.1670\n",
      "Epoch   26/40 | Train Loss: 2.1947 | Val Loss: 2.0929\n",
      "Epoch   31/40 | Train Loss: 2.1140 | Val Loss: 1.9838\n",
      "Epoch   36/40 | Train Loss: 1.9982 | Val Loss: 1.8385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:42:07,422] Trial 10 finished with value: 1.7032833685160325 and parameters: {'n_layers': 4, 'n_units_l0': 251, 'n_units_l1': 192, 'n_units_l2': 77, 'n_units_l3': 254, 'lr': 0.00011799867731985009, 'batch_size': 16, 'dropout_l0': 0.49284758166078424, 'dropout_l1': 0.3567489678801291, 'dropout_l2': 0.18113368694544146, 'dropout_l3': 0.36027481194765043, 'weight_decay': 0.00025422373545824295, 'optimizer': 'momentum', 'stopping_patience': 3, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'tanh', 'activation_l3': 'sigmoid'}. Best is trial 8 with value: 0.13473366015496685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 0.4137 | Val Loss: 0.2242\n",
      "Epoch    6/40 | Train Loss: 0.1433 | Val Loss: 0.1412\n",
      "Epoch   11/40 | Train Loss: 0.1112 | Val Loss: 0.1453\n",
      "Epoch   16/40 | Train Loss: 0.0988 | Val Loss: 0.1432\n",
      "Epoch   21/40 | Train Loss: 0.0950 | Val Loss: 0.1365\n",
      "Epoch   26/40 | Train Loss: 0.0929 | Val Loss: 0.1372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:42:39,527] Trial 11 finished with value: 0.13373855245599356 and parameters: {'n_layers': 3, 'n_units_l0': 70, 'n_units_l1': 117, 'n_units_l2': 145, 'lr': 0.009573900247273245, 'batch_size': 32, 'dropout_l0': 0.1850036094506437, 'dropout_l1': 0.02303126563887041, 'dropout_l2': 0.24392273911886378, 'weight_decay': 2.0514790562166312e-05, 'optimizer': 'adam', 'stopping_patience': 10, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'relu'}. Best is trial 11 with value: 0.13373855245599356.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 0.4488 | Val Loss: 0.2525\n",
      "Epoch    6/40 | Train Loss: 0.1272 | Val Loss: 0.1413\n",
      "Epoch   11/40 | Train Loss: 0.0985 | Val Loss: 0.1335\n",
      "Epoch   16/40 | Train Loss: 0.0900 | Val Loss: 0.1199\n",
      "Epoch   21/40 | Train Loss: 0.0881 | Val Loss: 0.1262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:43:13,868] Trial 12 finished with value: 0.11990374797082225 and parameters: {'n_layers': 3, 'n_units_l0': 223, 'n_units_l1': 119, 'n_units_l2': 148, 'lr': 0.014422495551370882, 'batch_size': 64, 'dropout_l0': 0.20220505668167005, 'dropout_l1': 0.016648577967786606, 'dropout_l2': 0.24887457658884835, 'weight_decay': 2.5106608438452368e-05, 'optimizer': 'adam', 'stopping_patience': 9, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'relu'}. Best is trial 12 with value: 0.11990374797082225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 0.3664 | Val Loss: 0.2089\n",
      "Epoch    6/40 | Train Loss: 0.1049 | Val Loss: 0.1393\n",
      "Epoch   11/40 | Train Loss: 0.0780 | Val Loss: 0.1315\n",
      "Epoch   16/40 | Train Loss: 0.0680 | Val Loss: 0.1304\n",
      "Epoch   21/40 | Train Loss: 0.0662 | Val Loss: 0.1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:44:10,229] Trial 13 finished with value: 0.12234977942581762 and parameters: {'n_layers': 3, 'n_units_l0': 253, 'n_units_l1': 129, 'n_units_l2': 167, 'lr': 0.009169430854768856, 'batch_size': 32, 'dropout_l0': 0.1959308439210785, 'dropout_l1': 0.005060251270100263, 'dropout_l2': 0.22081719653280305, 'weight_decay': 2.2178168264957516e-05, 'optimizer': 'adam', 'stopping_patience': 10, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'relu'}. Best is trial 12 with value: 0.11990374797082225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 2.7760 | Val Loss: 1.9118\n",
      "Epoch    6/40 | Train Loss: 1.1717 | Val Loss: 0.9486\n",
      "Epoch   11/40 | Train Loss: 0.8826 | Val Loss: 0.7471\n",
      "Epoch   16/40 | Train Loss: 0.8328 | Val Loss: 0.6991\n",
      "Epoch   21/40 | Train Loss: 0.8046 | Val Loss: 0.6770\n",
      "Epoch   26/40 | Train Loss: 0.8669 | Val Loss: 0.6942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:45:05,321] Trial 14 finished with value: 0.622608271159271 and parameters: {'n_layers': 4, 'n_units_l0': 254, 'n_units_l1': 117, 'n_units_l2': 179, 'n_units_l3': 146, 'lr': 0.09979527768184467, 'batch_size': 64, 'dropout_l0': 0.20032723582798734, 'dropout_l1': 0.011135834786620377, 'dropout_l2': 0.13130588622976475, 'dropout_l3': 0.2047107003460043, 'weight_decay': 2.4172462676873764e-05, 'optimizer': 'adam', 'stopping_patience': 10, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'relu', 'activation_l3': 'tanh'}. Best is trial 12 with value: 0.11990374797082225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 0.9719 | Val Loss: 0.3763\n",
      "Epoch    6/40 | Train Loss: 0.3568 | Val Loss: 0.2202\n",
      "Epoch   11/40 | Train Loss: 0.2985 | Val Loss: 0.2125\n",
      "Epoch   16/40 | Train Loss: 0.2680 | Val Loss: 0.1939\n",
      "Epoch   21/40 | Train Loss: 0.2616 | Val Loss: 0.2077\n",
      "Epoch   26/40 | Train Loss: 0.2646 | Val Loss: 0.1931\n",
      "Epoch   31/40 | Train Loss: 0.2553 | Val Loss: 0.1922\n",
      "Epoch   36/40 | Train Loss: 0.2665 | Val Loss: 0.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:46:54,003] Trial 15 finished with value: 0.17432956436808086 and parameters: {'n_layers': 4, 'n_units_l0': 230, 'n_units_l1': 67, 'n_units_l2': 167, 'n_units_l3': 256, 'lr': 0.01642187364308757, 'batch_size': 32, 'dropout_l0': 0.2893896149260602, 'dropout_l1': 0.32466141131249404, 'dropout_l2': 0.26571625241206975, 'dropout_l3': 0.20494717141697824, 'weight_decay': 1.0470934782321685e-06, 'optimizer': 'adam', 'stopping_patience': 9, 'activation_l0': 'tanh', 'activation_l1': 'leaky_relu', 'activation_l2': 'relu', 'activation_l3': 'relu'}. Best is trial 12 with value: 0.11990374797082225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 0.2910 | Val Loss: 0.1705\n",
      "Epoch    6/40 | Train Loss: 0.0667 | Val Loss: 0.1186\n",
      "Epoch   11/40 | Train Loss: 0.0430 | Val Loss: 0.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:47:57,528] Trial 16 finished with value: 0.11795482742652526 and parameters: {'n_layers': 3, 'n_units_l0': 228, 'n_units_l1': 186, 'n_units_l2': 117, 'lr': 0.0031405316937350753, 'batch_size': 16, 'dropout_l0': 0.1397419034113071, 'dropout_l1': 0.12263450228121416, 'dropout_l2': 0.2702223142987862, 'weight_decay': 6.493826677737431e-05, 'optimizer': 'adam', 'stopping_patience': 9, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 16 with value: 0.11795482742652526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/40 | Train Loss: 1.4328 | Val Loss: 0.6471\n",
      "Epoch    6/40 | Train Loss: 0.3458 | Val Loss: 0.2690\n",
      "Epoch   11/40 | Train Loss: 0.2714 | Val Loss: 0.2274\n",
      "Epoch   16/40 | Train Loss: 0.2282 | Val Loss: 0.2027\n",
      "Epoch   21/40 | Train Loss: 0.1971 | Val Loss: 0.1858\n",
      "Epoch   26/40 | Train Loss: 0.1748 | Val Loss: 0.1730\n",
      "Epoch   31/40 | Train Loss: 0.1565 | Val Loss: 0.1630\n",
      "Epoch   36/40 | Train Loss: 0.1404 | Val Loss: 0.1557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:49:19,048] Trial 17 finished with value: 0.14958021434280064 and parameters: {'n_layers': 3, 'n_units_l0': 206, 'n_units_l1': 254, 'n_units_l2': 107, 'lr': 0.002778406031037032, 'batch_size': 16, 'dropout_l0': 0.12879769511381456, 'dropout_l1': 0.12869825829090997, 'dropout_l2': 0.30329516021373104, 'weight_decay': 6.204603749720857e-05, 'optimizer': 'sgd', 'stopping_patience': 9, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 16 with value: 0.11795482742652526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 0.9821 | Val Loss: 0.4904\n",
      "Epoch    6/40 | Train Loss: 0.3399 | Val Loss: 0.2887\n",
      "Epoch   11/40 | Train Loss: 0.2666 | Val Loss: 0.2331\n",
      "Epoch   16/40 | Train Loss: 0.2214 | Val Loss: 0.1993\n",
      "Epoch   21/40 | Train Loss: 0.1945 | Val Loss: 0.1775\n",
      "Epoch   26/40 | Train Loss: 0.1772 | Val Loss: 0.1625\n",
      "Epoch   31/40 | Train Loss: 0.1602 | Val Loss: 0.1491\n",
      "Epoch   36/40 | Train Loss: 0.1491 | Val Loss: 0.1405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:52:58,523] Trial 18 finished with value: 0.13370189328818877 and parameters: {'n_layers': 4, 'n_units_l0': 233, 'n_units_l1': 196, 'n_units_l2': 76, 'n_units_l3': 171, 'lr': 0.00025414775522299057, 'batch_size': 16, 'dropout_l0': 0.08538429838487463, 'dropout_l1': 0.0815331650722507, 'dropout_l2': 0.10779112599694132, 'dropout_l3': 0.48659933179679615, 'weight_decay': 0.0004989243389134363, 'optimizer': 'adam', 'stopping_patience': 9, 'activation_l0': 'sigmoid', 'activation_l1': 'sigmoid', 'activation_l2': 'tanh', 'activation_l3': 'sigmoid'}. Best is trial 16 with value: 0.11795482742652526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/40 | Train Loss: 0.3756 | Val Loss: 0.1895\n",
      "Epoch    6/40 | Train Loss: 0.1079 | Val Loss: 0.1332\n",
      "Epoch   11/40 | Train Loss: 0.0730 | Val Loss: 0.1324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:53:59,118] Trial 19 finished with value: 0.13129211593291104 and parameters: {'n_layers': 3, 'n_units_l0': 225, 'n_units_l1': 175, 'n_units_l2': 120, 'lr': 0.0017142716681980584, 'batch_size': 16, 'dropout_l0': 0.268861914186062, 'dropout_l1': 0.1712371824059719, 'dropout_l2': 0.4913648174376183, 'weight_decay': 5.657471304432082e-06, 'optimizer': 'adam', 'stopping_patience': 6, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 16 with value: 0.11795482742652526.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best trial:\n",
      "  Value: 0.1180\n",
      "  Params:\n",
      "    n_layers: 3\n",
      "    n_units_l0: 228\n",
      "    n_units_l1: 186\n",
      "    n_units_l2: 117\n",
      "    lr: 0.0031405316937350753\n",
      "    batch_size: 16\n",
      "    dropout_l0: 0.1397419034113071\n",
      "    dropout_l1: 0.12263450228121416\n",
      "    dropout_l2: 0.2702223142987862\n",
      "    weight_decay: 6.493826677737431e-05\n",
      "    optimizer: adam\n",
      "    stopping_patience: 9\n",
      "    activation_l0: tanh\n",
      "    activation_l1: sigmoid\n",
      "    activation_l2: relu\n",
      "Epoch    1/100 | Train Loss: 0.2910 | Val Loss: 0.1705\n",
      "Epoch    6/100 | Train Loss: 0.0667 | Val Loss: 0.1186\n",
      "Epoch   11/100 | Train Loss: 0.0430 | Val Loss: 0.1265\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.69%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch (made faster)\n",
    "# Dataset: MNIST (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from functools import partial\n",
    "import optuna\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_data = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist_data[\"data\"]\n",
    "y = mnist_data[\"target\"]\n",
    "\n",
    "# Normalize\n",
    "print(\"Preprocessing data...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "show_images(X_train)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    x = np.clip(x, -500, 500) # prevent overflow in exp\n",
    "    if is_derivative:\n",
    "        s = 1.0 / (1.0 + np.exp(-x))\n",
    "        return s * (1.0 - s)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    t = np.tanh(x)\n",
    "    if is_derivative:\n",
    "        return 1.0 - t * t\n",
    "    return t\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        # return 1 where x>0, alpha otherwise\n",
    "        grad = np.ones_like(x)\n",
    "        grad[x < 0] = alpha\n",
    "        return grad\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    diff = Y - Y_pred.T\n",
    "    return 0.5 * np.mean(diff * diff)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    Y_pred = np.clip(Y_pred.T, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(Y * np.log(Y_pred), axis=1))\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims, activations):\n",
    "    np.random.seed(42)\n",
    "    sqrt = np.sqrt\n",
    "    standard_normal = np.random.standard_normal\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        fan_in = layer_dims[i - 1]\n",
    "        fan_out = layer_dims[i]\n",
    "\n",
    "        act = activations[i - 1]\n",
    "        scale = sqrt(2.0 / fan_in) if act in (\"relu\", \"leaky_relu\") else sqrt(1.0 / fan_in)\n",
    "\n",
    "        # allocate directly\n",
    "        W = standard_normal((fan_out, fan_in)).astype(np.float32) * scale\n",
    "        b = np.zeros((fan_out, 1), dtype=np.float32)\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, weights, biases, activations, dropout_rates=None, training=True):\n",
    "    A = X.T\n",
    "    cache = [A]  # store activations for backprop\n",
    "    Zs = []      # store pre-activations (Z values)\n",
    "    masks = []   # store dropout masks\n",
    "\n",
    "    # Activation function map for direct access\n",
    "    act_funcs = {\n",
    "        \"sigmoid\": sigmoid,\n",
    "        \"relu\": relu,\n",
    "        \"tanh\": tanh,\n",
    "        \"leaky_relu\": leaky_relu,\n",
    "        \"softmax\": softmax\n",
    "    }\n",
    "\n",
    "    for i, (W, b, act_name) in enumerate(zip(weights, biases, activations)):\n",
    "        Z = W @ A + b\n",
    "        Zs.append(Z)\n",
    "\n",
    "        A = act_funcs[act_name](Z)\n",
    "\n",
    "        # Dropout (training only, not for output layer)\n",
    "        if training and dropout_rates and i < len(dropout_rates):\n",
    "            rate = dropout_rates[i]\n",
    "            mask = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A *= mask\n",
    "            A /= (1 - rate)\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(None)\n",
    "\n",
    "        cache.append(A)\n",
    "\n",
    "    return A, (cache, Zs, masks)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, weights, cache_data, activations, dropout_rates=None, weight_decay=0.0):\n",
    "    cache, Zs, masks = cache_data\n",
    "    m = Y.shape[0]\n",
    "    L = len(weights)\n",
    "\n",
    "    dWs = [None] * L\n",
    "    dbs = [None] * L\n",
    "\n",
    "    Y = Y.T\n",
    "    dZ = cache[-1] - Y  # derivative of softmax-crossentropy\n",
    "\n",
    "    # Activation derivative map\n",
    "    act_derivs = {\n",
    "        \"sigmoid\": lambda Z: sigmoid(Z, True),\n",
    "        \"relu\": lambda Z: relu(Z, True),\n",
    "        \"tanh\": lambda Z: tanh(Z, True),\n",
    "        \"leaky_relu\": lambda Z: leaky_relu(Z, True)\n",
    "    }\n",
    "\n",
    "    for i in reversed(range(L)):\n",
    "        A_prev = cache[i]\n",
    "        W = weights[i]\n",
    "\n",
    "        # Gradients\n",
    "        dWs[i] = (dZ @ A_prev.T) / m + weight_decay * W\n",
    "        dbs[i] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Gradient clipping (optional, can disable for extra speed)\n",
    "        # dWs[i] = np.clip(dWs[i], -5, 5)\n",
    "        # dbs[i] = np.clip(dbs[i], -5, 5)\n",
    "\n",
    "        if i > 0:\n",
    "            dA_prev = W.T @ dZ\n",
    "\n",
    "            # Apply dropout if used\n",
    "            if masks[i - 1] is not None:\n",
    "                dA_prev *= masks[i - 1]\n",
    "                dA_prev /= (1 - dropout_rates[i - 1])\n",
    "\n",
    "            # Apply activation derivative\n",
    "            dZ = dA_prev * act_derivs[activations[i - 1]](Zs[i - 1])\n",
    "\n",
    "    return dWs, dbs\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    ^(t+1) <- ^t - L(y, y)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- v^t + (1-)L(y, y)^t\n",
    "    ^(t+1) <- ^t - v^(t+1)\n",
    "\n",
    "    Adam:\n",
    "    m^(t+1) <- 1 m^t + (1-1)L(y, y)^t\n",
    "    v^(t+1) <- 2 v^t + (1-2)(L(y, y)^t)^2\n",
    "    m^(t+1) <- m^(t+1) / (1 - 1^(t+1))\n",
    "    v^(t+1) <- v^(t+1) / (1 - 2^(t+1))\n",
    "    ^(t+1) <- ^t -  m^(t+1) / ((v^(t+1)) + )\n",
    "'''\n",
    "def update_parameters(weights, biases, dWs, dbs, lr, optimizer_type, velocity=None, beta=0.9, m=None, v=None, t=1, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    L = len(weights)\n",
    "\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(L):\n",
    "            weights[i] -= lr * dWs[i]\n",
    "            biases[i] -= lr * dbs[i]\n",
    "\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {\"W\": [np.zeros_like(W) for W in weights], \"b\": [np.zeros_like(b) for b in biases]}\n",
    "        for i in range(L):\n",
    "            velocity[\"W\"][i] = beta * velocity[\"W\"][i] + (1 - beta) * dWs[i]\n",
    "            velocity[\"b\"][i] = beta * velocity[\"b\"][i] + (1 - beta) * dbs[i]\n",
    "            weights[i] -= lr * velocity[\"W\"][i]\n",
    "            biases[i] -= lr * velocity[\"b\"][i]\n",
    "\n",
    "    elif optimizer_type == \"adam\":\n",
    "        if m is None:\n",
    "            m = {\"W\": [np.zeros_like(W) for W in weights], \"b\": [np.zeros_like(b) for b in biases]}\n",
    "            v = {\"W\": [np.zeros_like(W) for W in weights], \"b\": [np.zeros_like(b) for b in biases]}\n",
    "\n",
    "        for i in range(L):\n",
    "            # Update moment estimates\n",
    "            m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * dWs[i]\n",
    "            m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * dbs[i]\n",
    "            v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (dWs[i] ** 2)\n",
    "            v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (dbs[i] ** 2)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat_W = m[\"W\"][i] / (1 - beta1 ** t)\n",
    "            m_hat_b = m[\"b\"][i] / (1 - beta1 ** t)\n",
    "            v_hat_W = v[\"W\"][i] / (1 - beta2 ** t)\n",
    "            v_hat_b = v[\"b\"][i] / (1 - beta2 ** t)\n",
    "\n",
    "            # Parameter update\n",
    "            weights[i] -= lr * m_hat_W / (np.sqrt(v_hat_W) + eps)\n",
    "            biases[i] -= lr * m_hat_b / (np.sqrt(v_hat_b) + eps)\n",
    "\n",
    "    return weights, biases, velocity, (m, v)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    X_val, \n",
    "    Y_val,\n",
    "    hidden_layers=[128, 64], \n",
    "    lr=0.01, \n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=None, \n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=5, \n",
    "    print_every=5, \n",
    "    plot_loss=True,\n",
    "):\n",
    "    input_dim, output_dim = X_train.shape[1], Y_train.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    # Initialization\n",
    "    weights, biases = initialize_parameters(layer_dims, activations)\n",
    "    velocity, (m, v) = None, (None, None)\n",
    "    best_loss, patience_counter = np.inf, 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Mini-batch iterator\n",
    "    def batch_iterator(X, Y, batch_size):\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            yield X[i:i+batch_size], Y[i:i+batch_size]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled, Y_shuffled = X_train[perm], Y_train[perm]\n",
    "\n",
    "        # Training by batch\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for X_batch, Y_batch in batch_iterator(X_shuffled, Y_shuffled, batch_size):\n",
    "            # Forward\n",
    "            A_out, cache_data = forward_propagation(X_batch, weights, biases, activations, dropout_rates)\n",
    "\n",
    "            # Compute loss\n",
    "            if loss_function == \"cross_entropy\":\n",
    "                loss = cross_entropy_loss(Y_batch, A_out)\n",
    "            else:\n",
    "                loss = mse_loss(Y_batch, A_out)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "\n",
    "            # Backward\n",
    "            dWs, dbs = backward_propagation(Y_batch, weights, cache_data, activations, dropout_rates, weight_decay)\n",
    "\n",
    "            # Update params\n",
    "            weights, biases, velocity, (m, v) = update_parameters(weights, biases, dWs, dbs, lr, optimizer_type, velocity=velocity, m=m, v=v, t=epoch+1)\n",
    "\n",
    "        # Validation loss\n",
    "        A_val, _ = forward_propagation(X_val, weights, biases, activations)\n",
    "        val_loss = cross_entropy_loss(Y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(Y_val, A_val)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1:4d}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_weights = [W.copy() for W in weights]\n",
    "                best_biases = [b.copy() for b in biases]\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    weights, biases = best_weights, best_biases\n",
    "                    break\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(train_losses, label=\"Train\")\n",
    "        plt.plot(val_losses, label=\"Val\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, weights, biases, activations):\n",
    "    A_out, _ = forward_propagation(X, weights, biases, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Hyperparameter Optimization with Optuna\n",
    "# =======================================\n",
    "def objective(trial, loss_function=\"cross_entropy\", epochs=40):\n",
    "    # Hyperparameters to search\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 5)\n",
    "    hidden_layers = [trial.suggest_int(f\"n_units_l{i}\", 64, 256) for i in range(n_layers)]\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    dropout_rates = [trial.suggest_float(f\"dropout_l{i}\", 0.0, 0.5) for i in range(n_layers)]\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\", \"adam\"])\n",
    "    stopping_patience = trial.suggest_int(\"stopping_patience\", 3, 10)\n",
    "    # Activation functions per layer\n",
    "    possible_activations = [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\"]\n",
    "    activations = [trial.suggest_categorical(f\"activation_l{i}\", possible_activations) for i in range(n_layers)]\n",
    "    activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "    # Train network\n",
    "    weights, biases = train_neural_network(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        X_val, \n",
    "        y_val,\n",
    "        hidden_layers=hidden_layers,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        activations=activations,\n",
    "        loss_function=loss_function,\n",
    "        optimizer_type=optimizer_type,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_decay=weight_decay,\n",
    "        early_stopping=True,\n",
    "        patience=stopping_patience,\n",
    "        print_every=5, \n",
    "        plot_loss=False\n",
    "    )\n",
    "\n",
    "    # Optuna tries to minimize the validation loss function\n",
    "    A_val, _ = forward_propagation(X_val, weights, biases, activations, training=False)\n",
    "    val_loss = cross_entropy_loss(y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(y_val, A_val)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "loss_function = \"cross_entropy\"\n",
    "epochs = 40\n",
    "obj = partial(objective, loss_function=loss_function, epochs=epochs)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(obj, n_trials=20)\n",
    "\n",
    "# Best result summary\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 11. Retrain Best Model on Train + Val and Evaluate on Test\n",
    "# =======================================\n",
    "best_params = trial.params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_layers = [best_params[f\"n_units_l{i}\"] for i in range(n_layers)]\n",
    "lr = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "dropout_rates = [best_params[f\"dropout_l{i}\"] for i in range(n_layers)]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "optimizer_type = best_params[\"optimizer\"]\n",
    "stopping_patience = best_params[\"stopping_patience\"]\n",
    "activations = [best_params[f\"activation_l{i}\"] for i in range(n_layers)]\n",
    "activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "# Retrain final model\n",
    "weights, biases= train_neural_network(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_val, \n",
    "    y_val,\n",
    "    hidden_layers=hidden_layers,\n",
    "    lr=lr,\n",
    "    epochs=100,\n",
    "    batch_size=batch_size,\n",
    "    activations=activations,\n",
    "    loss_function=loss_function,\n",
    "    optimizer_type=optimizer_type,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_decay=weight_decay,\n",
    "    early_stopping=True,\n",
    "    patience=stopping_patience,\n",
    "    print_every=5,\n",
    "    plot_loss=True,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = predict(X_test, weights, biases, activations)\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   200 | Loss: 0.3335\n",
      "Epoch   400 | Loss: 0.2115\n",
      "Epoch   600 | Loss: 0.1330\n",
      "Epoch   800 | Loss: 0.1448\n",
      "Epoch  1000 | Loss: 0.1023\n",
      "Epoch  1200 | Loss: 0.1071\n",
      "Epoch  1400 | Loss: 0.1232\n",
      "Epoch  1600 | Loss: 0.0836\n",
      "Epoch  1800 | Loss: 0.1065\n",
      "Epoch  2000 | Loss: 0.0812\n",
      "Epoch  2200 | Loss: 0.1190\n",
      "Early stopping at epoch 2382\n",
      "\n",
      "Test Set Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch adding hyperparameters\n",
    "# Dataset: Iris (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Normalize features for better convergence\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, 0, x)\n",
    "        x = np.where(x >= 0, 1, x)\n",
    "        return x\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, alpha, 1)\n",
    "        return x\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_pred.T + eps)) / m\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1. / layer_dims[i - 1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations, dropout_rates=None, training=True):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i-1] == \"sigmoid\": A = sigmoid(Z)\n",
    "        elif activations[i-1] == \"relu\": A = relu(Z)\n",
    "        elif activations[i-1] == \"tanh\": A = tanh(Z)\n",
    "        elif activations[i-1] == \"leaky_relu\": A = leaky_relu(Z)\n",
    "        elif activations[i-1] == \"softmax\": A = softmax(Z)\n",
    "        else: raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        # Dropout\n",
    "        if training and dropout_rates and i <= len(dropout_rates):\n",
    "            rate = dropout_rates[i-1]\n",
    "            D = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A = (A * D) / (1 - rate)\n",
    "            cache[f\"D{i}\"] = D\n",
    "\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, params, cache, activations, dropout_rates=None, weight_decay=0.0):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    dZ = cache[f\"A{L}\"] - Y\n",
    "\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T) + weight_decay * W\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = np.dot(W.T, dZ) * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = np.dot(W.T, dZ) * relu(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"tanh\":\n",
    "                dZ = np.dot(W.T, dZ) * tanh(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"leaky_relu\":\n",
    "                dZ = np.dot(W.T, dZ) * leaky_relu(Z_prev, is_derivative=True)\n",
    "\n",
    "            # Apply dropout backward\n",
    "            if dropout_rates and (i - 2) < len(dropout_rates):\n",
    "                rate = dropout_rates[i-2]\n",
    "                D = cache.get(f\"D{i-1}\", np.ones_like(dZ))\n",
    "                dZ = (dZ * D) / (1 - rate)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "    θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "'''\n",
    "def update_parameters(params, grads, lr, optimizer_type=\"sgd\", velocity=None, beta=0.9):\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {f\"dW{i}\": np.zeros_like(grads[f\"dW{i}\"]) for i in range(1, len(params)//2 + 1)}\n",
    "            velocity.update({f\"db{i}\": np.zeros_like(grads[f\"db{i}\"]) for i in range(1, len(params)//2 + 1)})\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            velocity[f\"dW{i}\"] = beta * velocity[f\"dW{i}\"] + (1 - beta) * grads[f\"dW{i}\"]\n",
    "            velocity[f\"db{i}\"] = beta * velocity[f\"db{i}\"] + (1 - beta) * grads[f\"db{i}\"]\n",
    "            params[f\"W{i}\"] -= lr * velocity[f\"dW{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * velocity[f\"db{i}\"]\n",
    "    return params, velocity\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X, Y,\n",
    "    hidden_layers=[10],\n",
    "    lr=0.05,\n",
    "    epochs=5000,\n",
    "    batch_size=None,\n",
    "    print_every=500,\n",
    "    activations=None,\n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"sgd\",\n",
    "    dropout_rates=None,\n",
    "    weight_decay=0.0,\n",
    "    early_stopping_patience=200\n",
    "):\n",
    "    input_dim, output_dim = X.shape[1], Y.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "    velocity = None\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X.shape[0])\n",
    "        X_shuffled, Y_shuffled = X[perm], Y[perm]\n",
    "\n",
    "        # Mini-batch iteration\n",
    "        if batch_size is None:\n",
    "            batches = [(X_shuffled, Y_shuffled)]\n",
    "        else:\n",
    "            batches = [\n",
    "                (X_shuffled[i:i+batch_size], Y_shuffled[i:i+batch_size])\n",
    "                for i in range(0, X.shape[0], batch_size)\n",
    "            ]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for X_batch, Y_batch in batches:\n",
    "            A_out, cache = forward_propagation(X_batch, params, activations, dropout_rates, training=True)\n",
    "            loss = cross_entropy_loss(Y_batch, A_out) if loss_function == \"cross_entropy\" else mse_loss(Y_batch, A_out)\n",
    "            grads = backward_propagation(Y_batch, params, cache, activations, dropout_rates, weight_decay)\n",
    "            params, velocity = update_parameters(params, grads, lr, optimizer_type, velocity)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        epoch_loss /= len(batches)\n",
    "\n",
    "        # Early stopping check\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch:5d} | Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _ = forward_propagation(X, params, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Run Training and Evaluate\n",
    "# =======================================\n",
    "params = train_neural_network(\n",
    "    X_train, y_train,\n",
    "    hidden_layers=[10],\n",
    "    lr=0.05,\n",
    "    epochs=5000,\n",
    "    batch_size=16,\n",
    "    activations=[\"sigmoid\", \"softmax\"],\n",
    "    dropout_rates=[0.1],\n",
    "    weight_decay=1e-3,\n",
    "    optimizer_type=\"sgd\",\n",
    "    early_stopping_patience=500,\n",
    "    print_every=200\n",
    ")\n",
    "\n",
    "y_pred = predict(X_test, params, [\"sigmoid\", \"softmax\"])\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nTest Set Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

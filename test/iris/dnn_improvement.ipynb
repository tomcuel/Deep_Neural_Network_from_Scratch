{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbd164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:03,027] A new study created in memory with name: no-name-07cc5a3e-a6a6-43d3-8b42-514d15a48c45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 4) (23, 4) (23, 4)\n",
      "Epoch    1/1000 | Train Loss: 1.1480 | Val Loss: 1.0785\n",
      "Epoch  101/1000 | Train Loss: 1.0860 | Val Loss: 1.0263\n",
      "Epoch  201/1000 | Train Loss: 1.0077 | Val Loss: 0.9951\n",
      "Epoch  301/1000 | Train Loss: 0.9929 | Val Loss: 0.9718\n",
      "Epoch  401/1000 | Train Loss: 0.9155 | Val Loss: 0.9502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:03,425] Trial 0 finished with value: 0.8411672072248519 and parameters: {'n_layers': 1, 'n_units_l0': 38, 'lr': 0.0004968546591477243, 'batch_size': 32, 'dropout_l0': 0.20379574949710755, 'weight_decay': 1.925277430948873e-05, 'optimizer': 'sgd', 'stopping_patience': 96, 'activation_l0': 'sigmoid'}. Best is trial 0 with value: 0.8411672072248519.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  501/1000 | Train Loss: 0.8972 | Val Loss: 0.9300\n",
      "Epoch  601/1000 | Train Loss: 0.9373 | Val Loss: 0.9104\n",
      "Epoch  701/1000 | Train Loss: 0.8697 | Val Loss: 0.8917\n",
      "Epoch  801/1000 | Train Loss: 0.8594 | Val Loss: 0.8742\n",
      "Epoch  901/1000 | Train Loss: 0.8271 | Val Loss: 0.8572\n",
      "Epoch    1/1000 | Train Loss: 0.8167 | Val Loss: 0.5846\n",
      "Epoch  101/1000 | Train Loss: 0.0763 | Val Loss: 0.1236\n",
      "Epoch  201/1000 | Train Loss: 0.0608 | Val Loss: 0.1187\n",
      "Epoch  301/1000 | Train Loss: 0.0508 | Val Loss: 0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:03,923] Trial 1 finished with value: 0.11236772616451024 and parameters: {'n_layers': 1, 'n_units_l0': 49, 'lr': 0.055982613680720245, 'batch_size': 8, 'dropout_l0': 0.25081571183472545, 'weight_decay': 8.597765070060775e-06, 'optimizer': 'sgd', 'stopping_patience': 320, 'activation_l0': 'tanh'}. Best is trial 1 with value: 0.11236772616451024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  401/1000 | Train Loss: 0.0584 | Val Loss: 0.1222\n",
      "Epoch  501/1000 | Train Loss: 0.0404 | Val Loss: 0.1213\n",
      "Epoch  601/1000 | Train Loss: 0.0486 | Val Loss: 0.1259\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.2733 | Val Loss: 1.1839\n",
      "Epoch  101/1000 | Train Loss: 0.2861 | Val Loss: 0.3861\n",
      "Epoch  201/1000 | Train Loss: 0.2919 | Val Loss: 0.3323\n",
      "Epoch  301/1000 | Train Loss: 0.1866 | Val Loss: 0.3025\n",
      "Epoch  401/1000 | Train Loss: 0.2094 | Val Loss: 0.2777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:04,353] Trial 2 finished with value: 0.21671800447318584 and parameters: {'n_layers': 1, 'n_units_l0': 109, 'lr': 0.008652740740371274, 'batch_size': 32, 'dropout_l0': 0.4630115400441294, 'weight_decay': 2.636043885744971e-05, 'optimizer': 'sgd', 'stopping_patience': 394, 'activation_l0': 'relu'}. Best is trial 1 with value: 0.11236772616451024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  501/1000 | Train Loss: 0.1763 | Val Loss: 0.2618\n",
      "Epoch  601/1000 | Train Loss: 0.1406 | Val Loss: 0.2458\n",
      "Epoch  701/1000 | Train Loss: 0.1205 | Val Loss: 0.2360\n",
      "Epoch  801/1000 | Train Loss: 0.1174 | Val Loss: 0.2299\n",
      "Epoch  901/1000 | Train Loss: 0.1078 | Val Loss: 0.2277\n",
      "Epoch    1/1000 | Train Loss: 1.1598 | Val Loss: 1.0770\n",
      "Epoch  101/1000 | Train Loss: 1.0713 | Val Loss: 0.9760\n",
      "Epoch  201/1000 | Train Loss: 0.9829 | Val Loss: 0.9049\n",
      "Epoch  301/1000 | Train Loss: 0.9675 | Val Loss: 0.8391\n",
      "Epoch  401/1000 | Train Loss: 0.8991 | Val Loss: 0.7803\n",
      "Epoch  501/1000 | Train Loss: 0.8571 | Val Loss: 0.7248\n",
      "Epoch  601/1000 | Train Loss: 0.8534 | Val Loss: 0.6747\n",
      "Epoch  701/1000 | Train Loss: 0.7053 | Val Loss: 0.6310\n",
      "Epoch  801/1000 | Train Loss: 0.7540 | Val Loss: 0.5904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:06,035] Trial 3 finished with value: 0.5291349724396794 and parameters: {'n_layers': 3, 'n_units_l0': 117, 'n_units_l1': 113, 'n_units_l2': 77, 'lr': 0.00036072689913294997, 'batch_size': 16, 'dropout_l0': 0.382169266574253, 'dropout_l1': 0.24587309176078115, 'dropout_l2': 0.08775438501102178, 'weight_decay': 6.810555145200968e-05, 'optimizer': 'sgd', 'stopping_patience': 491, 'activation_l0': 'sigmoid', 'activation_l1': 'leaky_relu', 'activation_l2': 'tanh'}. Best is trial 1 with value: 0.11236772616451024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  901/1000 | Train Loss: 0.7332 | Val Loss: 0.5580\n",
      "Epoch    1/1000 | Train Loss: 1.4335 | Val Loss: 1.2699\n",
      "Epoch  101/1000 | Train Loss: 1.0889 | Val Loss: 1.0326\n",
      "Epoch  201/1000 | Train Loss: 1.1556 | Val Loss: 0.9361\n",
      "Epoch  301/1000 | Train Loss: 0.9374 | Val Loss: 0.8583\n",
      "Epoch  401/1000 | Train Loss: 0.9326 | Val Loss: 0.7958\n",
      "Epoch  501/1000 | Train Loss: 0.8062 | Val Loss: 0.7428\n",
      "Epoch  601/1000 | Train Loss: 0.8154 | Val Loss: 0.6963\n",
      "Epoch  701/1000 | Train Loss: 0.7998 | Val Loss: 0.6582\n",
      "Epoch  801/1000 | Train Loss: 0.6977 | Val Loss: 0.6251\n",
      "Epoch  901/1000 | Train Loss: 0.6314 | Val Loss: 0.5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:06,785] Trial 4 finished with value: 0.573157480707722 and parameters: {'n_layers': 2, 'n_units_l0': 51, 'n_units_l1': 117, 'lr': 0.0006687163725703356, 'batch_size': 32, 'dropout_l0': 0.19035150629081998, 'dropout_l1': 0.48152237291942174, 'weight_decay': 3.0247636467849347e-06, 'optimizer': 'momentum', 'stopping_patience': 410, 'activation_l0': 'sigmoid', 'activation_l1': 'leaky_relu'}. Best is trial 1 with value: 0.11236772616451024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.0787 | Val Loss: 0.6699\n",
      "Epoch  101/1000 | Train Loss: 0.0839 | Val Loss: 0.1763\n",
      "Epoch  201/1000 | Train Loss: 0.0777 | Val Loss: 0.1806\n",
      "Epoch  301/1000 | Train Loss: 0.0370 | Val Loss: 0.2070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:07,202] Trial 5 finished with value: 0.15108384497463404 and parameters: {'n_layers': 2, 'n_units_l0': 62, 'n_units_l1': 25, 'lr': 0.03803809311110164, 'batch_size': 16, 'dropout_l0': 0.0025907961214736708, 'dropout_l1': 0.362295188723749, 'weight_decay': 0.0012068201446701855, 'optimizer': 'sgd', 'stopping_patience': 448, 'activation_l0': 'relu', 'activation_l1': 'relu'}. Best is trial 1 with value: 0.11236772616451024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  401/1000 | Train Loss: 0.0523 | Val Loss: 0.2155\n",
      "Epoch  501/1000 | Train Loss: 0.0484 | Val Loss: 0.2158\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.1453 | Val Loss: 1.1108\n",
      "Epoch  101/1000 | Train Loss: 0.5250 | Val Loss: 0.3716\n",
      "Epoch  201/1000 | Train Loss: 0.4262 | Val Loss: 0.2338\n",
      "Epoch  301/1000 | Train Loss: 0.3755 | Val Loss: 0.1689\n",
      "Epoch  401/1000 | Train Loss: 0.2481 | Val Loss: 0.1291\n",
      "Epoch  501/1000 | Train Loss: 0.2526 | Val Loss: 0.1020\n",
      "Epoch  601/1000 | Train Loss: 0.2602 | Val Loss: 0.0754\n",
      "Epoch  701/1000 | Train Loss: 0.2184 | Val Loss: 0.0647\n",
      "Epoch  801/1000 | Train Loss: 0.2409 | Val Loss: 0.0573\n",
      "Epoch  901/1000 | Train Loss: 0.1449 | Val Loss: 0.0527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:09,192] Trial 6 finished with value: 0.04842751628387452 and parameters: {'n_layers': 3, 'n_units_l0': 16, 'n_units_l1': 104, 'n_units_l2': 45, 'lr': 0.0052713111200111485, 'batch_size': 8, 'dropout_l0': 0.3501631356684692, 'dropout_l1': 0.3427330066670643, 'dropout_l2': 0.15786598045307998, 'weight_decay': 0.005411739452991505, 'optimizer': 'momentum', 'stopping_patience': 271, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1273 | Val Loss: 1.0792\n",
      "Epoch  101/1000 | Train Loss: 1.0754 | Val Loss: 1.0561\n",
      "Epoch  201/1000 | Train Loss: 1.0582 | Val Loss: 1.0376\n",
      "Epoch  301/1000 | Train Loss: 1.0372 | Val Loss: 1.0229\n",
      "Epoch  401/1000 | Train Loss: 1.0246 | Val Loss: 1.0109\n",
      "Epoch  501/1000 | Train Loss: 1.0174 | Val Loss: 1.0004\n",
      "Epoch  601/1000 | Train Loss: 0.9904 | Val Loss: 0.9910\n",
      "Epoch  701/1000 | Train Loss: 1.0044 | Val Loss: 0.9823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:09,708] Trial 7 finished with value: 0.9582822087117763 and parameters: {'n_layers': 1, 'n_units_l0': 38, 'lr': 0.00010307132317128439, 'batch_size': 16, 'dropout_l0': 0.20613952460137597, 'weight_decay': 5.034976110346226e-05, 'optimizer': 'momentum', 'stopping_patience': 75, 'activation_l0': 'sigmoid'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  801/1000 | Train Loss: 1.0065 | Val Loss: 0.9740\n",
      "Epoch  901/1000 | Train Loss: 0.9934 | Val Loss: 0.9660\n",
      "Epoch    1/1000 | Train Loss: 1.0714 | Val Loss: 1.0071\n",
      "Epoch  101/1000 | Train Loss: 0.2024 | Val Loss: 0.2673\n",
      "Epoch  201/1000 | Train Loss: 0.1412 | Val Loss: 0.1779\n",
      "Epoch  301/1000 | Train Loss: 0.1158 | Val Loss: 0.1293\n",
      "Epoch  401/1000 | Train Loss: 0.0749 | Val Loss: 0.1181\n",
      "Epoch  501/1000 | Train Loss: 0.0631 | Val Loss: 0.1012\n",
      "Epoch  601/1000 | Train Loss: 0.1039 | Val Loss: 0.0963\n",
      "Epoch  701/1000 | Train Loss: 0.0514 | Val Loss: 0.0767\n",
      "Epoch  801/1000 | Train Loss: 0.0472 | Val Loss: 0.0609\n",
      "Epoch  901/1000 | Train Loss: 0.0691 | Val Loss: 0.0620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:10,324] Trial 8 finished with value: 0.056828971214531065 and parameters: {'n_layers': 3, 'n_units_l0': 87, 'n_units_l1': 74, 'n_units_l2': 27, 'lr': 0.021400702720680384, 'batch_size': 64, 'dropout_l0': 0.10794523583094173, 'dropout_l1': 0.13968627557451685, 'dropout_l2': 0.1263117855912126, 'weight_decay': 3.3665857648325408e-06, 'optimizer': 'sgd', 'stopping_patience': 421, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.2698 | Val Loss: 1.1924\n",
      "Epoch  101/1000 | Train Loss: 1.0050 | Val Loss: 0.8951\n",
      "Epoch  201/1000 | Train Loss: 0.7540 | Val Loss: 0.7360\n",
      "Epoch  301/1000 | Train Loss: 0.7051 | Val Loss: 0.6442\n",
      "Epoch  401/1000 | Train Loss: 0.6172 | Val Loss: 0.5864\n",
      "Epoch  501/1000 | Train Loss: 0.5745 | Val Loss: 0.5478\n",
      "Epoch  601/1000 | Train Loss: 0.5612 | Val Loss: 0.5201\n",
      "Epoch  701/1000 | Train Loss: 0.5582 | Val Loss: 0.4998\n",
      "Epoch  801/1000 | Train Loss: 0.4635 | Val Loss: 0.4841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:11,092] Trial 9 finished with value: 0.4606308944285112 and parameters: {'n_layers': 3, 'n_units_l0': 95, 'n_units_l1': 88, 'n_units_l2': 80, 'lr': 0.0003989478313079206, 'batch_size': 64, 'dropout_l0': 0.31922389833752984, 'dropout_l1': 0.36762435128729104, 'dropout_l2': 0.18560773422460164, 'weight_decay': 6.735110053316927e-05, 'optimizer': 'sgd', 'stopping_patience': 395, 'activation_l0': 'tanh', 'activation_l1': 'tanh', 'activation_l2': 'tanh'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  901/1000 | Train Loss: 0.4620 | Val Loss: 0.4710\n",
      "Epoch    1/1000 | Train Loss: 1.2802 | Val Loss: 1.1634\n",
      "Epoch  101/1000 | Train Loss: 0.8869 | Val Loss: 0.8076\n",
      "Epoch  201/1000 | Train Loss: 0.7265 | Val Loss: 0.5954\n",
      "Epoch  301/1000 | Train Loss: 0.6035 | Val Loss: 0.4984\n",
      "Epoch  401/1000 | Train Loss: 0.6386 | Val Loss: 0.4347\n",
      "Epoch  501/1000 | Train Loss: 0.5616 | Val Loss: 0.4087\n",
      "Epoch  601/1000 | Train Loss: 0.5329 | Val Loss: 0.3857\n",
      "Epoch  701/1000 | Train Loss: 0.5400 | Val Loss: 0.3663\n",
      "Epoch  801/1000 | Train Loss: 0.5288 | Val Loss: 0.3498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:12,552] Trial 10 finished with value: 0.32656020479024056 and parameters: {'n_layers': 3, 'n_units_l0': 5, 'n_units_l1': 34, 'n_units_l2': 14, 'lr': 0.0030172992140865356, 'batch_size': 8, 'dropout_l0': 0.48443186750869716, 'dropout_l1': 0.09448941855758058, 'dropout_l2': 0.41756452743907657, 'weight_decay': 0.009592668108817088, 'optimizer': 'momentum', 'stopping_patience': 195, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  901/1000 | Train Loss: 0.4886 | Val Loss: 0.3374\n",
      "Epoch    1/1000 | Train Loss: 1.1416 | Val Loss: 1.1203\n",
      "Epoch  101/1000 | Train Loss: 0.6107 | Val Loss: 0.6056\n",
      "Epoch  201/1000 | Train Loss: 0.4063 | Val Loss: 0.4028\n",
      "Epoch  301/1000 | Train Loss: 0.3325 | Val Loss: 0.3067\n",
      "Epoch  401/1000 | Train Loss: 0.3015 | Val Loss: 0.2497\n",
      "Epoch  501/1000 | Train Loss: 0.2022 | Val Loss: 0.2091\n",
      "Epoch  601/1000 | Train Loss: 0.2464 | Val Loss: 0.1743\n",
      "Epoch  701/1000 | Train Loss: 0.1704 | Val Loss: 0.1472\n",
      "Epoch  801/1000 | Train Loss: 0.1651 | Val Loss: 0.1260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:13,226] Trial 11 finished with value: 0.09499182512356667 and parameters: {'n_layers': 3, 'n_units_l0': 86, 'n_units_l1': 68, 'n_units_l2': 23, 'lr': 0.010675962286277811, 'batch_size': 64, 'dropout_l0': 0.07636440454992713, 'dropout_l1': 0.11504010242654128, 'dropout_l2': 0.016886613777264525, 'weight_decay': 0.0007375983189434509, 'optimizer': 'momentum', 'stopping_patience': 272, 'activation_l0': 'leaky_relu', 'activation_l1': 'sigmoid', 'activation_l2': 'leaky_relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  901/1000 | Train Loss: 0.1580 | Val Loss: 0.1085\n",
      "Epoch    1/1000 | Train Loss: 1.4085 | Val Loss: 1.2820\n",
      "Epoch  101/1000 | Train Loss: 0.3523 | Val Loss: 0.2993\n",
      "Epoch  201/1000 | Train Loss: 0.2654 | Val Loss: 0.2213\n",
      "Epoch  301/1000 | Train Loss: 0.2147 | Val Loss: 0.1731\n",
      "Epoch  401/1000 | Train Loss: 0.1509 | Val Loss: 0.1435\n",
      "Epoch  501/1000 | Train Loss: 0.1646 | Val Loss: 0.1278\n",
      "Epoch  601/1000 | Train Loss: 0.1634 | Val Loss: 0.1185\n",
      "Epoch  701/1000 | Train Loss: 0.1527 | Val Loss: 0.1117\n",
      "Epoch  801/1000 | Train Loss: 0.1140 | Val Loss: 0.1079\n",
      "Epoch  901/1000 | Train Loss: 0.1647 | Val Loss: 0.1031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:13,610] Trial 12 finished with value: 0.0988867594126542 and parameters: {'n_layers': 2, 'n_units_l0': 6, 'n_units_l1': 77, 'lr': 0.012800235094663059, 'batch_size': 64, 'dropout_l0': 0.09938216828997923, 'dropout_l1': 0.24571242891797498, 'weight_decay': 1.3453581508920291e-06, 'optimizer': 'momentum', 'stopping_patience': 275, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1910 | Val Loss: 1.1391\n",
      "Epoch  101/1000 | Train Loss: 0.3260 | Val Loss: 0.3433\n",
      "Epoch  201/1000 | Train Loss: 0.2655 | Val Loss: 0.2462\n",
      "Epoch  301/1000 | Train Loss: 0.2143 | Val Loss: 0.1948\n",
      "Epoch  401/1000 | Train Loss: 0.1647 | Val Loss: 0.1681\n",
      "Epoch  501/1000 | Train Loss: 0.1282 | Val Loss: 0.1413\n",
      "Epoch  601/1000 | Train Loss: 0.0949 | Val Loss: 0.1173\n",
      "Epoch  701/1000 | Train Loss: 0.0871 | Val Loss: 0.1126\n",
      "Epoch  801/1000 | Train Loss: 0.0892 | Val Loss: 0.1031\n",
      "Epoch  901/1000 | Train Loss: 0.1087 | Val Loss: 0.1020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:15,947] Trial 13 finished with value: 0.10532007115340702 and parameters: {'n_layers': 3, 'n_units_l0': 80, 'n_units_l1': 97, 'n_units_l2': 42, 'lr': 0.002425031848219533, 'batch_size': 8, 'dropout_l0': 0.34637359562730197, 'dropout_l1': 0.17096053325181956, 'dropout_l2': 0.2074667633042208, 'weight_decay': 0.0005773957057533575, 'optimizer': 'momentum', 'stopping_patience': 179, 'activation_l0': 'leaky_relu', 'activation_l1': 'tanh', 'activation_l2': 'leaky_relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1083 | Val Loss: 0.7853\n",
      "Epoch  101/1000 | Train Loss: 0.1268 | Val Loss: 0.0453\n",
      "Epoch  201/1000 | Train Loss: 0.1330 | Val Loss: 0.0457\n",
      "Epoch  301/1000 | Train Loss: 0.0788 | Val Loss: 0.0657\n",
      "Epoch  401/1000 | Train Loss: 0.1339 | Val Loss: 0.0841\n",
      "Epoch  501/1000 | Train Loss: 0.1092 | Val Loss: 0.0430\n",
      "Epoch  601/1000 | Train Loss: 0.1162 | Val Loss: 0.0421\n",
      "Epoch  701/1000 | Train Loss: 0.1006 | Val Loss: 0.0484\n",
      "Epoch  801/1000 | Train Loss: 0.0895 | Val Loss: 0.0421\n",
      "Epoch  901/1000 | Train Loss: 0.1121 | Val Loss: 0.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:17,747] Trial 14 finished with value: 0.05241113522796703 and parameters: {'n_layers': 3, 'n_units_l0': 24, 'n_units_l1': 47, 'n_units_l2': 121, 'lr': 0.09954501079038494, 'batch_size': 8, 'dropout_l0': 0.11308009076400133, 'dropout_l1': 0.02560740180945227, 'dropout_l2': 0.13831136495567628, 'weight_decay': 0.007681891802620772, 'optimizer': 'sgd', 'stopping_patience': 340, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.0741 | Val Loss: 1.0089\n",
      "Epoch  101/1000 | Train Loss: 0.1513 | Val Loss: 0.1198\n",
      "Epoch  201/1000 | Train Loss: 0.1341 | Val Loss: 0.1099\n",
      "Epoch  301/1000 | Train Loss: 0.1360 | Val Loss: 0.1071\n",
      "Epoch  401/1000 | Train Loss: 0.1350 | Val Loss: 0.1048\n",
      "Epoch  501/1000 | Train Loss: 0.1286 | Val Loss: 0.1021\n",
      "Epoch  601/1000 | Train Loss: 0.1282 | Val Loss: 0.1039\n",
      "Epoch  701/1000 | Train Loss: 0.1241 | Val Loss: 0.1032\n",
      "Epoch  801/1000 | Train Loss: 0.1411 | Val Loss: 0.1021\n",
      "Epoch  901/1000 | Train Loss: 0.1352 | Val Loss: 0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:18,977] Trial 15 finished with value: 0.1041544242000527 and parameters: {'n_layers': 2, 'n_units_l0': 23, 'n_units_l1': 48, 'lr': 0.08775370277530903, 'batch_size': 8, 'dropout_l0': 0.4144189754513643, 'dropout_l1': 0.015445620050883957, 'weight_decay': 0.008721066392037832, 'optimizer': 'momentum', 'stopping_patience': 329, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1770 | Val Loss: 1.1073\n",
      "Epoch  101/1000 | Train Loss: 0.9582 | Val Loss: 0.8900\n",
      "Epoch  201/1000 | Train Loss: 0.7559 | Val Loss: 0.6377\n",
      "Epoch  301/1000 | Train Loss: 0.6580 | Val Loss: 0.4865\n",
      "Epoch  401/1000 | Train Loss: 0.5542 | Val Loss: 0.3970\n",
      "Epoch  501/1000 | Train Loss: 0.5075 | Val Loss: 0.3323\n",
      "Epoch  601/1000 | Train Loss: 0.5561 | Val Loss: 0.2949\n",
      "Epoch  701/1000 | Train Loss: 0.4125 | Val Loss: 0.2631\n",
      "Epoch  801/1000 | Train Loss: 0.3685 | Val Loss: 0.2384\n",
      "Epoch  901/1000 | Train Loss: 0.4040 | Val Loss: 0.2195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:20,454] Trial 16 finished with value: 0.2025862486786637 and parameters: {'n_layers': 3, 'n_units_l0': 21, 'n_units_l1': 10, 'n_units_l2': 124, 'lr': 0.0013780592914926964, 'batch_size': 8, 'dropout_l0': 0.2781892842035234, 'dropout_l1': 0.363735525070744, 'dropout_l2': 0.31179792115887073, 'weight_decay': 0.0029419671509996344, 'optimizer': 'sgd', 'stopping_patience': 206, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid', 'activation_l2': 'relu'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.2469 | Val Loss: 1.2220\n",
      "Epoch  101/1000 | Train Loss: 0.4283 | Val Loss: 0.4675\n",
      "Epoch  201/1000 | Train Loss: 0.2897 | Val Loss: 0.3240\n",
      "Epoch  301/1000 | Train Loss: 0.2172 | Val Loss: 0.2328\n",
      "Epoch  401/1000 | Train Loss: 0.1551 | Val Loss: 0.1757\n",
      "Epoch  501/1000 | Train Loss: 0.1312 | Val Loss: 0.1424\n",
      "Epoch  601/1000 | Train Loss: 0.1187 | Val Loss: 0.1219\n",
      "Epoch  701/1000 | Train Loss: 0.1077 | Val Loss: 0.1087\n",
      "Epoch  801/1000 | Train Loss: 0.0999 | Val Loss: 0.1002\n",
      "Epoch  901/1000 | Train Loss: 0.1031 | Val Loss: 0.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:21,723] Trial 17 finished with value: 0.09128774576174337 and parameters: {'n_layers': 2, 'n_units_l0': 20, 'n_units_l1': 50, 'lr': 0.007522500684467407, 'batch_size': 8, 'dropout_l0': 0.04210686970052627, 'dropout_l1': 0.023447095402742743, 'weight_decay': 0.00021855912573582478, 'optimizer': 'momentum', 'stopping_patience': 321, 'activation_l0': 'tanh', 'activation_l1': 'sigmoid'}. Best is trial 6 with value: 0.04842751628387452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.0667 | Val Loss: 0.8342\n",
      "Epoch  101/1000 | Train Loss: 0.1067 | Val Loss: 0.0657\n",
      "Epoch  201/1000 | Train Loss: 0.1031 | Val Loss: 0.0467\n",
      "Epoch  301/1000 | Train Loss: 0.0688 | Val Loss: 0.0343\n",
      "Epoch  401/1000 | Train Loss: 0.0836 | Val Loss: 0.0356\n",
      "Epoch  501/1000 | Train Loss: 0.0903 | Val Loss: 0.0443\n",
      "Epoch  601/1000 | Train Loss: 0.0918 | Val Loss: 0.0396\n",
      "Epoch  701/1000 | Train Loss: 0.0738 | Val Loss: 0.0302\n",
      "Epoch  801/1000 | Train Loss: 0.0597 | Val Loss: 0.0265\n",
      "Epoch  901/1000 | Train Loss: 0.0672 | Val Loss: 0.0284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:23,327] Trial 18 finished with value: 0.017337688950080393 and parameters: {'n_layers': 3, 'n_units_l0': 30, 'n_units_l1': 55, 'n_units_l2': 112, 'lr': 0.026129900092584576, 'batch_size': 8, 'dropout_l0': 0.15805123407006877, 'dropout_l1': 0.499026389027496, 'dropout_l2': 0.25776700649006323, 'weight_decay': 0.0031408221572012556, 'optimizer': 'sgd', 'stopping_patience': 237, 'activation_l0': 'tanh', 'activation_l1': 'relu', 'activation_l2': 'relu'}. Best is trial 18 with value: 0.017337688950080393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.3272 | Val Loss: 1.2166\n",
      "Epoch  101/1000 | Train Loss: 0.2438 | Val Loss: 0.3180\n",
      "Epoch  201/1000 | Train Loss: 0.1632 | Val Loss: 0.2450\n",
      "Epoch  301/1000 | Train Loss: 0.1207 | Val Loss: 0.2223\n",
      "Epoch  401/1000 | Train Loss: 0.0851 | Val Loss: 0.1996\n",
      "Epoch  501/1000 | Train Loss: 0.0983 | Val Loss: 0.1934\n",
      "Epoch  601/1000 | Train Loss: 0.0538 | Val Loss: 0.1792\n",
      "Epoch  701/1000 | Train Loss: 0.1068 | Val Loss: 0.1844\n",
      "Epoch  801/1000 | Train Loss: 0.0558 | Val Loss: 0.1715\n",
      "Epoch  901/1000 | Train Loss: 0.0764 | Val Loss: 0.1623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:24,824] Trial 19 finished with value: 0.17196665101572542 and parameters: {'n_layers': 2, 'n_units_l0': 37, 'n_units_l1': 127, 'lr': 0.004885502978438331, 'batch_size': 8, 'dropout_l0': 0.1624511826398002, 'dropout_l1': 0.4977285631068534, 'weight_decay': 0.002225912346007973, 'optimizer': 'momentum', 'stopping_patience': 155, 'activation_l0': 'tanh', 'activation_l1': 'relu'}. Best is trial 18 with value: 0.017337688950080393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1175 | Val Loss: 1.0053\n",
      "Epoch  101/1000 | Train Loss: 0.1761 | Val Loss: 0.0911\n",
      "Epoch  201/1000 | Train Loss: 0.1580 | Val Loss: 0.0633\n",
      "Epoch  301/1000 | Train Loss: 0.0557 | Val Loss: 0.0364\n",
      "Epoch  401/1000 | Train Loss: 0.0630 | Val Loss: 0.0348\n",
      "Epoch  501/1000 | Train Loss: 0.0340 | Val Loss: 0.0273\n",
      "Epoch  601/1000 | Train Loss: 0.0435 | Val Loss: 0.0271\n",
      "Epoch  701/1000 | Train Loss: 0.0324 | Val Loss: 0.0276\n",
      "Epoch  801/1000 | Train Loss: 0.0305 | Val Loss: 0.0174\n",
      "Epoch  901/1000 | Train Loss: 0.0503 | Val Loss: 0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:27,039] Trial 20 finished with value: 0.02659424319589889 and parameters: {'n_layers': 3, 'n_units_l0': 63, 'n_units_l1': 89, 'n_units_l2': 57, 'lr': 0.02526551718850021, 'batch_size': 8, 'dropout_l0': 0.3010448960736422, 'dropout_l1': 0.42079147161473013, 'dropout_l2': 0.2998118085077935, 'weight_decay': 0.00023691105983176367, 'optimizer': 'momentum', 'stopping_patience': 238, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid'}. Best is trial 18 with value: 0.017337688950080393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1071 | Val Loss: 1.0647\n",
      "Epoch  101/1000 | Train Loss: 0.1474 | Val Loss: 0.1044\n",
      "Epoch  201/1000 | Train Loss: 0.1225 | Val Loss: 0.0696\n",
      "Epoch  301/1000 | Train Loss: 0.0955 | Val Loss: 0.0490\n",
      "Epoch  401/1000 | Train Loss: 0.0621 | Val Loss: 0.0488\n",
      "Epoch  501/1000 | Train Loss: 0.0810 | Val Loss: 0.0372\n",
      "Epoch  601/1000 | Train Loss: 0.0441 | Val Loss: 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:28,713] Trial 21 finished with value: 0.023341919962152492 and parameters: {'n_layers': 3, 'n_units_l0': 69, 'n_units_l1': 97, 'n_units_l2': 55, 'lr': 0.02832639112081792, 'batch_size': 8, 'dropout_l0': 0.29737174388181636, 'dropout_l1': 0.4169984204641556, 'dropout_l2': 0.3048241552048301, 'weight_decay': 0.00017154997299348172, 'optimizer': 'momentum', 'stopping_patience': 247, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid'}. Best is trial 18 with value: 0.017337688950080393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  701/1000 | Train Loss: 0.0488 | Val Loss: 0.0324\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.3130 | Val Loss: 1.1106\n",
      "Epoch  101/1000 | Train Loss: 0.2418 | Val Loss: 0.0886\n",
      "Epoch  201/1000 | Train Loss: 0.0968 | Val Loss: 0.0411\n",
      "Epoch  301/1000 | Train Loss: 0.1001 | Val Loss: 0.0384\n",
      "Epoch  401/1000 | Train Loss: 0.0928 | Val Loss: 0.0277\n",
      "Epoch  501/1000 | Train Loss: 0.0765 | Val Loss: 0.0294\n",
      "Epoch  601/1000 | Train Loss: 0.0590 | Val Loss: 0.0286\n",
      "Epoch  701/1000 | Train Loss: 0.0307 | Val Loss: 0.0185\n",
      "Epoch  801/1000 | Train Loss: 0.0452 | Val Loss: 0.0142\n",
      "Epoch  901/1000 | Train Loss: 0.0205 | Val Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:30,838] Trial 22 finished with value: 0.023886254096415437 and parameters: {'n_layers': 3, 'n_units_l0': 69, 'n_units_l1': 86, 'n_units_l2': 60, 'lr': 0.027141071175002463, 'batch_size': 8, 'dropout_l0': 0.2933321158664899, 'dropout_l1': 0.4338574321035155, 'dropout_l2': 0.30422458027742905, 'weight_decay': 0.00019952834338635194, 'optimizer': 'momentum', 'stopping_patience': 239, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid'}. Best is trial 18 with value: 0.017337688950080393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1904 | Val Loss: 1.0810\n",
      "Epoch  101/1000 | Train Loss: 0.1948 | Val Loss: 0.1143\n",
      "Epoch  201/1000 | Train Loss: 0.0880 | Val Loss: 0.0450\n",
      "Epoch  301/1000 | Train Loss: 0.1425 | Val Loss: 0.0300\n",
      "Epoch  401/1000 | Train Loss: 0.0822 | Val Loss: 0.0283\n",
      "Epoch  501/1000 | Train Loss: 0.0649 | Val Loss: 0.0217\n",
      "Epoch  601/1000 | Train Loss: 0.0490 | Val Loss: 0.0159\n",
      "Epoch  701/1000 | Train Loss: 0.0457 | Val Loss: 0.0152\n",
      "Epoch  801/1000 | Train Loss: 0.0838 | Val Loss: 0.0111\n",
      "Epoch  901/1000 | Train Loss: 0.0268 | Val Loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:33,079] Trial 23 finished with value: 0.013524682356693874 and parameters: {'n_layers': 3, 'n_units_l0': 75, 'n_units_l1': 59, 'n_units_l2': 95, 'lr': 0.021672574817019934, 'batch_size': 8, 'dropout_l0': 0.2395219381589836, 'dropout_l1': 0.43188923340156543, 'dropout_l2': 0.3822926345597569, 'weight_decay': 0.0002204204000948876, 'optimizer': 'momentum', 'stopping_patience': 121, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid'}. Best is trial 23 with value: 0.013524682356693874.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/1000 | Train Loss: 1.1816 | Val Loss: 1.0595\n",
      "Epoch  101/1000 | Train Loss: 0.2499 | Val Loss: 0.1756\n",
      "Epoch  201/1000 | Train Loss: 0.1214 | Val Loss: 0.0764\n",
      "Epoch  301/1000 | Train Loss: 0.1082 | Val Loss: 0.0539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:34,256] Trial 24 finished with value: 0.036906992770515616 and parameters: {'n_layers': 3, 'n_units_l0': 73, 'n_units_l1': 57, 'n_units_l2': 98, 'lr': 0.015678703841895056, 'batch_size': 8, 'dropout_l0': 0.15638810526034685, 'dropout_l1': 0.4270212484506206, 'dropout_l2': 0.4142444593306671, 'weight_decay': 0.00038122659952673026, 'optimizer': 'momentum', 'stopping_patience': 121, 'activation_l0': 'relu', 'activation_l1': 'relu', 'activation_l2': 'sigmoid'}. Best is trial 23 with value: 0.013524682356693874.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  401/1000 | Train Loss: 0.0794 | Val Loss: 0.0448\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.0754 | Val Loss: 0.7157\n",
      "Epoch  101/1000 | Train Loss: 0.0776 | Val Loss: 0.1268\n",
      "Epoch  201/1000 | Train Loss: 0.0512 | Val Loss: 0.1251\n",
      "Epoch  301/1000 | Train Loss: 0.0376 | Val Loss: 0.0957\n",
      "Epoch  401/1000 | Train Loss: 0.1016 | Val Loss: 0.0219\n",
      "Epoch  501/1000 | Train Loss: 0.0359 | Val Loss: 0.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:35,219] Trial 25 finished with value: 0.01132955380443001 and parameters: {'n_layers': 2, 'n_units_l0': 52, 'n_units_l1': 58, 'lr': 0.040241812498449325, 'batch_size': 8, 'dropout_l0': 0.2375860985027711, 'dropout_l1': 0.30959928527576625, 'weight_decay': 0.00011775430478344215, 'optimizer': 'momentum', 'stopping_patience': 141, 'activation_l0': 'relu', 'activation_l1': 'relu'}. Best is trial 25 with value: 0.01132955380443001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  601/1000 | Train Loss: 0.0220 | Val Loss: 0.0280\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 0.8920 | Val Loss: 0.5790\n",
      "Epoch  101/1000 | Train Loss: 0.0926 | Val Loss: 0.1317\n",
      "Epoch  201/1000 | Train Loss: 0.0765 | Val Loss: 0.0803\n",
      "Epoch  301/1000 | Train Loss: 0.0348 | Val Loss: 0.0490\n",
      "Epoch  401/1000 | Train Loss: 0.0479 | Val Loss: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:35,930] Trial 26 finished with value: 0.01860569771923478 and parameters: {'n_layers': 2, 'n_units_l0': 53, 'n_units_l1': 59, 'lr': 0.048550386512943224, 'batch_size': 8, 'dropout_l0': 0.2382630230530695, 'dropout_l1': 0.294344098521903, 'weight_decay': 0.001282129917067116, 'optimizer': 'sgd', 'stopping_patience': 133, 'activation_l0': 'relu', 'activation_l1': 'relu'}. Best is trial 25 with value: 0.01132955380443001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  501/1000 | Train Loss: 0.0546 | Val Loss: 0.0490\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.2705 | Val Loss: 1.0296\n",
      "Epoch  101/1000 | Train Loss: 0.0746 | Val Loss: 0.1854\n",
      "Epoch  201/1000 | Train Loss: 0.0876 | Val Loss: 0.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:36,112] Trial 27 finished with value: 0.11935519493464784 and parameters: {'n_layers': 2, 'n_units_l0': 101, 'n_units_l1': 35, 'lr': 0.06175708700694599, 'batch_size': 32, 'dropout_l0': 0.1523858688813893, 'dropout_l1': 0.3103005042811366, 'weight_decay': 2.571112650627635e-05, 'optimizer': 'momentum', 'stopping_patience': 51, 'activation_l0': 'relu', 'activation_l1': 'relu'}. Best is trial 25 with value: 0.01132955380443001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.3131 | Val Loss: 0.9895\n",
      "Epoch  101/1000 | Train Loss: 0.2547 | Val Loss: 0.2346\n",
      "Epoch  201/1000 | Train Loss: 0.1700 | Val Loss: 0.1981\n",
      "Epoch  301/1000 | Train Loss: 0.1154 | Val Loss: 0.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:36,653] Trial 28 finished with value: 0.14437511906874798 and parameters: {'n_layers': 2, 'n_units_l0': 33, 'n_units_l1': 61, 'lr': 0.016276308633749667, 'batch_size': 16, 'dropout_l0': 0.24120903963750795, 'dropout_l1': 0.4691665507402243, 'weight_decay': 0.00011871271376116145, 'optimizer': 'sgd', 'stopping_patience': 164, 'activation_l0': 'relu', 'activation_l1': 'relu'}. Best is trial 25 with value: 0.01132955380443001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  401/1000 | Train Loss: 0.1557 | Val Loss: 0.1571\n",
      "Epoch  501/1000 | Train Loss: 0.1392 | Val Loss: 0.1677\n",
      "Early stopping triggered.\n",
      "Epoch    1/1000 | Train Loss: 1.0556 | Val Loss: 0.8820\n",
      "Epoch  101/1000 | Train Loss: 0.1683 | Val Loss: 0.2486\n",
      "Epoch  201/1000 | Train Loss: 0.1013 | Val Loss: 0.1895\n",
      "Epoch  301/1000 | Train Loss: 0.0972 | Val Loss: 0.1746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-25 23:33:36,838] Trial 29 finished with value: 0.149102847273756 and parameters: {'n_layers': 1, 'n_units_l0': 44, 'lr': 0.0369171919181557, 'batch_size': 32, 'dropout_l0': 0.22068576755110658, 'weight_decay': 1.2540206464724856e-05, 'optimizer': 'sgd', 'stopping_patience': 105, 'activation_l0': 'relu'}. Best is trial 25 with value: 0.01132955380443001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  401/1000 | Train Loss: 0.0689 | Val Loss: 0.1655\n",
      "Epoch  501/1000 | Train Loss: 0.1370 | Val Loss: 0.1614\n",
      "Early stopping triggered.\n",
      "Best trial:\n",
      "  Value: 0.0113\n",
      "  Params:\n",
      "    n_layers: 2\n",
      "    n_units_l0: 52\n",
      "    n_units_l1: 58\n",
      "    lr: 0.040241812498449325\n",
      "    batch_size: 8\n",
      "    dropout_l0: 0.2375860985027711\n",
      "    dropout_l1: 0.30959928527576625\n",
      "    weight_decay: 0.00011775430478344215\n",
      "    optimizer: momentum\n",
      "    stopping_patience: 141\n",
      "    activation_l0: relu\n",
      "    activation_l1: relu\n",
      "Epoch    1/5000 | Train Loss: 1.0754 | Val Loss: 0.7157\n",
      "Epoch  101/5000 | Train Loss: 0.0776 | Val Loss: 0.1268\n",
      "Epoch  201/5000 | Train Loss: 0.0512 | Val Loss: 0.1251\n",
      "Epoch  301/5000 | Train Loss: 0.0376 | Val Loss: 0.0957\n",
      "Epoch  401/5000 | Train Loss: 0.1016 | Val Loss: 0.0219\n",
      "Epoch  501/5000 | Train Loss: 0.0359 | Val Loss: 0.0297\n",
      "Epoch  601/5000 | Train Loss: 0.0220 | Val Loss: 0.0280\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 95.65%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch improved\n",
    "# Dataset: Iris (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from functools import partial\n",
    "import optuna\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Normalize features for better convergence\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        s = 1.0 / (1.0 + np.exp(-x))\n",
    "        return s * (1.0 - s)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    t = np.tanh(x)\n",
    "    if is_derivative:\n",
    "        return 1.0 - t * t\n",
    "    return t\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        # return 1 where x>0, alpha otherwise\n",
    "        grad = np.ones_like(x)\n",
    "        grad[x < 0] = alpha\n",
    "        return grad\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    diff = Y - Y_pred.T\n",
    "    return 0.5 * np.mean(diff * diff)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    Y_pred = np.clip(Y_pred.T, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(Y * np.log(Y_pred), axis=1))\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims, activations):\n",
    "    np.random.seed(42)\n",
    "    sqrt = np.sqrt\n",
    "    standard_normal = np.random.standard_normal\n",
    "\n",
    "    weights = []\n",
    "    biases = []\n",
    "\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        fan_in = layer_dims[i - 1]\n",
    "        fan_out = layer_dims[i]\n",
    "\n",
    "        act = activations[i - 1]\n",
    "        scale = sqrt(2.0 / fan_in) if act in (\"relu\", \"leaky_relu\") else sqrt(1.0 / fan_in)\n",
    "\n",
    "        # allocate directly\n",
    "        W = standard_normal((fan_out, fan_in)).astype(np.float32) * scale\n",
    "        b = np.zeros((fan_out, 1), dtype=np.float32)\n",
    "\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, weights, biases, activations, dropout_rates=None, training=True):\n",
    "    A = X.T\n",
    "    cache = [A]  # store activations for backprop\n",
    "    Zs = []      # store pre-activations (Z values)\n",
    "    masks = []   # store dropout masks\n",
    "\n",
    "    # Activation function map for direct access\n",
    "    act_funcs = {\n",
    "        \"sigmoid\": sigmoid,\n",
    "        \"relu\": relu,\n",
    "        \"tanh\": tanh,\n",
    "        \"leaky_relu\": leaky_relu,\n",
    "        \"softmax\": softmax\n",
    "    }\n",
    "\n",
    "    for i, (W, b, act_name) in enumerate(zip(weights, biases, activations)):\n",
    "        Z = W @ A + b\n",
    "        Zs.append(Z)\n",
    "\n",
    "        A = act_funcs[act_name](Z)\n",
    "\n",
    "        # Dropout (training only, not for output layer)\n",
    "        if training and dropout_rates and i < len(dropout_rates):\n",
    "            rate = dropout_rates[i]\n",
    "            mask = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A *= mask\n",
    "            A /= (1 - rate)\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            masks.append(None)\n",
    "\n",
    "        cache.append(A)\n",
    "\n",
    "    return A, (cache, Zs, masks)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, weights, cache_data, activations, dropout_rates=None, weight_decay=0.0):\n",
    "    cache, Zs, masks = cache_data\n",
    "    m = Y.shape[0]\n",
    "    L = len(weights)\n",
    "\n",
    "    dWs = [None] * L\n",
    "    dbs = [None] * L\n",
    "\n",
    "    Y = Y.T\n",
    "    dZ = cache[-1] - Y  # derivative of softmax-crossentropy\n",
    "\n",
    "    # Activation derivative map\n",
    "    act_derivs = {\n",
    "        \"sigmoid\": lambda Z: sigmoid(Z, True),\n",
    "        \"relu\": lambda Z: relu(Z, True),\n",
    "        \"tanh\": lambda Z: tanh(Z, True),\n",
    "        \"leaky_relu\": lambda Z: leaky_relu(Z, True)\n",
    "    }\n",
    "\n",
    "    for i in reversed(range(L)):\n",
    "        A_prev = cache[i]\n",
    "        W = weights[i]\n",
    "\n",
    "        # Gradients\n",
    "        dWs[i] = (dZ @ A_prev.T) / m + weight_decay * W\n",
    "        dbs[i] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Gradient clipping (optional, can disable for extra speed)\n",
    "        # dWs[i] = np.clip(dWs[i], -5, 5)\n",
    "        # dbs[i] = np.clip(dbs[i], -5, 5)\n",
    "\n",
    "        if i > 0:\n",
    "            dA_prev = W.T @ dZ\n",
    "\n",
    "            # Apply dropout if used\n",
    "            if masks[i - 1] is not None:\n",
    "                dA_prev *= masks[i - 1]\n",
    "                dA_prev /= (1 - dropout_rates[i - 1])\n",
    "\n",
    "            # Apply activation derivative\n",
    "            dZ = dA_prev * act_derivs[activations[i - 1]](Zs[i - 1])\n",
    "\n",
    "    return dWs, dbs\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    ^(t+1) <- ^t - L(y, y)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- v^t + (1-)L(y, y)^t\n",
    "    ^(t+1) <- ^t - v^(t+1)\n",
    "\n",
    "    Adam:\n",
    "    m^(t+1) <- 1 m^t + (1-1)L(y, y)^t\n",
    "    v^(t+1) <- 2 v^t + (1-2)(L(y, y)^t)^2\n",
    "    m^(t+1) <- m^(t+1) / (1 - 1^(t+1))\n",
    "    v^(t+1) <- v^(t+1) / (1 - 2^(t+1))\n",
    "    ^(t+1) <- ^t -  m^(t+1) / ((v^(t+1)) + )\n",
    "'''\n",
    "def update_parameters(weights, biases, dWs, dbs, lr, optimizer_type, velocity=None, beta=0.9, m=None, v=None, t=1, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "    L = len(weights)\n",
    "\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(L):\n",
    "            weights[i] -= lr * dWs[i]\n",
    "            biases[i] -= lr * dbs[i]\n",
    "\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {\"W\": [np.zeros_like(W) for W in weights], \"b\": [np.zeros_like(b) for b in biases]}\n",
    "        for i in range(L):\n",
    "            velocity[\"W\"][i] = beta * velocity[\"W\"][i] + (1 - beta) * dWs[i]\n",
    "            velocity[\"b\"][i] = beta * velocity[\"b\"][i] + (1 - beta) * dbs[i]\n",
    "            weights[i] -= lr * velocity[\"W\"][i]\n",
    "            biases[i] -= lr * velocity[\"b\"][i]\n",
    "\n",
    "    elif optimizer_type == \"adam\":\n",
    "        if m is None:\n",
    "            m = {\"W\": [np.zeros_like(W) for W in weights], \"b\": [np.zeros_like(b) for b in biases]}\n",
    "            v = {\"W\": [np.zeros_like(W) for W in weights], \"b\": [np.zeros_like(b) for b in biases]}\n",
    "\n",
    "        for i in range(L):\n",
    "            # Update moment estimates\n",
    "            m[\"W\"][i] = beta1 * m[\"W\"][i] + (1 - beta1) * dWs[i]\n",
    "            m[\"b\"][i] = beta1 * m[\"b\"][i] + (1 - beta1) * dbs[i]\n",
    "            v[\"W\"][i] = beta2 * v[\"W\"][i] + (1 - beta2) * (dWs[i] ** 2)\n",
    "            v[\"b\"][i] = beta2 * v[\"b\"][i] + (1 - beta2) * (dbs[i] ** 2)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat_W = m[\"W\"][i] / (1 - beta1 ** t)\n",
    "            m_hat_b = m[\"b\"][i] / (1 - beta1 ** t)\n",
    "            v_hat_W = v[\"W\"][i] / (1 - beta2 ** t)\n",
    "            v_hat_b = v[\"b\"][i] / (1 - beta2 ** t)\n",
    "\n",
    "            # Parameter update\n",
    "            weights[i] -= lr * m_hat_W / (np.sqrt(v_hat_W) + eps)\n",
    "            biases[i] -= lr * m_hat_b / (np.sqrt(v_hat_b) + eps)\n",
    "\n",
    "    return weights, biases, velocity, (m, v)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    X_val, \n",
    "    Y_val,\n",
    "    hidden_layers=[10], \n",
    "    lr=0.01, \n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=[0.1],\n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=100,\n",
    "    print_every=100,\n",
    "    plot_loss=True,\n",
    "):\n",
    "    input_dim, output_dim = X_train.shape[1], Y_train.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    # Initialization\n",
    "    weights, biases = initialize_parameters(layer_dims, activations)\n",
    "    velocity, (m, v) = None, (None, None)\n",
    "    best_loss, patience_counter = np.inf, 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Mini-batch iterator\n",
    "    def batch_iterator(X, Y, batch_size):\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            yield X[i:i+batch_size], Y[i:i+batch_size]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled, Y_shuffled = X_train[perm], Y_train[perm]\n",
    "\n",
    "        # Training by batch\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for X_batch, Y_batch in batch_iterator(X_shuffled, Y_shuffled, batch_size):\n",
    "            # Forward\n",
    "            A_out, cache_data = forward_propagation(X_batch, weights, biases, activations, dropout_rates)\n",
    "\n",
    "            # Compute loss\n",
    "            if loss_function == \"cross_entropy\":\n",
    "                loss = cross_entropy_loss(Y_batch, A_out)\n",
    "            else:\n",
    "                loss = mse_loss(Y_batch, A_out)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "\n",
    "            # Backward\n",
    "            dWs, dbs = backward_propagation(Y_batch, weights, cache_data, activations, dropout_rates, weight_decay)\n",
    "\n",
    "            # Update params\n",
    "            weights, biases, velocity, (m, v) = update_parameters(weights, biases, dWs, dbs, lr, optimizer_type, velocity=velocity, m=m, v=v, t=epoch+1)\n",
    "\n",
    "        # Validation loss\n",
    "        A_val, _ = forward_propagation(X_val, weights, biases, activations)\n",
    "        val_loss = cross_entropy_loss(Y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(Y_val, A_val)\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1:4d}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_weights = [W.copy() for W in weights]\n",
    "                best_biases = [b.copy() for b in biases]\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    weights, biases = best_weights, best_biases\n",
    "                    break\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.plot(train_losses, label=\"Train\")\n",
    "        plt.plot(val_losses, label=\"Val\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, weights, biases, activations):\n",
    "    A_out, _ = forward_propagation(X, weights, biases, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Hyperparameter Optimization with Optuna\n",
    "# =======================================\n",
    "def objective(trial, loss_function=\"cross_entropy\", epochs=1000):\n",
    "    # Hyperparameters to search\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    hidden_layers = [trial.suggest_int(f\"n_units_l{i}\", 5, 128) for i in range(n_layers)]\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 0.1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
    "    dropout_rates = [trial.suggest_float(f\"dropout_l{i}\", 0.0, 0.5) for i in range(n_layers)]\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"momentum\"])\n",
    "    stopping_patience = trial.suggest_int(\"stopping_patience\", 50, 500)\n",
    "    # Activation functions per layer\n",
    "    possible_activations = [\"relu\", \"sigmoid\", \"tanh\", \"leaky_relu\"]\n",
    "    activations = [trial.suggest_categorical(f\"activation_l{i}\", possible_activations) for i in range(n_layers)]\n",
    "    activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "    # Train network\n",
    "    weights, biases = train_neural_network(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        X_val, \n",
    "        y_val,\n",
    "        hidden_layers=hidden_layers,\n",
    "        lr=lr,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        activations=activations,\n",
    "        loss_function=loss_function,\n",
    "        optimizer_type=optimizer_type,\n",
    "        dropout_rates=dropout_rates,\n",
    "        weight_decay=weight_decay,\n",
    "        early_stopping=True,\n",
    "        patience=stopping_patience,\n",
    "        print_every=100, \n",
    "        plot_loss=False,\n",
    "    )\n",
    "\n",
    "    # Optuna tries to minimize the loss function\n",
    "    A_val, _ = forward_propagation(X_val, weights, biases, activations, training=False)\n",
    "    val_loss = cross_entropy_loss(y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(y_val, A_val)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna study\n",
    "loss_function = \"cross_entropy\"\n",
    "epochs = 1000\n",
    "obj = partial(objective, loss_function=loss_function, epochs=epochs)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(obj, n_trials=30)\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value:.4f}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# =======================================\n",
    "# 11. Retrain Best Model on Train + Val and Evaluate on Test\n",
    "# =======================================\n",
    "\n",
    "best_params = trial.params\n",
    "n_layers = best_params[\"n_layers\"]\n",
    "hidden_layers = [best_params[f\"n_units_l{i}\"] for i in range(n_layers)]\n",
    "lr = best_params[\"lr\"]\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "dropout_rates = [best_params[f\"dropout_l{i}\"] for i in range(n_layers)]\n",
    "weight_decay = best_params[\"weight_decay\"]\n",
    "optimizer_type = best_params[\"optimizer\"]\n",
    "stopping_patience = best_params[\"stopping_patience\"]\n",
    "activations = [best_params[f\"activation_l{i}\"] for i in range(n_layers)]\n",
    "activations.append(\"softmax\")  # output layer always softmax\n",
    "\n",
    "# Combine train and val sets\n",
    "weights, biases = train_neural_network(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_val, \n",
    "    y_val,\n",
    "    hidden_layers=hidden_layers,\n",
    "    lr=lr,\n",
    "    epochs=5000,\n",
    "    batch_size=batch_size,\n",
    "    activations=activations,\n",
    "    loss_function=loss_function,\n",
    "    optimizer_type=optimizer_type,\n",
    "    dropout_rates=dropout_rates,\n",
    "    weight_decay=weight_decay,\n",
    "    early_stopping=True,\n",
    "    patience=stopping_patience,\n",
    "    print_every=100,\n",
    "    plot_loss=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = predict(X_test, weights, biases, activations)\n",
    "y_test_true = np.argmax(y_test, axis=1)\n",
    "test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | Loss: 1.3451\n",
      "Epoch   500 | Loss: 1.0717\n",
      "Epoch  1000 | Loss: 0.9548\n",
      "Epoch  1500 | Loss: 0.5260\n",
      "Epoch  2000 | Loss: 0.3474\n",
      "Epoch  2500 | Loss: 0.2406\n",
      "Epoch  3000 | Loss: 0.1657\n",
      "Epoch  3500 | Loss: 0.1208\n",
      "Epoch  4000 | Loss: 0.0961\n",
      "Epoch  4500 | Loss: 0.0820\n",
      "Epoch  5000 | Loss: 0.0730\n",
      "\n",
      "Test Set Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch\n",
    "# Dataset: Iris (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Normalize features for better convergence\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# =======================================\n",
    "# 2. Helper Functions\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, 0, x)\n",
    "        x = np.where(x >= 0, 1, x)\n",
    "        return x\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_pred.T + eps)) / m\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Parameter Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1. / layer_dims[i - 1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Forward & Backward Propagation\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i - 1] == \"sigmoid\":\n",
    "            A = sigmoid(Z)\n",
    "        elif activations[i - 1] == \"relu\":\n",
    "            A = relu(Z)\n",
    "        elif activations[i - 1] == \"softmax\":\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        cache[f\"Z{i}\"] = Z\n",
    "        cache[f\"A{i}\"] = A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache\n",
    "\n",
    "def backward_propagation(Y, params, cache, activations):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    A_L = cache[f\"A{L}\"]\n",
    "\n",
    "    # Output layer gradient\n",
    "    dZ = A_L - Y\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = np.dot(W.T, dZ) * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = np.dot(W.T, dZ) * relu(Z_prev, is_derivative=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activations[i-2]}\")\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(params, grads, lr):\n",
    "    for key in params.keys():\n",
    "        if key.startswith(\"W\"):\n",
    "            idx = key[1:]\n",
    "            params[f\"W{idx}\"] -= lr * grads[f\"dW{idx}\"]\n",
    "            params[f\"b{idx}\"] -= lr * grads[f\"db{idx}\"]\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(X, Y, hidden_layers=[10], lr=0.05, epochs=5000, print_every=500, activations=None, loss_function=\"cross_entropy\"):\n",
    "    input_dim, output_dim = X.shape[1], Y.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "\n",
    "    if activations is None:\n",
    "        activations = [\"sigmoid\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "\n",
    "    for i in range(epochs + 1):\n",
    "        A_out, cache = forward_propagation(X, params, activations)\n",
    "\n",
    "        if loss_function == \"mse\":\n",
    "            loss = mse_loss(Y, A_out)\n",
    "        elif loss_function == \"cross_entropy\":\n",
    "            loss = cross_entropy_loss(Y, A_out)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "        grads = backward_propagation(Y, params, cache, activations)\n",
    "        params = update_parameters(params, grads, lr)\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(f\"Epoch {i:5d} | Loss: {loss:.4f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _ = forward_propagation(X, params, activations)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Run Training and Evaluate\n",
    "# =======================================\n",
    "params = train_neural_network(\n",
    "    X_train, y_train,\n",
    "    hidden_layers=[10, 10, 10],\n",
    "    lr=0.05,\n",
    "    epochs=5000,\n",
    "    print_every=500,\n",
    "    activations=[\"sigmoid\", \"sigmoid\", \"sigmoid\", \"softmax\"]\n",
    ")\n",
    "\n",
    "y_pred = predict(X_test, params, [\"sigmoid\", \"sigmoid\", \"sigmoid\", \"softmax\"])\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nTest Set Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36728c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "Epoch     0 | Loss: 1.3303\n",
      "Epoch  1000 | Loss: 0.9697\n",
      "Epoch  2000 | Loss: 0.3686\n",
      "Epoch  3000 | Loss: 0.1729\n",
      "Validation Accuracy: 96.67%\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "Epoch     0 | Loss: 1.3744\n",
      "Epoch  1000 | Loss: 0.9500\n",
      "Epoch  2000 | Loss: 0.3627\n",
      "Epoch  3000 | Loss: 0.1692\n",
      "Validation Accuracy: 96.67%\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "Epoch     0 | Loss: 1.3643\n",
      "Epoch  1000 | Loss: 0.9365\n",
      "Epoch  2000 | Loss: 0.3511\n",
      "Epoch  3000 | Loss: 0.1584\n",
      "Validation Accuracy: 96.67%\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "Epoch     0 | Loss: 1.3263\n",
      "Epoch  1000 | Loss: 0.9604\n",
      "Epoch  2000 | Loss: 0.3504\n",
      "Epoch  3000 | Loss: 0.1578\n",
      "Validation Accuracy: 93.33%\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "Epoch     0 | Loss: 1.3318\n",
      "Epoch  1000 | Loss: 0.9355\n",
      "Epoch  2000 | Loss: 0.3407\n",
      "Epoch  3000 | Loss: 0.1747\n",
      "Validation Accuracy: 96.67%\n",
      "\n",
      "K-Fold Mean Accuracy: 96.00% Â± 1.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(96.0), np.float64(1.333333333333337))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================\n",
    "# Shallow Neural Network from Scratch with K-Fold Cross-Validation\n",
    "# Dataset: Iris (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Normalize features for better convergence\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Helper Functions\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, 0, x)\n",
    "        x = np.where(x >= 0, 1, x)\n",
    "        return x\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, alpha, 1)\n",
    "        return x\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_pred.T + eps)) / m\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Parameter Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1. / layer_dims[i - 1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Forward & Backward Propagation\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i - 1] == \"sigmoid\":\n",
    "            A = sigmoid(Z)\n",
    "        elif activations[i - 1] == \"relu\":\n",
    "            A = relu(Z)\n",
    "        elif activations[i - 1] == \"softmax\":\n",
    "            A = softmax(Z)\n",
    "        elif activations[i - 1] == \"tanh\":\n",
    "            A = tanh(Z)\n",
    "        elif activations[i - 1] == \"leaky_relu\":\n",
    "            A = leaky_relu(Z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        cache[f\"Z{i}\"] = Z\n",
    "        cache[f\"A{i}\"] = A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache\n",
    "\n",
    "def backward_propagation(Y, params, cache, activations):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    A_L = cache[f\"A{L}\"]\n",
    "\n",
    "    # Output layer gradient\n",
    "    dZ = A_L - Y\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = np.dot(W.T, dZ) * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = np.dot(W.T, dZ) * relu(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"tanh\":\n",
    "                dZ = np.dot(W.T, dZ) * tanh(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"leaky_relu\":\n",
    "                dZ = np.dot(W.T, dZ) * leaky_relu(Z_prev, is_derivative=True)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported activation: {activations[i-2]}\")\n",
    "\n",
    "    return grads\n",
    "\n",
    "def update_parameters(params, grads, lr):\n",
    "    for key in params.keys():\n",
    "        if key.startswith(\"W\"):\n",
    "            idx = key[1:]\n",
    "            params[f\"W{idx}\"] -= lr * grads[f\"dW{idx}\"]\n",
    "            params[f\"b{idx}\"] -= lr * grads[f\"db{idx}\"]\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(X, Y, hidden_layers=[10], lr=0.05, epochs=5000, print_every=500, activations=None, loss_function=\"cross_entropy\"):\n",
    "    input_dim, output_dim = X.shape[1], Y.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "\n",
    "    if activations is None:\n",
    "        activations = [\"sigmoid\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "\n",
    "    for i in range(epochs + 1):\n",
    "        A_out, cache = forward_propagation(X, params, activations)\n",
    "\n",
    "        if loss_function == \"mse\":\n",
    "            loss = mse_loss(Y, A_out)\n",
    "        elif loss_function == \"cross_entropy\":\n",
    "            loss = cross_entropy_loss(Y, A_out)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "        grads = backward_propagation(Y, params, cache, activations)\n",
    "        params = update_parameters(params, grads, lr)\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(f\"Epoch {i:5d} | Loss: {loss:.4f}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _ = forward_propagation(X, params, activations)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. K-Fold Cross Validation\n",
    "# =======================================\n",
    "def cross_validate_model(X, Y, k=5, **train_kwargs):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        print(f\"\\n=== Fold {fold}/{k} ===\")\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
    "        params = train_neural_network(X_train, Y_train, **train_kwargs)\n",
    "        y_pred = predict(X_val, params, train_kwargs.get(\"activations\"))\n",
    "        y_true = np.argmax(Y_val, axis=1)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        accuracies.append(acc)\n",
    "        print(f\"Validation Accuracy: {acc:.2f}%\")\n",
    "        fold += 1\n",
    "\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    print(f\"\\nK-Fold Mean Accuracy: {mean_acc:.2f}% Â± {std_acc:.2f}%\")\n",
    "    return mean_acc, std_acc\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# Run K-Fold Cross Validation\n",
    "# =======================================\n",
    "\n",
    "cross_validate_model(\n",
    "    X, y,\n",
    "    k=5,\n",
    "    hidden_layers=[10, 10, 10],\n",
    "    lr=0.05,\n",
    "    epochs=3000,\n",
    "    print_every=1000,\n",
    "    activations=[\"sigmoid\", \"sigmoid\", \"sigmoid\", \"softmax\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbd164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104, 4) (23, 4) (23, 4)\n",
      "Epoch   1/5000 | Train Loss: 1.2235 | Val Loss: 1.2503\n",
      "Epoch 101/5000 | Train Loss: 0.1360 | Val Loss: 0.1572\n",
      "Epoch 201/5000 | Train Loss: 0.1430 | Val Loss: 0.1068\n",
      "Epoch 301/5000 | Train Loss: 0.0725 | Val Loss: 0.0937\n",
      "Epoch 401/5000 | Train Loss: 0.0846 | Val Loss: 0.0871\n",
      "Epoch 501/5000 | Train Loss: 0.0851 | Val Loss: 0.0847\n",
      "Early stopping triggered.\n",
      "\n",
      "Test Accuracy: 95.65%\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Deep Neural Network from Scratch with validation in the training loop\n",
    "# Dataset: Iris (Multiclass Classification)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Load and Preprocess Data\n",
    "# =======================================\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# Normalize features for better convergence\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "def train_val_test_split(X, y, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
    "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1\"\n",
    "\n",
    "    # First split: train vs temp (val+test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - train_size), random_state=random_state, stratify=y)\n",
    "\n",
    "    # Compute proportion of validation relative to temp\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1 - val_prop), random_state=random_state, stratify=y_temp)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 2. Activations\n",
    "# =======================================\n",
    "def sigmoid(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return np.exp(-x) / ((1 + np.exp(-x)) ** 2)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x, is_derivative=False):\n",
    "    if is_derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, is_derivative=False, alpha=0.01):\n",
    "    if is_derivative:\n",
    "        x = np.where(x < 0, alpha, 1)\n",
    "        return x\n",
    "    return np.maximum(alpha * x, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_shifted = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exp_shifted / np.sum(exp_shifted, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Loss Functions\n",
    "# =======================================\n",
    "def mse_loss(Y, Y_pred):\n",
    "    m = Y.shape[0]\n",
    "    return np.sum((Y - Y_pred.T) ** 2) / (2 * m)\n",
    "\n",
    "def cross_entropy_loss(Y, Y_pred, eps=1e-9):\n",
    "    m = Y.shape[0]\n",
    "    return -np.sum(Y * np.log(Y_pred.T + eps)) / m\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Initialization\n",
    "# =======================================\n",
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    params = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        params[f\"W{i}\"] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(1. / layer_dims[i - 1])\n",
    "        params[f\"b{i}\"] = np.zeros((layer_dims[i], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 5. Forward\n",
    "# =======================================\n",
    "def forward_propagation(X, params, activations, dropout_rates=None, training=True):\n",
    "    cache = {\"A0\": X.T}\n",
    "    L = len(activations)\n",
    "    dropout_masks = {}\n",
    "\n",
    "    for i in range(1, L + 1):\n",
    "        W, b = params[f\"W{i}\"], params[f\"b{i}\"]\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "\n",
    "        if activations[i-1] == \"sigmoid\": A = sigmoid(Z)\n",
    "        elif activations[i-1] == \"relu\": A = relu(Z)\n",
    "        elif activations[i-1] == \"tanh\": A = tanh(Z)\n",
    "        elif activations[i-1] == \"leaky_relu\": A = leaky_relu(Z)\n",
    "        elif activations[i-1] == \"softmax\": A = softmax(Z)\n",
    "        else: raise ValueError(f\"Unsupported activation: {activations[i-1]}\")\n",
    "\n",
    "        # Dropout\n",
    "        if training and dropout_rates and i <= len(dropout_rates):\n",
    "            rate = dropout_rates[i - 1]\n",
    "            mask = (np.random.rand(*A.shape) > rate).astype(float)\n",
    "            A *= mask\n",
    "            A /= (1 - rate)\n",
    "            dropout_masks[f\"M{i}\"] = mask\n",
    "\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "\n",
    "    return cache[f\"A{L}\"], cache, dropout_masks\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 6. Backpropagation\n",
    "# =======================================\n",
    "def backward_propagation(Y, params, cache, activations, dropout_masks=None, dropout_rates=None, weight_decay=0.0):\n",
    "    grads = {}\n",
    "    m = Y.shape[0]\n",
    "    L = len(activations)\n",
    "    Y = Y.T\n",
    "    dZ = cache[f\"A{L}\"] - Y\n",
    "\n",
    "    for i in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f\"A{i-1}\"]\n",
    "        W = params[f\"W{i}\"]\n",
    "\n",
    "        grads[f\"dW{i}\"] = (1 / m) * np.dot(dZ, A_prev.T) + weight_decay * W\n",
    "        grads[f\"db{i}\"] = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        grads = {k: np.clip(v, -1, 1) for k, v in grads.items()}\n",
    "\n",
    "        if i > 1:\n",
    "            Z_prev = cache[f\"Z{i-1}\"]\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "            if dropout_masks and i - 1 in [int(k[1:]) for k in dropout_masks.keys()]:\n",
    "                mask = dropout_masks[f\"M{i-1}\"]\n",
    "                dA_prev *= mask\n",
    "                dA_prev /= (1 - dropout_rates[i - 2])\n",
    "\n",
    "            if activations[i - 2] == \"sigmoid\":\n",
    "                dZ = dA_prev * sigmoid(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"relu\":\n",
    "                dZ = dA_prev * relu(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"tanh\":\n",
    "                dZ = dA_prev * tanh(Z_prev, is_derivative=True)\n",
    "            elif activations[i - 2] == \"leaky_relu\":\n",
    "                dZ = dA_prev * leaky_relu(Z_prev, is_derivative=True)\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 7. Optimizers\n",
    "# =======================================\n",
    "'''\n",
    "    Stochatic Gradient Descent (SGD):\n",
    "    θ^(t+1) <- θ^t - η∇L(y, ŷ)\n",
    "\n",
    "    Momentum:\n",
    "    v^(t+1) <- βv^t + (1-β)∇L(y, ŷ)^t\n",
    "    θ^(t+1) <- θ^t - ηv^(t+1)\n",
    "'''\n",
    "def update_parameters(params, grads, lr, optimizer_type, velocity=None, beta=0.9):\n",
    "    if optimizer_type == \"sgd\":\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "\n",
    "    elif optimizer_type == \"momentum\":\n",
    "        if velocity is None:\n",
    "            velocity = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "\n",
    "        for i in range(1, len(params)//2 + 1):\n",
    "            velocity[f\"W{i}\"] = beta * velocity[f\"W{i}\"] + (1 - beta) * grads[f\"dW{i}\"]\n",
    "            velocity[f\"b{i}\"] = beta * velocity[f\"b{i}\"] + (1 - beta) * grads[f\"db{i}\"]\n",
    "            params[f\"W{i}\"] -= lr * velocity[f\"W{i}\"]\n",
    "            params[f\"b{i}\"] -= lr * velocity[f\"b{i}\"]\n",
    "\n",
    "    return params, velocity\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 8. Training Loop\n",
    "# =======================================\n",
    "def train_neural_network(\n",
    "    X_train, \n",
    "    Y_train,\n",
    "    X_val, \n",
    "    Y_val,\n",
    "    hidden_layers=[10], \n",
    "    lr=0.01, \n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    activations=None, \n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=[0.1], \n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=True, \n",
    "    patience=100, \n",
    "    print_every=100\n",
    "):\n",
    "    input_dim, output_dim = X_train.shape[1], Y_train.shape[1]\n",
    "    layer_dims = [input_dim] + hidden_layers + [output_dim]\n",
    "    if activations is None:\n",
    "        activations = [\"relu\"] * len(hidden_layers) + [\"softmax\"]\n",
    "\n",
    "    params = initialize_parameters(layer_dims)\n",
    "    velocity = None\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X_shuffled, Y_shuffled = X_train[perm], Y_train[perm]\n",
    "\n",
    "        # Mini-batch iteration\n",
    "        if batch_size is None:\n",
    "            batches = [(X_shuffled, Y_shuffled)]\n",
    "        else:\n",
    "            batches = [(X_shuffled[i:i+batch_size], Y_shuffled[i:i+batch_size]) for i in range(0, X_shuffled.shape[0], batch_size)]\n",
    "\n",
    "        # Training by batch\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, Y_batch in batches:\n",
    "            A_out, cache, dropout_masks = forward_propagation(X_batch, params, activations, dropout_rates)\n",
    "            loss = cross_entropy_loss(Y_batch, A_out)\n",
    "            grads = backward_propagation(Y_batch, params, cache, activations, dropout_masks, dropout_rates, weight_decay)\n",
    "            params, velocity = update_parameters(params, grads, lr, optimizer_type, velocity)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        # Validation loss\n",
    "        A_val, _, _ = forward_propagation(X_val, params, activations, training=False)\n",
    "        val_loss = cross_entropy_loss(Y_val, A_val) if loss_function == \"cross_entropy\" else mse_loss(Y_val, A_val)\n",
    "        loss = epoch_loss / len(batches)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_params = params.copy()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    params = best_params\n",
    "                    break\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 9. Prediction & Evaluation\n",
    "# =======================================\n",
    "def predict(X, params, activations):\n",
    "    A_out, _, _ = forward_propagation(X, params, activations, training=False)\n",
    "    return np.argmax(A_out, axis=0)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 10. Run Training and Evaluate\n",
    "# =======================================\n",
    "params = train_neural_network(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    hidden_layers=[10],\n",
    "    lr=0.05,\n",
    "    epochs=5000,\n",
    "    batch_size=16,\n",
    "    activations=[\"relu\", \"softmax\"],\n",
    "    loss_function=\"cross_entropy\",\n",
    "    optimizer_type=\"momentum\",\n",
    "    dropout_rates=[0.1],\n",
    "    weight_decay=1e-4, \n",
    "    early_stopping=500,\n",
    "    print_every=100\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = predict(X_test, params, [\"relu\", \"softmax\"])\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"\\nTest Accuracy: {acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

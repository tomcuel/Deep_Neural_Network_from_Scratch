activations:
- leaky_relu
- sigmoid
- relu
batch_size: 32
dropout_rates:
- 0.26194833172354354
- 0.2619767130345951
- 0.08039059997884218
early_stopping: true
epochs: 40
layer_dims:
- 89
- 44
- 116
learning_rate: 0.012702943402314938
loss_function: cross_entropy
optimizer_method: adam
patience: 5
print_every: 5
weight_decay: 2.2043110498347255e-06
